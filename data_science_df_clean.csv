,documents,documents_processed,document_tokens,first_100_letters,documents_processed_2
0,"python tokens, Keywords, Identifiers, Literals, Punctuations, Operators 

In every language be it human language (or natural language, though it is also developed by human being not a natural one) and computer language, there are two components: tokens (smallest meaningful part of any statement, expression or command) and syntax or grammer.

Python tokens  are:

1. Keywords (basic words of any language)

2. Identifiers (programmer defined words of any program)

3. Literals (data or data structure)

4. Punctuations (, : etc)

5. Operators (+-*/)",python token keyword identifi liter punctuat oper,"['python', 'token', 'keyword', 'identifi', 'liter', 'punctuat', 'oper']","python tokens, Keywords, Identifiers, Literals, Punctuations, Operators ",everi languag human languag natur languag though also develop human natur one comput languag two compon token smallest meaning part statement express command syntax grammerpython token are1 keyword basic word language2 identifi programm defin word program3 liter data data structure4 punctuat etc5 oper
1,"Python keywords

There are 32 keywords in C++ and 35 keywords in python. 

Python keywords are:

(False, await, else, import, pass,
None, break, except, in, raise,
True, class, finally, is, return,
and, continue, for, lambda, try,
as, def, from, nonlocal, while,
assert, del, global, not, with,
async, elif, if, or, yield)
",python keyword,"['python', 'keyword']",Python keywords,32 keyword c 35 keyword python python keyword arefals await els import pass none break except rais true class final return continu lambda tri def nonloc assert del global async elif yield
2,"python data types 

(Numeric, Boolean, String, Datetime)

Numeric data types: Integers or whole numbers (1,2,-5,1000), Floats or real numbers (1.2,-0.5,2e2 or 2E2) and Complex numbers

Boolean Variable (true or false): comparison Operators (==, !=, >, <, >=, <=) returns a boolean value

String: ""mango"", ""mango is very tasty"" etc.

> 2e2 (called Scientific notation) means 2*10^2",python data type,"['python', 'data', 'type']",python data types ,numer boolean string datetimenumer data type integ whole number 1251000 float real number 12052e2 2e2 complex numbersboolean variabl true fals comparison oper return boolean valuestr mango mango tasti etc 2e2 call scientif notat mean 2102
3,"Arithmatic Operation

(addition[+], subtraction[-], multiplication[*], division[/], floor division[//], exponentiation[**], remainder modulus[%])

12%4 is 0, 13%4 is 1, 1%2 is 1, 4%5 is 4, 13%0O7 is 6

12//4 is 3, 13//4 is 3 and 1//2 is 0

a += 2 means first add 2 with ""a"" and then save it again in ""a""

same instruction is a = a+2

a /=2 means first divide ""a"" by 2 and then save it again in ""a""

same instruction is a = a/2",arithmat oper,"['arithmat', 'oper']",Arithmatic Operation,addit subtract multipl divis floor divis exponenti remaind modulus124 0 134 1 12 1 45 4 130o7 6124 3 134 3 12 0a 2 mean first add 2 save asam instruct a2a 2 mean first divid 2 save asam instruct a2
4,"floor function

In mathematics and computer science, the floor function is the function that takes as input a real number x, and gives as output the greatest integer less than or equal to x, denoted floor or ⌊x⌋.

import math
math.floor(x)

math.floor(5.5) returns 5",floor function,"['floor', 'function']",floor function,mathemat comput scienc floor function function take input real number x give output greatest integ less equal x denot floor ⌊x⌋import math mathfloorxmathfloor55 return 5
5,"ceiling function

The ceiling function maps x to the least integer greater than or equal to x, denoted ceil or ⌈x⌉

import math
math.ceil(x)

math.ceil(5.5) returns 6",ceil function,"['ceil', 'function']",ceiling function,ceil function map x least integ greater equal x denot ceil ⌈x⌉import math mathceilxmathceil55 return 6
6,"Understanding Variable

Variable is the storing place in computer memory for a data (Integers, Floats, Booleans, string, datetime) or data structure (list, tuple, set, dictionary,np.array, pd.DataFrame etc.) and the data/data structure can be changed or varied

Variable name (must start with letter or underscore)

Variable name for data=x,y,z etc.

Variable name for data structure=age_of_students, _students_age etc.

case_sensitive (best naming practice), CASE_SENSITIVE, and Case_Sensitive are each a different variable

> List and tuple are built on the idea of sequence",understand variabl,"['understand', 'variabl']",Understanding Variable,variabl store place comput memori data integ float boolean string datetim data structur list tupl set dictionarynparray pddatafram etc datadata structur chang variedvari name must start letter underscorevari name dataxyz etcvari name data structureageofstud studentsag etccasesensit best name practic casesensit casesensit differ variabl list tupl built idea sequenc
7,"commenting multiple lines in colab

Ctrl + /",comment multipl line colab,"['comment', 'multipl', 'line', 'colab']",commenting multiple lines in colab,ctrl
8,"Perfect number

Perfect number is a positive integer that is equal to the sum of its proper divisors. The smallest perfect number is 6, which is the sum of 1, 2, and 3. ",perfect number,"['perfect', 'number']",Perfect number,perfect number posit integ equal sum proper divisor smallest perfect number 6 sum 1 2 3
9,"To show a float upto two decimal places

print(f'Bananas in the bag is {percentage_bananas:.2f} %')

round(any_float,2)

For Numpy

np.around(my_array,2)",show float upto two decim place,"['show', 'float', 'upto', 'two', 'decim', 'place']",To show a float upto two decimal places,printfbanana bag percentagebananas2f roundanyfloat2for numpynparoundmyarray2
10,"To get the absolute value

abs(5.8-7.8) returns 2

",get absolut valu,"['get', 'absolut', 'valu']",To get the absolute value,abs5878 return 2
11,"The del keyword in python is primarily used to delete objects in Python

del variable_1, variable_2",del keyword python primarili use delet object python,"['del', 'keyword', 'python', 'primarili', 'use', 'delet', 'object', 'python']",The del keyword in python is primarily used to delete objects in Python,del variable1 variable2
12,"order of precedence in python

i) Parentheses

ii) Exponential

iii) Multiplication

iv) Division

v) Addition

vi) Subtraction",order preced python,"['order', 'preced', 'python']",order of precedence in python,parenthesesii exponentialiii multiplicationiv divisionv additionvi subtract
13,"An application or app or software or model is nothing but a soft machine (a machinery products).

Every machine has minimum one function which takes some inputs and gives some outputs.

Products are mainly of two types: machinery products and non-machinery products",applic app softwar model noth soft machin machineri product,"['applic', 'app', 'softwar', 'model', 'noth', 'soft', 'machin', 'machineri', 'product']",An application or app or software or model is nothing but a soft machine (a machinery products).,everi machin minimum one function take input give outputsproduct main two type machineri product nonmachineri product
14,"Four basic answers of any question

yes, no, may be and don't know",four basic answer question,"['four', 'basic', 'answer', 'question']",Four basic answers of any question,yes may dont know
15,"
logical resoning

Statement: Is buying things on installments profitable to the customer?

Arguments:

I. Yes. He has to pay less.

II. No, paying instalments upsets the family budget.

1. Only I is strong

2. Only II is strong

3. Bother I & II are strong

4. Either I or II is strong (when both are strong but opposite)

5. Neither I nor II is strong",logic reson,"['logic', 'reson']","
logical resoning",statement buy thing instal profit customerargumentsi yes pay lessii pay instal upset famili budget1 strong2 ii strong3 bother ii strong4 either ii strong strong opposite5 neither ii strong
16,"pprint module

The pprint module provides a capability to “pretty-print” arbitrary Python data structures in a well-formatted and more readable way! ",pprint modul,"['pprint', 'modul']",pprint module,pprint modul provid capabl “prettyprint” arbitrari python data structur wellformat readabl way
17,"Understanding String

String is a word, a phrase, a sentence, a paragraph or an entire encyclopedia

> It has sequence and indexing

> string indexing begins from 0

> 'str' object does not support item assignment means string is immutable

'string' or ""string"" 

double quotes are used when there is any 's in the string

> A string can be called a safe bridge if it has no gaps in it i.e, no spaces.",understand string,"['understand', 'string']",Understanding String,string word phrase sentenc paragraph entir encyclopedia sequenc index string index begin 0 str object support item assign mean string immutablestr string doubl quot use string string call safe bridg gap ie space
18,"to print a new line

print('Use \n to print a new line')",print new line,"['print', 'new', 'line']",to print a new line,printus n print new line
19,"for color print

from termcolor import colored

print(colored('Hello', 'green', attrs=['bold']))",color print,"['color', 'print']",for color print,termcolor import coloredprintcoloredhello green attrsbold
20,"Grabbing the element by index 

print(string_name[3]), 

print(string_name[-2]) 

means (length-2)th index",grab element index,"['grab', 'element', 'index']",Grabbing the element by index ,printstringname3 printstringname2 mean length2th index
21,"Basics of String Slicing

[starting_index : ending_index],  element located at the ending_index is not included

string[0:13] or string[:13] are same

string[2:18] or string[2:] are same when 18 is the last index

string[:]  If we do not specify the starting and the ending index, it will extract all elements of the string

Reversing a string in python

string_name[::-1]

slicing with step size, string[3:14:2] means forward slicing with step size 2, string[14:3:-2] means backward slicing with step size 2",basic string slice,"['basic', 'string', 'slice']",Basics of String Slicing,startingindex endingindex element locat endingindex includedstring013 string13 samestring218 string2 18 last indexstr specifi start end index extract element stringrevers string pythonstringname1sl step size string3142 mean forward slice step size 2 string1432 mean backward slice step size 2
22,"concatenate strings

print(string1 + string2 )

print(""D"", end = ' ')

print(""C"", end = ' ')

will return D C
",concaten string,"['concaten', 'string']",concatenate strings,printstring1 string2 printd end printc end return c
23,"String functions

print(), type(), len()

ord('z') -ord('a') returns an integer

ord() method converts a character into its Unicode code.",string function,"['string', 'function']",String functions,print type lenordz orda return integerord method convert charact unicod code
24,"String methods 

string.lower(), 

string.upper(), 

string.count('n'),

string.index('n'), 

string.find('n'), 

string.replace('n','L'), 

> for removing any character we can use 
string.replace('n','')

string.split(' '), 

string.split()-default spliting by space,

''.join([""g"", ""h"", ""o""]), 

string.title()-converts the first character in each word to Uppercase and remaining characters to Lowercase

string.rfind(""any_substr"") find the last position index of a given substring

string.startswith(""matching_string"") returns True/False

> As string is immutable all the method like upper, lower, split etc. will not change the main variable. Thus,

my_string=""Mango""
my_string.lower() returns mango
print(my_string) returns Mango",string method,"['string', 'method']",String methods ,stringlow stringupp stringcountnstringindexn stringfindn stringreplacenl remov charact use stringreplacenstringsplit stringsplitdefault splite spacejo h stringtitleconvert first charact word uppercas remain charact lowercasestringrfindanysubstr find last posit index given substringstringstartswithmatchingstr return truefals string immut method like upper lower split etc chang main variabl thusmystringmango mystringlow return mango printmystr return mango
25,"way to embed expressions inside string literals

first_name = 'Rahul'

last_name = 'Modi'

full_name = f'Left plus right makes {first_name}  {last_name}'  

f-strings provide a way to embed expressions inside string literals",way emb express insid string liter,"['way', 'emb', 'express', 'insid', 'string', 'liter']",way to embed expressions inside string literals,firstnam rahullastnam modifullnam fleft plus right make firstnam lastnam fstring provid way emb express insid string liter
26,"String duplication

String duplication occurs when we multiply string by a number, 

my_name='prabir'

print(my_name*3)",string duplic,"['string', 'duplic']",String duplication,string duplic occur multipli string number mynameprabirprintmyname3
27,"Taking user's input

Read Input From stdin in Python using input()

inputted_number = int(input())
 
or

marks = int(input(""Enter our marks: ""))",take user input,"['take', 'user', 'input']",Taking user's input,read input stdin python use inputinputtednumb intinput ormark intinputent mark
28,"String matching

import re
 pattern = re.compile(""%s"" % common_term)

      topics=[x for x in list_of_topics if pattern.match(x)]",string match,"['string', 'match']",String matching,import pattern recompil commonterm topicsx x listoftop patternmatchx
29,"Understanding List

List is a data structure to store multiple items in a single variable. It has sequence, indexing and is mutable.

['A string',23,100.232,'o',True]

> We know that every structure has dimensions. Thus a data structure also has dimensions (1 to n)

> Grabbing the element by index is same as string

> List Slicing is same as string slicing

> List concatenation is same as string concatenation 

> Duplication method similar to strings

my_string=[0]*26

> List mutability

my_list= ['spam', 'egg', 'bacon', 'tomato', 'ham', 'lobster']

my_list[2] = 10

print(my_list)

> Lists are like arrays but lists are more flexible means they have no size constraint or type constraint",understand list,"['understand', 'list']",Understanding List,list data structur store multipl item singl variabl sequenc index mutablea string23100232otru know everi structur dimens thus data structur also dimens 1 n grab element index string list slice string slice list concaten string concaten duplic method similar stringsmystring026 list mutabilitymylist spam egg bacon tomato ham lobstermylist2 10printmylist list like array list flexibl mean size constraint type constraint
30,"List functions

len(), min(), max(), sum(), sorted(), sorted(my_list,reverse=True) for sorting in reverse order, zip() function  returns a list of n-paired tuples

list(""any string"") split string into list of characters and returns

['a', 'n', 'y', ' ', 's', 't', 'r', 'i', 'n', 'g']",list function,"['list', 'function']",List functions,len min max sum sort sortedmylistreversetru sort revers order zip function return list npair tupleslistani string split string list charact returnsa n r n g
31,"List methods

At first store the list in a variable,

variable_name .append(one_element), 

variable_name.extend(list_of_elements), 

variable_name.pop()  by default removes last index, 
variable_name.remove() means remove by element, 

variable_name.count(), 

variable_name.index(),  

variable_name.sort(), 

variable_name.reverse()",list method,"['list', 'method']",List methods,first store list variablevariablenam appendoneel variablenameextendlistofel variablenamepop default remov last index variablenameremov mean remov element variablenamecount variablenameindex variablenamesort variablenamerevers
32,"Nested List

[lst_1,lst_2,lst_3]

Entire table of data can be stored in nested list variable",nest list,"['nest', 'list']",Nested List,lst1lst2lst3entir tabl data store nest list variabl
33,"Range function to get a sequence list of numbers. 

for k in range(10):

list(range(10)) or list(range(0,10)) or list(range(0,10,1))

range(0) does not give any output

floats are not allowed in range function",rang function get sequenc list number,"['rang', 'function', 'get', 'sequenc', 'list', 'number']",Range function to get a sequence list of numbers. ,k range10listrange10 listrange010 listrange0101range0 give outputfloat allow rang function
34,"Main Subjects of Data Science Stream

> Coding

> Analytics Framework

> Mathematics

> Artificial Intelligence

> Business Intelligence

> Computer Engineering",main subject data scienc stream,"['main', 'subject', 'data', 'scienc', 'stream']",Main Subjects of Data Science Stream,code analyt framework mathemat artifici intellig busi intellig comput engin
35,"Main Modules of Coding

> Python

> SQL",main modul code,"['main', 'modul', 'code']",Main Modules of Coding,python structur queri languag
36,"Main Modules of Analytics Framework

> Analytics Framework",main modul analyt framework,"['main', 'modul', 'analyt', 'framework']",Main Modules of Analytics Framework,analyt framework
37,"Main Modules of Mathematics

> Mathematics",main modul mathemat,"['main', 'modul', 'mathemat']",Main Modules of Mathematics,mathemat
38,"Main Modules of Artificial Intelligence

> Machine Learning

> Deep Learning",main modul artifici intellig,"['main', 'modul', 'artifici', 'intellig']",Main Modules of Artificial Intelligence,machin learn deep learn
39,"Main Modules of Business Intelligence

> Industry Insights",main modul busi intellig,"['main', 'modul', 'busi', 'intellig']",Main Modules of Business Intelligence,industri insight
40,"Main Modules of Computer Engineering

> Data Engineering",main modul comput engin,"['main', 'modul', 'comput', 'engin']",Main Modules of Computer Engineering,data engin
41,"Main Topics of Python

> Integers, Floats and Booleans

> Strings

> Lists

> Tuples, Sets and Dictionaries

> Statements, Indentation and Conditionals

> Loops and Iterations

> List comprehension

> Functions and Methods

> Production Grade Programming

> Competitive Coding

> General Knowledge for a Data Scientist

> Numpy

> Pandas

> Data Wrangling

> Data Visualization",main topic python,"['main', 'topic', 'python']",Main Topics of Python,integ float boolean string list tupl set dictionari statement indent condit loop iter list comprehens function method product grade program competit code general knowledg data scientist numpi panda data wrangl data visual
42,"Main Topics of Analytics Framework

> Excel

> Tableau

> Business KPI ",main topic analyt framework,"['main', 'topic', 'analyt', 'framework']",Main Topics of Analytics Framework,excel tableau busi kpi
43,"Main Topics of Mathematics

> Overview of mathematics

> Calculus

> Vector Algebra

> Matrix Algebra

> Probability Theory

> Summarizing Data

> Random Variables

> Discrete Distributions

> Continuous Distributions

> Joint Distributions

> Sampling & Statistical Inference

> Confidence Intervals

> Hypothesis Testing",main topic mathemat,"['main', 'topic', 'mathemat']",Main Topics of Mathematics,overview mathemat calculus vector algebra matrix algebra probabl theori summar data random variabl discret distribut continu distribut joint distribut sampl statist infer confid interv hypothesi test
44,"Main Topics of Machine Learning

> Computer science and machine learning

> General Modeling Techniques

> Linear Regression

> Bias variance tradeoff

> Regularized Linear Regression

> Cross validation and hyperparameter tuning

> Logistic regression

> Decision Trees

> Ensembles of decision trees

> Model Explainability

> k-nearest neighbors

> Naive Bayes Classifier

> Support Vector Machines

> K-means clustering

> Hierarchical clustering

> Principal Component Analysis

> Anomaly Detection

> Natural Language Processing 

> Topic modeling

> Recommender Systems

> Time Series Analysis",main topic machin learn,"['main', 'topic', 'machin', 'learn']",Main Topics of Machine Learning,comput scienc machin learn general model techniqu linear regress bias varianc tradeoff regular linear regress cross valid hyperparamet tune logist regress decis tree ensembl decis tree model explain knearest neighbor naiv bay classifi support vector machin kmean cluster hierarch cluster princip compon analysi anomali detect natur languag process topic model recommend system time seri analysi
45,"Main Topics of Deep Learning

> Neural Networks

> Deep Neural Networks

> Improving Deep Neural Networks

> Structuring ML Projects

> Convolutional Neural Networks

> Recurrent Neural Network",main topic deep learn,"['main', 'topic', 'deep', 'learn']",Main Topics of Deep Learning,neural network deep neural network improv deep neural network structur machin learn project convolut neural network recurr neural network
46,"Main Topics of Industry Insights

> Guesstimate

> Case Study-ML in Heathcare

> Case Study-ML in Fraud risk analytics

> Case Study-ML in Credit Risk

> Case Study-ML in E-commerce",main topic industri insight,"['main', 'topic', 'industri', 'insight']",Main Topics of Industry Insights,guesstim case studyml heathcar case studyml fraud risk analyt case studyml credit risk case studyml ecommerc
47,"Main Topics of Data Engineering

> Linux basics and terminal commands

> Python modules and project setup

> Version control-Git

> API basics with flask

>  FastAPI

> Docker

> Microservices and streamlit

> ML Lifecycle

> Cloud Computing

> MLOps-MLFlow

> PySpark

> Airflow",main topic data engin,"['main', 'topic', 'data', 'engin']",Main Topics of Data Engineering,linux basic termin command python modul project setup version controlgit applic program interfac basic flask fastappl program interfac docker microservic streamachin learningit machin learn lifecycl cloud comput machin learningopsmachin learningflow pyspark airflow
48,"Subtopics of Integers, Floats and Booleans

> python tokens, Keywords, Identifiers, Literals, Punctuations, Operators

> Python keywords

> python data types

> Arithmatic Operation 

> floor function

> ceiling function

> Understanding Variable 

> order of precedence in python",subtop integ float boolean,"['subtop', 'integ', 'float', 'boolean']","Subtopics of Integers, Floats and Booleans",python token keyword identifi liter punctuat oper python keyword python data type arithmat oper floor function ceil function understand variabl order preced python
49,"Understanding Tuples Basics

Tuples, Sets and Dictionaries are data structures to store multiple items in a single variable

Tuples are similar to list but immutable

('A string',23,100.232,'o',True) or 'A string',23,100.232,'o',True

Say,
my_tuple= 'A string',23,100.232,'o',True

a, b, *c= my_tuple

Then, print(c) will return [100.232, 'o', True]

> Tuple indexing same as list indexing

> Tuple slicing same as list slicing

> Tuple functions are same as list

> Tuples methods are less than list, .index(), .count()

> Tuples are used rarely only when immutability is a must",understand tupl basic,"['understand', 'tupl', 'basic']",Understanding Tuples Basics,tupl set dictionari data structur store multipl item singl variabletupl similar list immutablea string23100232otru string23100232otruesay mytupl string23100232otruea b c mytuplethen printc return 100232 true tupl index list index tupl slice list slice tupl function list tupl method less list index count tupl use rare immut must
50,"Set basics

Sets are an unordered collection of unique elements

set() function creates an emty set. {1,6,4,'abc'} is a non empty set. Sets are mutable like list

A set cannot have a mutable item like list within.

We can not convert a set into list

my_set_a = {1,6,4,'abc'}

my_set_b = {'abc'}

(my_set_a - my_set_a) is not equal to (my_set_b - my_set_a)",set basic,"['set', 'basic']",Set basics,set unord collect uniqu elementsset function creat emti set 164abc non empti set set mutabl like lista set cannot mutabl item like list withinw convert set listmyseta 164abcmysetb abcmyseta myseta equal mysetb myseta
51,"Set methods

.add()

 .update([2,3,4]) helps to add multiple elements to a set

.remove(element)
 
.union(another list, tuple or set)

 .intersection(another list, tuple or set)

set.intersection(set1,set2,set3)
set.intersection(*set_list)

 .difference(another list, tuple or set)

.symmetric_difference(another set) returns excluding intersection",set method,"['set', 'method']",Set methods,add update234 help add multipl element setremoveel unionanoth list tupl set intersectionanoth list tupl setsetintersectionset1set2set3 setintersectionsetlist differenceanoth list tupl setsymmetricdifferenceanoth set return exclud intersect
52,"Dictionary Basics

Dictionaries are hash tables or hash maps that can map keys to values

{} makes an emty dictionary. 

Non emty dictionary {key1:value1,key2:value2,key3:value3}

We can call values by their key, 

my_dict[key_name] or can use .get(key_name) method. 

We can also call an element of any data structure values and can apply method on that element.",dictionari basic,"['dictionari', 'basic']",Dictionary Basics,dictionari hash tabl hash map map key valu make emti dictionari non emti dictionari key1value1key2value2key3value3w call valu key mydictkeynam use getkeynam method also call element data structur valu appli method element
53,"Dictionary methods

.keys(), .values(), .items(), .pop(key) 

.items() returns a list of tuples with key and values",dictionari method,"['dictionari', 'method']",Dictionary methods,key valu item popkey item return list tupl key valu
54,"Dictionary operations

We can add a new key-value pair

 my_dict['Design'] ='Sr Data Scientist' 

We can also do this by using .update({'Design':'Sr Data Scientist'}) method. 

.update() method can also be used to update exsisting key-values

We can delete a key-value pair

del my_dict[key]

dict() function converts list of paired elements into dictionary",dictionari oper,"['dictionari', 'oper']",Dictionary operations,add new keyvalu pair mydictdesign sr data scientist also use updatedesignsr data scientist method updat method also use updat exsist keyvaluesw delet keyvalu pairdel mydictkeydict function convert list pair element dictionari
55,"Subtopics of Strings

> Understanding String

> Grabbing the element by index 

> Basics of String Slicing

> concatenate strings

> String functions

> String methods 

>  way to embed expressions inside string literals

> String duplication",subtop string,"['subtop', 'string']",Subtopics of Strings,understand string grab element index basic string slice concaten string string function string method way emb express insid string liter string duplic
56,"Subtopics of Lists

> Understanding List

> List functions

> List methods

> Nested List

> Apply function to each element of a list",subtop list,"['subtop', 'list']",Subtopics of Lists,understand list list function list method nest list appli function element list
57,"Subtopics of Tuples, Sets and Dictionaries

> Understanding Tuples basics

> Set basics

> Set methods

> Dictionary Basics

> Dictionary methods

> Dictionary operations",subtop tupl set dictionari,"['subtop', 'tupl', 'set', 'dictionari']","Subtopics of Tuples, Sets and Dictionaries",understand tupl basic set basic set method dictionari basic dictionari method dictionari oper
58,"Subtopics of Statements, Indentation and Conditionals

> Assignment statement, conditional statement

> Understanding Expression

> Multi-line statements

> Understanding Comments

> Auto indentation",subtop statement indent condit,"['subtop', 'statement', 'indent', 'condit']","Subtopics of Statements, Indentation and Conditionals",assign statement condit statement understand express multilin statement understand comment auto indent
59,"Subtopics of Loops and Iterations

> Understanding Loops

> Understanding the main loops of python

> Enumerate function

> break, continue, and pass statements 

> Calculating program execution time",subtop loop iter,"['subtop', 'loop', 'iter']",Subtopics of Loops and Iterations,understand loop understand main loop python enumer function break continu pass statement calcul program execut time
60,"Subtopics of List comprehension

> Understanding List comprehension

> Set Comprehension 

> Dictionary comprehension",subtop list comprehens,"['subtop', 'list', 'comprehens']",Subtopics of List comprehension,understand list comprehens set comprehens dictionari comprehens
61,"Subtopics of Functions and Methods

> Understanding Functions, Model and  Environment 

> Defining a function 

> Functions details 

> Function and method 

> Global variable",subtop function method,"['subtop', 'function', 'method']",Subtopics of Functions and Methods,understand function model environ defin function function detail function method global variabl
62,"Subtopics of Production Grade Programming

> Understanding Production Grade Programming

> Object Oriented Programming

> File handling in python

>  Understanding Attribute

> Defining a class

> Understanding Polymorphism

> Understanding Inheritance

> Exception handling",subtop product grade program,"['subtop', 'product', 'grade', 'program']",Subtopics of Production Grade Programming,understand product grade program object orient program file handl python understand attribut defin class understand polymorph understand inherit except handl
63,"Subtopics of Competitive Coding

> Understanding Competitive coding

1. Python code for finding longest substring of vowels

2. Python code for checking valid parentheses

3.  Python code for converting roman to integer

4.  Python code for finding the indices of the two numbers such that they add up to target integer

5.  Python code for converting Integer to the Sum of Two No-Zero Integers

6.  Python code for finding Longest Common Prefix

7. Python code for finding the Anagrams

8. Python code for finding the sum as a binary string

9. Python code for finding First and Last Position of Element in Sorted Array

10. Python code for finding a perfect number

11. Python Code to Find Next Prime Number

12. Python code to count vowels and consonants in the string

13. Python code to find the first non-repeating character in given string

14. Python code to find the largest prime factor of a given number

15. Python code to find the greatest common divisor (GCD) of two integers

16. Python code to find the biggest even number between two numbers inclusive

17. Python code to find the string consisting of all the words whose lengths are prime numbers

Input: The quick brown fox jumps over the lazy dog.

Output: The quick brown fox jumps the

18. Python code to print a Fibonacci series

19. Python code to get a list, sorted in increasing order by the last element in each tuple from a given list of non-empty tuples

Input:
[(2, 5), (1, 2), (4, 4), (2, 3), (2, 1)]

Output:
[(2, 1), (1, 2), (2, 3), (4, 4), (2, 5)]

20. Python code to remove duplicates from a list of lists

Input:
[[10, 20], [40], [30, 56, 25], [10, 20], [33], [40]]

Output:
[[10, 20], [40], [30, 56, 25], [33]]

21. Python code to find the strings in a given list, starting with a given prefix

Input:
['cat', 'car', 'fear', 'center']

""ca""

Output:
Strings in the said list starting with a given prefix:

['cat', 'car']

22. Python program to determine the direction ('increasing' or 'decreasing') of monotonic sequence numbers

Input:
[1,2,3,4,5,6]

Output:
Increasing

23. Python code that accepts a comma separated sequence of words as input and prints the unique words in sorted form (alphanumerically)

Input:
lst= [red, black, pink, green, black, green]

Output:
black,green,pink,red

24. Program in Python to count number of duplicates in an array having multiple duplicates

25. Python program to find all n-digit integers that start or end with 2

26. Given a list of numbers and a number to inject, write a Python program to create a list containing that number in between each pair of adjacent numbers

27. Python code to find a substring in a given string which contains a vowel between two consonants

Input:
Hello

Output:
Hel

28. Python program to find the indices of three numbers that sum to 0 in a given list of numbers

Input:
[12, -7, 3, -89, 14, 4, -78, -1, 2, 7]

Output:
[1, 2, 5]

29. Python program to get the single digits in numbers sorted backwards and converted to English words

Input:
[1, 3, 4, 5, 11]

Output:
['five', 'four', 'three', 'one']

30. Python code to find the sum of all the numbers (within certain range) that can be written as the sum of fifth powers of their digits

Input:
range=1000000

Output:
443839

31. Python program to find the vowels from each of the original texts (y counts as a vowel at the end of the word) from a given list of strings

Input:
['w3resource', 'Python', 'Java', 'C++']

Output:
['eoue', 'o', 'aa', '']

32. Python program to find the two closest distinct numbers in a given a list of numbers

Input:
[1.3, 5.24, 0.89, 21.0, 5.27, 1.3]

Output:
[5.24, 5.27]

33. Python program to find all 5's in integers less than n that are divisible by 9 or 15

Input:
50

Output:
[[15, 1], [45, 1]]

34. Python program to find the sum of the even elements that are at odd indices

Input:
[1, 2, 8, 3, 9, 4]

Output:
6

35. Python program to find three numbers from an array such that the sum of three numbers equal to zero

Input:
[-1,0,1,2,-1,-4]

Output:
[[-1, -1, 2], [-1, 0, 1]]

36. Python program to find the factorial of the number using recursion method

37. Python program to find words with both alphabets and numbers from an input string

Input:
string= Emma25 is Data scientist50 and AI Expert

Output:
Displaying words with alphabets and numbers

Emma25

38. Python program to restore the original string by entering the compressed string

Input:
Original text: #39+1=1#30

Output:
999+1=1000

39. Python program to pack consecutive duplicates of a given list elements into sublists

Input:
List= [0, 0, 1, 2, 3, 4, 4, 5, 6, 6, 6, 7, 8, 9]

Output:
[[0, 0], [1], [2], [3], [4, 4], [5], [6, 6, 6], [7], [8], [9]]

40. Python program to create a list whose ith element is the maximum of the first i elements from a input list

Input:
[0, -1, 3, 8, 5, 9, 8, 14, 2, 4, 3, -10, 10, 17, 41, 22, -4, -4, -15, 0]

Output:
[0, 0, 3, 8, 8, 9, 9, 14, 14, 14, 14, 14, 14, 17, 41, 41, 41, 41, 41, 41]

41. Python program to find all integers <= 1000 that are the product of exactly three primes. Each integer should represent as the list of its three prime factors

Input:
10

Output:
[[2, 2, 2]]

42. Python program to find the closest palindrome from a given string

Input:
abcde

Output:
abcba

43. Python program to find a list of numbers: the first element is the smallest, the second is the largest of the remaining, the third is the smallest of the remaining, the fourth is the smallest of the remaining, etc.

Input:
[1, 3, 4, 5, 11]

Output:
[1, 11, 3, 5, 4]

44. Python program to compute the depth of each parentheses group

Input:
(()) (()) () ((()()()))

Output:
[2, 2, 1, 3]

45. Python program to split it into groups of perfectly matched parentheses without any whitespace

Input:
( ()) ((()()())) (()) ()

Output:
['(())', '((()()()))', '(())', '()']

46. Python program to find a string consisting of space-separated characters with given counts

Input:
{""f"": 1, ""o"": 2}

Output:
f o o

47. Python Program to Determine Whether one String is a Rotation of Another

Input:
Given first string = ""btechgeeks""

Given second string = ""geeksbtech""

Output:
The given second string is the rotation of the given first string

48. Python program to print Pascal triangle as shown below

49.  Python code to create a sentence from a dictionary

Input: {""d"":""data"", ""r"":""reader"", ""o"":""of"", ""n"":""new"", ""a"":""age""}

Output: ""drona stands for data reader of new age""

50. Data Scientist must have statistical thinking and meditative mind in their attitude and must have skills like python, sql and data visualization. Python code for converting a dictionary into a table using pandas

Input: {""attitude"":[""statistical thinking"", ""meditative mind"",""-""], ""skills"": [""python"", ""sql"", ""data visualization""]}

Output: table
",subtop competit code,"['subtop', 'competit', 'code']",Subtopics of Competitive Coding,understand competit coding1 python code find longest substr vowels2 python code check valid parentheses3 python code convert roman integer4 python code find indic two number add target integer5 python code convert integ sum two nozero integers6 python code find longest common prefix7 python code find anagrams8 python code find sum binari string9 python code find first last posit element sort array10 python code find perfect number11 python code find next prime number12 python code count vowel conson string13 python code find first nonrep charact given string14 python code find largest prime factor given number15 python code find greatest common divisor gcd two integers16 python code find biggest even number two number inclusive17 python code find string consist word whose length prime numbersinput quick brown fox jump lazi dogoutput quick brown fox jump the18 python code print fibonacci series19 python code get list sort increas order last element tupl given list nonempti tuplesinput 2 5 1 2 4 4 2 3 2 1output 2 1 1 2 2 3 4 4 2 520 python code remov duplic list listsinput 10 20 40 30 56 25 10 20 33 40output 10 20 40 30 56 25 3321 python code find string given list start given prefixinput cat car fear centercaoutput string sartifici intelligenc list start given prefixcat car22 python program determin direct increas decreas monoton sequenc numbersinput 123456output increasing23 python code accept comma separ sequenc word input print uniqu word sort form alphanumericallyinput lst red black pink green black greenoutput blackgreenpinkred24 program python count number duplic array multipl duplicates25 python program find ndigit integ start end 226 given list number number inject write python program creat list contartifici intelligencen number partifici intelligenc adjac numbers27 python code find substr given string contartifici intelligencen vowel two consonantsinput hellooutput hel28 python program find indic three number sum 0 given list numbersinput 12 7 3 89 14 4 78 1 2 7output 1 2 529 python program get singl digit number sort backward convert english wordsinput 1 3 4 5 11output five four three one30 python code find sum number within certartifici intelligencen rang written sum fifth power digitsinput range1000000output 44383931 python program find vowel origin text count vowel end word given list stringsinput w3resourc python java coutput eoue aa 32 python program find two closest distinct number given list numbersinput 13 524 089 210 527 13output 524 52733 python program find 5s integ less n divis 9 15input 50output 15 1 45 134 python program find sum even element odd indicesinput 1 2 8 3 9 4output 635 python program find three number array sum three number equal zeroinput 101214output 1 1 2 1 0 136 python program find factori number use recurs method37 python program find word alphabet number input stringinput string emma25 data scientist50 artifici intellig expertoutput display word alphabet numbersemma2538 python program restor origin string enter compress stringinput origin text 391130output 9991100039 python program pack consecut duplic given list element sublistsinput list 0 0 1 2 3 4 4 5 6 6 6 7 8 9output 0 0 1 2 3 4 4 5 6 6 6 7 8 940 python program creat list whose ith element maximum first element input listinput 0 1 3 8 5 9 8 14 2 4 3 10 10 17 41 22 4 4 15 0output 0 0 3 8 8 9 9 14 14 14 14 14 14 17 41 41 41 41 41 4141 python program find integ 1000 product exact three prime integ repres list three prime factorsinput 10output 2 2 242 python program find closest palindrom given stringinput abcdeoutput abcba43 python program find list number first element smallest second largest remartifici intelligencen third smallest remartifici intelligencen fourth smallest remartifici intelligencen etcinput 1 3 4 5 11output 1 11 3 5 444 python program comput depth parenthes groupinput output 2 2 1 345 python program split group perfect match parenthes without whitespaceinput output 46 python program find string consist spacesepar charact given countsinput f 1 2output f o47 python program determin whether one string rotat anotherinput given first string btechgeeksgiven second string geeksbtechoutput given second string rotat given first string48 python program print pascal triangl shown below49 python code creat sentenc dictionaryinput ddata rreader oof nnew aageoutput drona stand data reader new age50 data scientist must statist think medit mind attitud must skill like python structur queri languag data visual python code convert dictionari tabl use pandasinput attitudestatist think medit mind skill python structur queri languag data visualizationoutput tabl
64,"Assignment statement, conditional statement

Instructions that a Python interpreter can execute are called statements. A statement may or may not return a value. 

Assignment statement (assign some data or data structure to a variable) & conditional statement (if, elif (used for nested if), else, while, for and import)

x=2+3 is an assignment statement

If x==5: is a conditional statment",assign statement condit statement,"['assign', 'statement', 'condit', 'statement']","Assignment statement, conditional statement",instruct python interpret execut call statement statement may may return valu assign statement assign data data structur variabl condit statement elif use nest els importx23 assign statementif x5 condit statment
65,"Understanding Expression

Expression needs to be executed and evaluated and returns a value. 

For example 2+3 expression returns 5 in python.  All functions and methods in python returns a value and hence they are expressions

Thus we can conclude that all expressions are statements but all statements are not expressions",understand express,"['understand', 'express']",Understanding Expression,express need execut evalu return valu exampl 23 express return 5 python function method python return valu henc expressionsthus conclud express statement statement express
66,"Multi-line statements 

line continuation by () or [] or {}. Large string can be broken in multi line by \

a=(5+
   6)

We can also put multiple statements in a single line using semicolons. a = 1 ; b = 2 ; c = 3 or a,b,c = 1,2,3",multilin statement,"['multilin', 'statement']",Multi-line statements ,line continu larg string broken multi line a5 6we also put multipl statement singl line use semicolon 1 b 2 c 3 abc 123
67,"Understanding Comments

Comments (single line or multi line) are for programmers to better understand a program. Do comments by #. Another way of doing this is to use triple quotes, either ''' or """"""",understand comment,"['understand', 'comment']",Understanding Comments,comment singl line multi line programm better understand program comment anoth way use tripl quot either
68,"Auto indentation

Auto indentation is done with (: + enter) or manually with tab to maintain is proper structure of the code",auto indent,"['auto', 'indent']",Auto indentation,auto indent done enter manual tab maintain proper structur code
69,"""truthy"" and ""falsy""

We use ""truthy"" and ""falsy"" to differentiate from the boolean values True and False.",truthi falsi,"['truthi', 'falsi']","""truthy"" and ""falsy""",use truthi falsi differenti boolean valu true fals
70,"Subtopics of Numpy

> Understanding Library

> Understanding Package

> Understanding Module

> Modular programming

> N-dimensional array

> Creating a two-dimensional array

> attributes of numpy array

> Advantages of numpy array

> Creating ndarray

> Defining Array size

> Conversion of list and tuple to ndarray

> Indexing and Slicing of ndarray

> Grabbing element by index of 2D array

> Array Manipulation

> Important functions in numpy

> Difference between view and copy in numpy

> Basic Operations & Functions in numpy

> Difference between list and numpy array

> Exponentiation of vectors

> Normalizing rows

> Writing softmax function with numpy",subtop numpi,"['subtop', 'numpi']",Subtopics of Numpy,understand librari understand packag understand modul modular program ndimension array creat twodimension array attribut numpi array advantag numpi array creat ndarray defin array size convers list tupl ndarray index slice ndarray grab element index 2d array array manipul import function numpi differ view copi numpi basic oper function numpi differ list numpi array exponenti vector normal row write softmax function numpi
71,"Subtopics of Pandas

> Basics of Pandas Dataframe

> Rows and columns of pandas dataframe

> Pandas series

> Convertion of numpy array to pandas dataframe

> Methods and attributes of pandas DataFrame 

> Connecting google drive to colab notebook

> For uploading any file from local computer to the colab session

> Converting a csv file to pandas DataFrame

> Converting a csv file to pandas DataFrame

> Writing data in a csv file

> Appending new row to an exsisting csv file

> Editing a csv file

> Excel file with multiple worksheets

> Writing in a new excel file  

> Editing in pandas df and then save them to csv/excel

> Slicing operation on Pandas DataFrame

> Slicing operation on a particular column  in Pandas DataFrame

> Conditional Slicing or Conditional Filtering in  Pandas DataFrame

> Adding new column in existing df

> Removing one or multiple columns 

> Removing one or multiple rows

> Setting a column as row index

> Methods for particular column

> Sort values in  Pandas DataFrame

> Creating a new column with lambda function

> Difference among and & |

> Pandas data display options

> Another way to convert dict to df

> Converting pandas df to numpy array

> String to numeric in  Pandas DataFrame

> Numpy array or pandas df to matrix convertion",subtop panda,"['subtop', 'panda']",Subtopics of Pandas,basic panda datafram row column panda datafram panda seri convert numpi array panda datafram method attribut panda datafram connect googl drive colab notebook upload file local comput colab session convert csv file panda datafram convert csv file panda datafram write data csv file append new row exsist csv file edit csv file excel file multipl worksheet write new excel file edit panda df save csvexcel slice oper panda datafram slice oper particular column panda datafram condit slice condit filter panda datafram ad new column exist df remov one multipl column remov one multipl row set column row index method particular column sort valu panda datafram creat new column lambda function differ among panda data display option anoth way convert dict df convert panda df numpi array string numer panda datafram numpi array panda df matrix convert
72,"Subtopics of Data Wrangling

> Basics of data analysis and data mining

> Basics of Data wrangling

> Concatenating pandas DataFrame

> DataFrame merging operation through joins

> Groupby operation on pandas dataframe for data analysis

> Detailed EDA

> Quick EDA

> EDA practice

> Use of ast library

> Use of explode function",subtop data wrangl,"['subtop', 'data', 'wrangl']",Subtopics of Data Wrangling,basic data analysi data mine basic data wrangl concaten panda datafram datafram merg oper join groupbi oper panda datafram data analysi detail exploratori data analysi quick exploratori data analysi exploratori data analysi practic use ast librari use explod function
73,"Subtopics of Data Visualization

> libraries for data visualization

> Matplotlib library

> Matplotlib pyplot function

> Matplotlib Line Plot

> Matplotlib Horizontal Bar Plot

> Matplotlib Box plot and Scatter Plot

> drawing a trend line in the scatter plot

> drawing horizontal or vertial lines

> Seaborn library

> seaborn Line plot, Scatter plot, Distribution/Density Plot

> seaborn Joint Distribution Plot, Heatmap, Bar Plot

> seaborn Histogram, Factor Plot, Box plot

> Seaborn Pairplot

> Ploting directly from pandas dataframe

> Editing the plot area with matplotlib and seaborn

> Combining two plots in a single graph

> saving the plot as png in google drive",subtop data visual,"['subtop', 'data', 'visual']",Subtopics of Data Visualization,librari data visual matplotlib librari matplotlib pyplot function matplotlib line plot matplotlib horizont bar plot matplotlib box plot scatter plot draw trend line scatter plot draw horizont vertial line seaborn librari seaborn line plot scatter plot distributiondens plot seaborn joint distribut plot heatmap bar plot seaborn histogram factor plot box plot seaborn pairplot plote direct panda datafram edit plot area matplotlib seaborn combin two plot singl graph save plot png googl drive
74,"Subtopics of Excel

> Excel basics

> Data Handling in excel

> sections in excel file (top to bottom)

> Menu Bar Tabs in excel

> Understanding circular reference

> Data types in excel

> To view the groupby statistics like pandas df in excel

> Filter in excel

> Conditional Formatting in excel

> Sorting in excel

> Removing Duplicates in excel

> Pivot and Slicers in excel

> Refreshing all pivot tables

> Charts in excel

> Dashboard in excel

> Waterfall or birdge graph analysis

> Excel functions

> Concatenation of text in excel

> Power Query in Excel or Power BI",subtop excel,"['subtop', 'excel']",Subtopics of Excel,excel basic data handl excel section excel file top bottom menu bar tab excel understand circular refer data type excel view groupbi statist like panda df excel filter excel condit format excel sort excel remov duplic excel pivot slicer excel refresh pivot tabl chart excel dashboard excel waterfal birdg graph analysi excel function concaten text excel power queri excel power bi
75,"Subtopics of Tableau

> Tableau basics

> Best practices for visualization

> Panes of Tableau

> Trend line models in Tableau

> Creating Calculated field in Tableau

> Chart area of Tableau

> Chart types in Tableau

> Tableau Pills

>  Tableau Desktop applications

> Components of a Dashboard

> COUNTD function

> Reference line and Reference band

> File extensions in Tableau

> Filters in Tableau

> Data blending

> Data Types in Tableau",subtop tableau,"['subtop', 'tableau']",Subtopics of Tableau,tableau basic best practic visual pane tableau trend line model tableau creat calcul field tableau chart area tableau chart type tableau tableau pill tableau desktop applic compon dashboard countd function refer line refer band file extens tableau filter tableau data blend data type tableau
76,"Subtopics of Business KPI

> KPI Basics

> KPI vs Metric

> Need of KPI for the company

> Types of Indicators

> Effectiveness and Efficiency

> Ways to develop KPI

> Three steps to a stronger KPI strategy

> Key Performance Indicators in practice

> Executive Dashboard

> Possible dangers of industrial performance indicators ",subtop busi kpi,"['subtop', 'busi', 'kpi']",Subtopics of Business KPI,kpi basic kpi vs metric need kpi compani type indic effect effici way develop kpi three step stronger kpi strategi key perform indic practic execut dashboard possibl danger industri perform indic
77,"Main Topics of SQL

> Introduction to SQL

> BASIC SQL COMMANDS

> Practicing sql in python environment

> SQL keys

> ADVANCED SQL COMMANDS",main topic structur queri languag,"['main', 'topic', 'structur', 'queri', 'languag']",Main Topics of SQL,introduct structur queri languag basic structur queri languag command practic structur queri languag python environ structur queri languag key advanc structur queri languag command
78,"Subtopics of Calculus

> Radian measure

> Machine Learning Use Cases of Calculus

> Basics of derivative

> The chain rule

> Point of maxima and point of minima

> Partial Derivatives

> Ways to find the slope of f(x,y)

> Jacobian Matrix

> Derivatives Formulas

>  Definite Integrals

> Formulas for integration

> Basics of Limit

> Solution method of limit of indeterminate form",subtop calculus,"['subtop', 'calculus']",Subtopics of Calculus,radian measur machin learn use case calculus basic deriv chain rule point maxima point minima partial deriv way find slope fxi jacobian matrix deriv formula definit integr formula integr basic limit solut method limit indetermin form
79,"Subtopics of Vector Algebra

> Scaler, Vector, Matrix and Tensor

> Dimension and Direction

> position vector/ location vector/ radius vector

> Applications of Linear Algebra in Data Science

> Vector operations

> Vector Dot Product

> Vector Projection

> Vector Cross product

> Vector norms

> Representing multivariable linear equation in vector space

> Multivariable linear equations

> Visualization of vector",subtop vector algebra,"['subtop', 'vector', 'algebra']",Subtopics of Vector Algebra,scaler vector matrix tensor dimens direct posit vector locat vector radius vector applic linear algebra data scienc vector oper vector dot product vector project vector cross product vector norm repres multivari linear equat vector space multivari linear equat visual vector
80,"Subtopics of Matrix Algebra

> Types of Matrices

> Matrix operations

> Matrix multiplication

> Equation in matrix form

> Identity matrix

> Determinant of a matrix

> Transpose of a matrix

> Adjoint of a matrix

> cofactor matrix

> Inverse of a matrix

> Eigenvalues and Eigenvectors of a square matrix

> Use of eigen values and eigen vectors

> Nilpotent matrix ",subtop matrix algebra,"['subtop', 'matrix', 'algebra']",Subtopics of Matrix Algebra,type matric matrix oper matrix multipl equat matrix form ident matrix determin matrix transpos matrix adjoint matrix cofactor matrix invers matrix eigenvalu eigenvector squar matrix use eigen valu eigen vector nilpot matrix
81,"Subtopics of Probability Theory

> Importance of Probability theory

> Set Theory

> Experiment, sample space, observation and experience

> Intersection and Union of Sets

> Mutually exclusive sets

> Permutation & Combination

> Basic Probability

> Probability Axiom#1

> Probability Axiom#2

> Probability Axiom#3 or Special Addition Rule

> Addition Rule

> Conditional Probability

> Multiplication Rule

> Solving any set or probability problem

> Difference between mutually exclusive and independent events",subtop probabl theori,"['subtop', 'probabl', 'theori']",Subtopics of Probability Theory,import probabl theori set theori experi sampl space observ experi intersect union set mutual exclus set permut combin basic probabl probabl axiom1 probabl axiom2 probabl axiom3 special addit rule addit rule condit probabl multipl rule solv set probabl problem differ mutual exclus independ event
82,"Understanding Loops

Loops help us to execute a block of code repeatedly

When a statement  repeatedly execute a single statement or group of statements as long as the condition is true, then it is called a 'loop'",understand loop,"['understand', 'loop']",Understanding Loops,loop help us execut block code repeatedlywhen statement repeat execut singl statement group statement long condit true call loop
83,"Understanding the main loops of python

for and while (similar to an if statement but continues to execute the code repeatedly as long as the condition is True)

A while loop in Python is used for 
indefinite type of iteration

for loops are used to loop through an iterable object (string, list, tuple, set and dict) and perform the same action on each element

for i in iterable_object:
last_element=i

Here, i is the element or key of the data_structure

for x, y in list_of_tuples:
   print(x,y) 

A word of caution however! It is possible to create an infinitely running loop with while statements",understand main loop python,"['understand', 'main', 'loop', 'python']",Understanding the main loops of python,similar statement continu execut code repeat long condit truea loop python use indefinit type iterationfor loop use loop iter object string list tupl set dict perform action elementfor iterableobject lastelementiher element key datastructurefor x listoftupl printxi word caution howev possibl creat infinit run loop statement
84,"Iterate means utter repeatedly.

for house in got_houses_list[::]:
  print(f""House {house}""). 

Here we are assigning the variable house as the elements of sliced list",iter mean utter repeat,"['iter', 'mean', 'utter', 'repeat']",Iterate means utter repeatedly.,hous gothouseslist printfhous hous assign variabl hous element slice list
85,"Enumerate function 

enumerate() function adds a counter to an iterable(string, list, tuple, set and dict) and returns it in a form of enumerate object. This enumerate object can then be used directly in for loops or be converted into a list of tuples using list() function

l1 = [""eat"",""sleep"",""repeat""]

print (list(enumerate(l1)))

returns

[(0, 'eat'), (1, 'sleep'), (2, 'repeat')]",enumer function,"['enumer', 'function']",Enumerate function ,enumer function add counter iterablestr list tupl set dict return form enumer object enumer object use direct loop convert list tupl use list functionl1 eatsleeprepeatprint listenumeratel1returns0 eat 1 sleep 2 repeat
86,"break, continue, and pass statements in our loops add additional functionality

break: Breaks out of the current closest enclosing loop.
continue: Goes to the top of the closest enclosing loop.
pass: Does nothing at all.",break continu pass statement loop add addit function,"['break', 'continu', 'pass', 'statement', 'loop', 'add', 'addit', 'function']","break, continue, and pass statements in our loops add additional functionality",break break current closest enclos loop continu goe top closest enclos loop pass noth
87,"Calculating program execution time

import time

>> For “real-world time”

start = time.time() # at the starting of the code

end = time.time() # at the ending of the code

print(end - start)

>> For Relative Time (It has no defined relationship to real-world time). This is mainly used to evaluate relative performance of two versions of code

start = time.process_time() # at the starting of the code

end = time.process_time() # at the ending of the code

print(f""{1000 *(end - start)} ms"")
",calcul program execut time,"['calcul', 'program', 'execut', 'time']",Calculating program execution time,import time “realworld time”start timetim start codeend timetim end codeprintend start relat time defin relationship realworld time main use evalu relat perform two version codestart timeprocesstim start codeend timeprocesstim end codeprintf1000 end start ms
88,"Subtopics of Summarizing Data

> Basics of Summarizing Data

> Numerical, Categorical, Dichotomous, and Ordinal Data

> Measure of central tendency

> Understanding Mean

> Understanding Median

> Understanding Mode

> Measure of spread

> Understanding Range

> Understanding Variance

> Understanding Standard Deviation

> Understanding Interquartile Range

> Steps to Calculate IQR

> Outliers with respect to IQR

> Measures of Symmetry

> Understanding skewness

> Libraries for Summarizing Data

> Skewness and Kurtosis measurements",subtop summar data,"['subtop', 'summar', 'data']",Subtopics of Summarizing Data,basic summar data numer categor dichotom ordin data measur central tendenc understand mean understand median understand mode measur spread understand rang understand varianc understand standard deviat understand interquartil rang step calcul iqr outlier respect iqr measur symmetri understand skew librari summar data skew kurtosi measur
89,"Subtopics of Random Variables

> Understanding random variable

> Python Code for Random Variables

> Random seed

> Types of Random Variables

> Continuous and Discrete Random variables using numpy array

> Discrete Random Variables in Probability Distribution

> Continuous Random Variables in Probability Distribution

> Mean of Random Variables

> Variance of Random Variables

> Point probability, cumulative probability

> Formulas for expectation and variance",subtop random variabl,"['subtop', 'random', 'variabl']",Subtopics of Random Variables,understand random variabl python code random variabl random seed type random variabl continu discret random variabl use numpi array discret random variabl probabl distribut continu random variabl probabl distribut mean random variabl varianc random variabl point probabl cumul probabl formula expect varianc
90,"Subtopics of Discrete Distributions

> Basics of Probability distribution

> Types of Discrete Probability Distribution

> Uniform Distribution-discrete

> Bernoulli Distribution

> Binomial Distribution

> Geometric Distribution

> Poisson Distribution

> Moments of probability distribution

> Moments in Physics",subtop discret distribut,"['subtop', 'discret', 'distribut']",Subtopics of Discrete Distributions,basic probabl distribut type discret probabl distribut uniform distributiondiscret bernoulli distribut binomi distribut geometr distribut poisson distribut moment probabl distribut moment physic
91,"Subtopics of Continuous Distributions

> Basics of continuous distribution

> Types of continuous probability distribution

> Uniform Distribution-continuous

> Normal or Gaussian Distribution

> Standard Normal Distribution

> Exponential Distribution

> Gamma Function

> Gamma Distribution

> Chi-square Distribution

>  t-Distribution

> F-Distribution

> log-normal distribution

> Exponential expressions

> Euler's number

> Python Code for Continuous Distributions

> Continuous distributions in ML

> Most frequent types of distribution for data scientist",subtop continu distribut,"['subtop', 'continu', 'distribut']",Subtopics of Continuous Distributions,basic continu distribut type continu probabl distribut uniform distributioncontinu normal gaussian distribut standard normal distribut exponenti distribut gamma function gamma distribut chisquar distribut tdistribut fdistribut lognorm distribut exponenti express euler number python code continu distribut continu distribut ml frequent type distribut data scientist
92,"Subtopics of Joint Distributions

> Basics of Joint Distribution

> Two dimensional random vector

> Marginal Distribution and Conditional Distribution

> Independent Random Variables

> Probability mass function

> Degree of association

> Understanding of Covariance

> Understanding of Correlation",subtop joint distribut,"['subtop', 'joint', 'distribut']",Subtopics of Joint Distributions,basic joint distribut two dimension random vector margin distribut condit distribut independ random variabl probabl mass function degre associ understand covari understand correl
93,"Subtopics of Sampling & Statistical Inference

> Basics of Sampling & Statistical Inference

> Random sample

> Understanding of Statistics

> Statistical Inference

> Sample Mean and Sample Variance

> Independent and indentically distributed random variables

> Sampling with Replacement

> Central Limit Theorem (CLT) or z-results or z-score

> t-Result or t-Score

> F-result or F-Score

> Point Estimation

> Mean and Expectation",subtop sampl statist infer,"['subtop', 'sampl', 'statist', 'infer']",Subtopics of Sampling & Statistical Inference,basic sampl statist infer random sampl understand statist statist infer sampl mean sampl varianc independ indent distribut random variabl sampl replac central limit theorem clt zresult zscore tresult tscore fresult fscore point estim mean expect
94,"Subtopics of Confidence Intervals

> Basics of Confidence interval

> Calculating population parameters for one sample

> Population proportion for binomial experiment

> Calculating population parameters for two samples from difference population

> Pooled variance

> Confidence limits

> Sampling schemes from best to worst",subtop confid interv,"['subtop', 'confid', 'interv']",Subtopics of Confidence Intervals,basic confid interv calcul popul paramet one sampl popul proport binomi experi calcul popul paramet two sampl differ popul pool varianc confid limit sampl scheme best worst
95,"Subtopics of Hypothesis Testing

> Basics of Hypothesis Testing

> Rare Event Rule for Inferential Statistics

> Components of a formal hypothesis test

> Null Hypothesis

> Alternative Hypothesis

> Identifying the null and alternative hypothesis

> Test Statistic

> Significance Level

> Critical Region

> Critical Value

> Two-tailed, Right-tailed, Left-tailed Tests

> P-value or probability value

> Conclusions in Hypothesis Testing based on P-value

> Type-I error

> Type-II error

> Power of a hypothesis test

> Practical implementation of hypothesis test

> A/B Testing",subtop hypothesi test,"['subtop', 'hypothesi', 'test']",Subtopics of Hypothesis Testing,basic hypothesi test rare event rule inferenti statist compon formal hypothesi test null hypothesi altern hypothesi identifi null altern hypothesi test statist signific level critic region critic valu twotail righttail lefttail test pvalu probabl valu conclus hypothesi test base pvalu typei error typeii error power hypothesi test practic implement hypothesi test ab test
96,"Subtopics of Linear Regression

> Supervised, parametric, regression algorithm

> Basics of linear regression

> Error or residuals

> Loss function and cost function

> Types of loss function

> OLS method for finding out the model parameters

> Gradient Descent Fundamentals

> Assumptions of regression

> Multicollinearity issue

> Heteroscedasticity issue

> Properties of regression line

> Advantages of linear regression

> Limitations of linear regression

> Data preparation for linear regression

> Omission of relevant variable from a regression equation

> Visualizing Linear Regression

> Understanding of Feature scaling

> Difference bwteen Matrix and metric

> Metrics that help in evaluating our model’s accuracy

> Model Health using KS Scores

> Decision Making using Risk Bins

> Libraries for linear regression

> Importance of csv file

> Implementation Steps of Linear Regression

> transform and fit_transform

> Polynomial model",subtop linear regress,"['subtop', 'linear', 'regress']",Subtopics of Linear Regression,supervis parametr regress algorithm basic linear regress error residu loss function cost function type loss function ordinari least squar method find model paramet gradient descent fundament assumpt regress multicollinear issu heteroscedast issu properti regress line advantag linear regress limit linear regress data prepar linear regress omiss relev variabl regress equat visual linear regress understand featur scale differ bwteen matrix metric metric help evalu model accuraci model health use ks score decis make use risk bin librari linear regress import csv file implement step linear regress transform fittransform polynomi model
97,"Subtopics of Bias variance tradeoff

> Optimal Model

> Underfit Model

> Overfit Model

> Understanding Estimator

> Noise, Underfittiing, Overfitting and Overgeneralizing

> Techniques to reduce overfitting",subtop bias varianc tradeoff,"['subtop', 'bias', 'varianc', 'tradeoff']",Subtopics of Bias variance tradeoff,optim model underfit model overfit model understand estim nois underfitti overfit overgener techniqu reduc overfit
98,"Subtopics of Regularized Linear Regression

> Basics of Regularized Linear Regression

> Types of Regularization

> Ridge Regression (L2 Regularization)

> Lasso Regression (L1 Regularization)

> Libraries for Regularized Linear Regression

> Alpha value",subtop regular linear regress,"['subtop', 'regular', 'linear', 'regress']",Subtopics of Regularized Linear Regression,basic regular linear regress type regular ridg regress l2 regular lasso regress l1 regular librari regular linear regress alpha valu
99,"Understanding list comprehension 

A list comprehension is a syntax for creating a list based on an existing list

For loop which returns a list/tuple can be written as list comprehension. Tuple function may be used to convert list comprehension into tuple.

[output_expression for  variable in input_sequence (string, list, tuple, set or dict) conditionals] 

[number**2 for number in list_of_numbers if number%2!=0]

[1 for act,pred in zipped_list if act==pred]",understand list comprehens,"['understand', 'list', 'comprehens']",Understanding list comprehension ,list comprehens syntax creat list base exist listfor loop return listtupl written list comprehens tupl function may use convert list comprehens tupleoutputexpress variabl inputsequ string list tupl set dict condit number2 number listofnumb number201 actpr zippedlist actpr
100,"Set Comprehension

Structure of Set Comprehension is same as list comprehension.",set comprehens,"['set', 'comprehens']",Set Comprehension,structur set comprehens list comprehens
101,"Dictionary comprehension

Structure of Dictionary comprehension is same as list comprehension.",dictionari comprehens,"['dictionari', 'comprehens']",Dictionary comprehension,structur dictionari comprehens list comprehens
102,"Function range and domain

Range is defined as all the possible values which a function  f(x)  can take.

Domain is defined as all the possible values which  x  can take.",function rang domain,"['function', 'rang', 'domain']",Function range and domain,rang defin possibl valu function fx takedomain defin possibl valu x take
103,"Subtopics of Cross validation and hyperparameter tuning

>  Cross validation Basics

> Simple Validation vs Cross Validation

> k-fold CV

> Python coding for CV

> yellowbrick CVScores

> Fundamentals of hyperparameters

> Hyperparameters tuning

> Coding for Hyperparameters tuning",subtop cross valid hyperparamet tune,"['subtop', 'cross', 'valid', 'hyperparamet', 'tune']",Subtopics of Cross validation and hyperparameter tuning,cross valid basic simpl valid vs cross valid kfold cv python code cv yellowbrick cvscore fundament hyperparamet hyperparamet tune code hyperparamet tune
104,"Subtopics of Logistic regression

> Basics of Logistic regression

> Meaning of odds and logit function in probability

> Logistic function or sigmoid function

> Working of parametric model

> Benefits of Parametric ML models

> Limitations of Parametric ML Models

> Classification model and probability

> Generative models  and Discriminative models

> Coding for Logistic regression

> Logistic Regression Parameters

> Evaluation Metrics for Imbalanced Classification

> Confusion Matrix

> Accuracy, Precision, Recall and F1-Score

> Importance of F1 Score

> Checking Cross-validation scores

> Receiver operating characteristic and AUC

> Sparse matrix and Dense matrix

> Synthetic Minority Over-sampling Technique

> Under-sampling: Tomek links",subtop logist regress,"['subtop', 'logist', 'regress']",Subtopics of Logistic regression,basic logist regress mean odd logit function probabl logist function sigmoid function work parametr model benefit parametr machin learn model limit parametr machin learn model classif model probabl generat model discrimin model code logist regress logist regress paramet evalu metric imbalanc classif confus matrix accuraci precis recal f1score import f1 score check crossvalid score receiv oper characterist auc spars matrix dens matrix synthet minor oversampl techniqu undersampl tomek link
105,"Subtopics of Decision Trees

> Important Terminology in Decision Tree ML Model

> Steps of Decision tree algorithm

> Methods to measure the similarity of child nodes

> Fitting Decision tree classifier

> Fitting Decision tree regressor

> Coding for Visualizing Decision Tree

> Advantages of Decision tree

> Disadvantages of Decision tree",subtop decis tree,"['subtop', 'decis', 'tree']",Subtopics of Decision Trees,import terminolog decis tree machin learn model step decis tree algorithm method measur similar child node fit decis tree classifi fit decis tree regressor code visual decis tree advantag decis tree disadvantag decis tree
106,"Subtopics of Ensembles of decision trees

> Basics of Ensembles of decision trees

> Ensemble techniques

> Bagging technique

> Boosting technique

> Extreme Gradient Boosting

> Stacking technique

> Python coding for Ensembles of decision trees

> Finding feature importance

> Classification report

> Ways to improve random forest accuracy

> Out of Bag (OOB) score

> Random forest hyperparameters",subtop ensembl decis tree,"['subtop', 'ensembl', 'decis', 'tree']",Subtopics of Ensembles of decision trees,basic ensembl decis tree ensembl techniqu bag techniqu boost techniqu extrem gradient boost stack techniqu python code ensembl decis tree find featur import classif report way improv random forest accuraci bag oob score random forest hyperparamet
107,"Subtopics of Model Explainability

> Basics of Model Explainability

> Black Box Model vs. White Box Model

> Explainable AI

>  Importance of explainability

>  Scope of explainability

>  Approach of explainability

> Techniques or Libraries for Explainability in ML

> Local Interpretable Model-Agnostic Explanations

> Shapley Additive Explanations

> Implementing SHAP

> Explain Like I'm 5

> Other techniques for Explainability in ML",subtop model explain,"['subtop', 'model', 'explain']",Subtopics of Model Explainability,basic model explain black box model vs white box model explain ai import explain scope explain approach explain techniqu librari explain machin learn local interpret modelagnost explan shapley addit explan implement shap explain like im 5 techniqu explain machin learn
108,"Subtopics of k-nearest neighbors

> Basics of KNN

> Euclidean Distance

> Working of kNN

> Ways to select the value of k in the kNN Algorithm

> Disadvantages of kNN Algorithm

> Python coding for KNN

> Knn for recommender system",subtop knearest neighbor,"['subtop', 'knearest', 'neighbor']",Subtopics of k-nearest neighbors,basic knearest neighbor euclidean distanc work knearest neighbor way select valu k knearest neighbor algorithm disadvantag knearest neighbor algorithm python code knearest neighbor knearest neighbor recommend system
109,"Subtopics of Naive Bayes Classifier

> Basics of Naive Bayes

> Bayes theorem

> Understanding of Naive Bayes

> Text Pre-processing

> One hot encoding

> Bag of words

> Part-of-speech (POS) tagging

> Python coding for Naive Bayes",subtop naiv bay classifi,"['subtop', 'naiv', 'bay', 'classifi']",Subtopics of Naive Bayes Classifier,basic naiv bay bay theorem understand naiv bay text preprocess one hot encod bag word partofspeech pos tag python code naiv bay
110,"Subtopics of Support Vector Machines

> Basics of SVM

> Kernel function

> Hinge loss function, hypersurface and hyperplane

> Python coding for SVM

> Tuning the SVM

> Advantages of SVM

> Disadvantages of SVM

> SVR hyperparameters",subtop support vector machin,"['subtop', 'support', 'vector', 'machin']",Subtopics of Support Vector Machines,basic svm kernel function hing loss function hypersurfac hyperplan python code svm tune svm advantag svm disadvantag svm svr hyperparamet
111,"Subtopics of General Modeling Techniques

> Performance of a machine learning model

> Understanding Feature engineering

> Importance of Feature Engineering

> Basic EDA

> Understanding Outliers

> Conversion of Categorical column to numerical

> One-hot encoding vs Label Encoding

> Multi Label columns to Binary

> Number-String to numerical value

> Variance Inflation Factor (VIF)

> Steps of ML modelling

> Understanding Warnings

> Data preparation or data preprocessing

> Linear Transformation or Scaling

> Non-linear Transformations

> Time Complexity and Space Complexity

> Cosine Similarity",subtop general model techniqu,"['subtop', 'general', 'model', 'techniqu']",Subtopics of General Modeling Techniques,perform machin learn model understand featur engin import featur engin basic eda understand outlier convers categor column numer onehot encod vs label encod multi label column binari numberstr numer valu varianc inflat factor vif step machin learn model understand warn data prepar data preprocess linear transform scale nonlinear transform time complex space complex cosin similar
112,"Subtopics of K-means clustering

> Understanding Clustering Algorithm

> Python coding for Kmeans

> Few issues of Kmeans

> Expectation-Maximization approach

> Stopping Criteria for K-Means Clustering

> Silhouette score for finding best no. of clusters

> Elbow method for finding best no. of clusters

> Plotting Clusters

> Features of KMeans model

> Density-based clustering

> K-means++ Algorithm",subtop kmean cluster,"['subtop', 'kmean', 'cluster']",Subtopics of K-means clustering,understand cluster algorithm python code kmean issu kmean expectationmaxim approach stop criteria kmean cluster silhouett score find best cluster elbow method find best cluster plot cluster featur kmean model densitybas cluster kmean algorithm
113,"Subtopics of Hierarchical clustering

> Basics of Hierarchical clustering

> Types of hierarchical clustering

> Proximity matrix

>  Understanding Linkage

> Finding out no. of clusters from visualization

> Python coding for Hierarchical clustering",subtop hierarch cluster,"['subtop', 'hierarch', 'cluster']",Subtopics of Hierarchical clustering,basic hierarch cluster type hierarch cluster proxim matrix understand linkag find cluster visual python code hierarch cluster
114,"Subtopics of Principal Component Analysis

> Basics of PCA

> Drawback of PCA

> Python Coding of PCA

> Understanding the important features in PCA

> Deeper Understanding of PCA

> Math behind PCA

> PCA and Truncated SVD

> Scree Plot

> Trace of matrix",subtop princip compon analysi,"['subtop', 'princip', 'compon', 'analysi']",Subtopics of Principal Component Analysis,basic princip compon analysi drawback princip compon analysi python code princip compon analysi understand import featur princip compon analysi deeper understand princip compon analysi math behind princip compon analysi princip compon analysi truncat svd scree plot trace matrix
115,"Subtopics of Anomaly Detection

> Basics of Anomaly detection

> Assumptions in Anomaly detection

> Finding global outliers

> Univariate Anomaly Detection

> Multivariate Anomaly Detection

> Swamping and Masking

> Python coding for Isolation forest

> Finding local outliers-LOF

> Visual representation of univariate anomalies

> Deeper Understanding of Anomalies

> Potential or Use of Anomalies",subtop anomali detect,"['subtop', 'anomali', 'detect']",Subtopics of Anomaly Detection,basic anomali detect assumpt anomali detect find global outlier univari anomali detect multivari anomali detect swamp mask python code isol forest find local outlierslof visual represent univari anomali deeper understand anomali potenti use anomali
116,"Subtopics of Natural Language Processing 

> Natural Language Understanding (NLU) in five dimensions

> Important Applications of NLP

> Feature engineering in NLP

> n-gram in NLP

> Understanding TF-IDF

> TFIDF hyperparameters

> RNN or LSTM in NLP

> Transformer Network

> Positional Encoding in Transformer

> Libraries for NLP

> Right order for a text classification

> Measuring the complexity of a sentence",subtop natur languag process,"['subtop', 'natur', 'languag', 'process']",Subtopics of Natural Language Processing ,natur languag understand nlu five dimens import applic nlp featur engin nlp ngram nlp understand tfidf tfidf hyperparamet recurr neural network long short term memori nlp transform network posit encod transform librari nlp right order text classif measur complex sentenc
117,"Subtopics of Topic modeling

> Basics of Topic modeling

> Basic assumptions of all topic models

> Libraries for Topic Modeling

> Understanding LDA

> Plate Notation

> Python coding of LDA model

> Visualization of LDA

> Similarity between LDA and PCA

> LDA hyperparameters",subtop topic model,"['subtop', 'topic', 'model']",Subtopics of Topic modeling,basic topic model basic assumpt topic model librari topic model understand latent dirichlet alloc plate notat python code latent dirichlet alloc model visual latent dirichlet alloc similar latent dirichlet alloc pca latent dirichlet alloc hyperparamet
118,"Subtopics of Recommender Systems

> Basics of Recommender System

> Popular Recommender Systems

> Collaborative Filtering

> Content-Based Filtering

> Hybrid Approach

> Implementation strategies of Collaborative Filtering

> Latent factor models

> Matrix Factorization

> Implementation of SVD

> metrics used for evaluation of Recommender systems

> Shortcoming of content-based recommender systems",subtop recommend system,"['subtop', 'recommend', 'system']",Subtopics of Recommender Systems,basic recommend system popular recommend system collabor filter contentbas filter hybrid approach implement strategi collabor filter latent factor model matrix factor implement svd metric use evalu recommend system shortcom contentbas recommend system
119,"Understanding Functions, Model and  Environment

In natural language, function means assigning a memory location for taking some input and giving some output. Model is a combination of multiple functions. Environment is a collection of multiple models.

Functions is one of the most basic levels of reusing code. It groups together a set of statements so they can be run more than once.",understand function model environ,"['understand', 'function', 'model', 'environ']","Understanding Functions, Model and  Environment",natur languag function mean assign memori locat take input give output model combin multipl function environ collect multipl modelsfunct one basic level reus code group togeth set statement run
120,"Defining a function

There are many inbuilt functions associated with data or data structure, however we can define our own functions as and when required.

def example_function(argument):
  '''
  This functions returns the …...
  '''
  desired_result= f""Your input is: {argument}""

  return desired_result

> argument means the input

> function without return statement,

def change(one):
   print(one*3)",defin function,"['defin', 'function']",Defining a function,mani inbuilt function associ data data structur howev defin function requireddef examplefunctionargu function return … desiredresult fyour input argument return desiredresult argument mean input function without return statementdef changeon printone3
121,"Functions details

We can pass 'n' number of arguments in a function

def change(first, *second):
  return print(f""First input is {first}\nSecond input is {second}"")
 
change(1,2,3,4) returns

First input is 1
Second input is (2, 3, 4)

> Function can have multiple arguments like 

example_function(arg1,arg2,..etc) or multiple returns like return x,y,…etc

> Function can have multiple return statement like 

if ….return x else return y",function detail,"['function', 'detail']",Functions details,pass n number argument functiondef changefirst second return printffirst input firstnsecond input second change1234 returnsfirst input 1 second input 2 3 4 function multipl argument like examplefunctionarg1arg2etc multipl return like return xy…etc function multipl return statement like …return x els return
122,"Function and method

Function and method both look similar as they perform in an almost similar way. A method is called by its name but it is associated with an object (dependent).

Functions are to view the properties of a data or data structure and methods are to perform some actions on data or data structure. 

e.g. functions and methods can be applied on string, list, tuple, set, dict

A function may be defined outside a class for doing any intended task, but a method can not be defined outside a class.

# Basic Python method
class ABC:
    def method_abc (self):
        print(""I am a method_abc of ABC class. "")

Methods are of the form:

object.method(arg1,arg2,etc...)",function method,"['function', 'method']",Function and method,function method look similar perform almost similar way method call name associ object dependentfunct view properti data data structur method perform action data data structur eg function method appli string list tupl set dicta function may defin outsid class intend task method defin outsid class basic python method class abc def methodabc self printi methodabc abc class method formobjectmethodarg1arg2etc
123,"Global variable

Iside a function, we can call global variable (defined outside function)

b=10 

def change():
  global b
  return b

Then, change() will return 10",global variabl,"['global', 'variabl']",Global variable,isid function call global variabl defin outsid functionb10 def chang global b return bthen chang return 10
124,"Subtopics of Time Series Analysis

> Basics of Time Series

> Components of Time Series

> Multiplicative Model

> Ways to approach a Time Series Prediction Problem

> Naive Forecast

> Moving Average

> Weighted average

> Exponential smoothing

> Double exponential smoothing

> Econometric approach

> Assumption in Time Series

> Understanding ARIMA

> Understanding SARIMA

> Libraries for Time Series Analysis

> Machine learning approach in Time series

> Downside of SARIMA

> Understanding Leakage

> Prophet or Facebook Prophet

> Cross Validation in Time Series

> Demand pattern classification in time series

> Product forecastability

> Anomaly Detection in Time Series",subtop time seri analysi,"['subtop', 'time', 'seri', 'analysi']",Subtopics of Time Series Analysis,basic time seri compon time seri multipl model way approach time seri predict problem naiv forecast move averag weight averag exponenti smooth doubl exponenti smooth econometr approach assumpt time seri understand arima understand sarima librari time seri analysi machin learn approach time seri downsid sarima understand leakag prophet facebook prophet cross valid time seri demand pattern classif time seri product forecast anomali detect time seri
125,"Subtopics of Neural Networks

> Basics of Artificial Neural Network

> Difference between artificial intelligence and neural logic

> Perceptron, neuron, node, unit

> Model coefficients

> The components of neural network

> Backward propagation

> Understanding epochs

> Batch size and iteration

> Optimal accuracy in Neural Network

> Applications of Neural Networks

> RNN for machine translation

> Steps involved in a Neural Network

> Libraries for ANN model

> Define, Create and Compile ANN model

> fit the ANN model on the dataset

> evaluate the ANN model

> Plotting roc_curve",subtop neural network,"['subtop', 'neural', 'network']",Subtopics of Neural Networks,basic artifici neural network differ artifici intellig neural logic perceptron neuron node unit model coeffici compon neural network backward propag understand epoch batch size iter optim accuraci neural network applic neural network recurr neural network machin translat step involv neural network librari artifici neural network model defin creat compil artifici neural network model fit artifici neural network model dataset evalu artifici neural network model plot roccurv
126,"Subtopics of Deep Neural Networks

> Basic of Deep Neural Networks

> ANN, CNN and RNN

> Channels of RGB image

> Deeper understanding of deep neural network

> Loss function and Cost Function in DNN

> Writing sigmoid function

> Writing initialize weight function

> Writing propagate function

> Writing optimization function

> Writing predict function

> Computational graph

> Calculation behind gradient descent

> Need of vectorization

> Vectorized implementation of forward propagation for layer l

> Notations in DNN

> Element-wise matrix multiplication

> Notation for multiple layers

> Activation functions

> Linear and nonlinear activation function

> Keras and TensorFlow

> Problem of zero initialization

> Initializing parameters for the model

>  Deep Learning capabilities

> Layer dimension notation in Neural Network

> Weight notation in Neural Network

> Bias notation in Neural Network

> Network notation for i th experience

> Forward propagation and backpropagation

> shallow neural network",subtop deep neural network,"['subtop', 'deep', 'neural', 'network']",Subtopics of Deep Neural Networks,basic deep neural network ann convolut neural network rnn channel rgb imag deeper understand deep neural network loss function cost function dnn write sigmoid function write initi weight function write propag function write optim function write predict function comput graph calcul behind gradient descent need vector vector implement forward propag layer l notat dnn elementwis matrix multipl notat multipl layer activ function linear nonlinear activ function kera tensorflow problem zero initi initi paramet model deep learn capabl layer dimens notat neural network weight notat neural network bias notat neural network network notat th experi forward propag backpropag shallow neural network
127,"Subtopics of Improving Deep Neural Networks

> Basics of Improving Deep Neural Networks

> Basic 'recipe' for all machine learning models

> DNN model complexity

> Dropout regularization

> Implementing dropout

> Data Augmentation

> Early stopping technique to stop overfitting

> Importance of normalized inputs

> Exploding and Vanishing gradient

> Gradient checking

> Optimization Algorithms

> Mini-Batch gradient descent

> Notation of Mini-batch

> Choosing our mini-batch size

> Gradient Descent with momentum

> Exponentially weighted averages

> RMSprop

> Adam Optimization Algorithm

> Learning rate decay

> Problem of local optima

> Hyperparameters tuning in DNN

> Batch normalization

> Multi-class classification

> Deep learning programming framework

> Python coding for DNN

> Different vector operations in tensor flow

> Cross entropy loss function

> L2 regularization and weight decay",subtop improv deep neural network,"['subtop', 'improv', 'deep', 'neural', 'network']",Subtopics of Improving Deep Neural Networks,basic improv deep neural network basic recip machin learn model deep neural network model complex dropout regular implement dropout data augment earli stop techniqu stop overfit import normal input explod vanish gradient gradient check optim algorithm minibatch gradient descent notat minibatch choos minibatch size gradient descent momentum exponenti weight averag rmsprop adam optim algorithm learn rate decay problem local optima hyperparamet tune deep neural network batch normal multiclass classif deep learn program framework python code deep neural network differ vector oper tensor flow cross entropi loss function l2 regular weight decay
128,"Subtopics of Structuring ML Projects

> Basics Structuring ML project

> Orthogonalization Basics

> Fundamental assumptions of supervised learning

> Setting up our goal

> Changing dev/test sets and metrics

> Comparing to human level performance

> Human level error and avoidable bias

> Error Analysis

> Mismatched training and dev/test data

> Learning from multiple tasks

> Transfer learning

> Multi-task learning

> End to end deep learning

> Pros and cons of end to end deep learning

> Difference between Multi-class and multi-task learning

> Testing the model for the entire dataset",subtop structur machin learn project,"['subtop', 'structur', 'machin', 'learn', 'project']",Subtopics of Structuring ML Projects,basic structur machin learn project orthogon basic fundament assumpt supervis learn set goal chang devtest set metric compar human level perform human level error avoid bias error analysi mismatch train devtest data learn multipl task transfer learn multitask learn end end deep learn pros con end end deep learn differ multiclass multitask learn test model entir dataset
129,"Subtopics of Convolutional Neural Networks

> Perceptual task

> Basics of Pixel

> Convolution on Black-and-white Image

> Convolution operation

> Padding in CNN

> Strided Convolution

> Convolution on RGB image

> Types of layer in a convolutional network

> Pooling Layer

> Necessity of Convolution

> Calulating the size of output volume for convolution or pooling

> Notation for multiple CONV layers

> Common behavior of all the CNN architecture

> LeNet-5 architecture

> AlexNet architecture

> VGG-16 architecture

> ResNet and Inception architecture

> Network in Network

> Using open-source implementation

> Common Augmentation methods

> Sources of data for any ML model

> Object detection

> Tips for winning competitions

> Importing Kaggle dataset in colab

> Building blocks of deep learning model

> Common steps for pre-processing Image Data

> Difference between image classification and object detection

> Image Classification with Localization

> Landmark detection

> Convolutional implementation of sliding windows for detecting multiple objects 

> Object detection algorithm

> YOLO algorithm

> Steps in YOLO algorithm

> Intersection over union  algorithm

> Non-max supression

> anchor box algorithm

> Face Verification

> Face Recognition

> Siamese network

> Training set for calculating Triplet loss

> Neural Style Transfer

> Finding generated image

> Style and style matrix ",subtop convolut neural network,"['subtop', 'convolut', 'neural', 'network']",Subtopics of Convolutional Neural Networks,perceptu task basic pixel convolut blackandwhit imag convolut oper pad convolut neural network stride convolut convolut rgb imag type layer convolut network pool layer necess convolut calul size output volum convolut pool notat multipl conv layer common behavior convolut neural network architectur lenet5 architectur alexnet architectur vgg16 architectur resnet incept architectur network network use opensourc implement common augment method sourc data machin learn model object detect tip win competit import kaggl dataset colab build block deep learn model common step preprocess imag data differ imag classif object detect imag classif local landmark detect convolut implement slide window detect multipl object object detect algorithm yolo algorithm step yolo algorithm intersect union algorithm nonmax supress anchor box algorithm face verif face recognit siames network train set calcul triplet loss neural style transfer find generat imag style style matrix
130,"Subtopics of Recurrent Neural Network

> Examples of Models with sequence data

> Notation in RNN

> Vectorization of words

> Problem of a standard neural network in sequence data

> Summary of RNN

> Types of RNN

> Language Modelling

> Sampling or creating sequence

> Character-level language model

> Gated Recurrent Unit

> Long short-term memory

> Bidirectional RNN

> Deep RNN

> Natural language generation

> Huggingface Transformer

> Word Embedding

> Visualizing word embeddings

> Embedding matrix

> Transfer learning and word embeddings

> Learning word embeddings

> Context/target pairs

> Word2Vec model

> Problem with softmax classification

> Negative Sampling

> GloVe model

> The problem of bias in word embeddings

> Use of Pre-trained model for getting word embeddings

> Machine translation

> Beam Search

> Refinements to beam search

> BFS and DFS

> Error analysis on beam search

> Speech recognition

> Attention model

> Blue Score

> Basic Rule of CTC based technique

> Trigger word detection",subtop recurr neural network,"['subtop', 'recurr', 'neural', 'network']",Subtopics of Recurrent Neural Network,exampl model sequenc data notat rnn vector word problem standard neural network sequenc data summari rnn type rnn languag model sampl creat sequenc characterlevel languag model gate recurr unit long shortterm memori bidirect rnn deep rnn natur languag generat huggingfac transform word embed visual word embed embed matrix transfer learn word embed learn word embed contexttarget pair word2vec model problem softmax classif negat sampl glove model problem bias word embed use pretrain model get word embed machin translat beam search refin beam search breadth first search dfs error analysi beam search speech recognit attent model blue score basic rule ctc base techniqu trigger word detect
131,"Subtopics of Guesstimate

> Basics of case study

> Categorization of case studies

> Guesstimate (Problem Solving Approach)",subtop guesstim,"['subtop', 'guesstim']",Subtopics of Guesstimate,basic case studi categor case studi guesstim problem solv approach
132,"Subtopics of Case Study-ML in Heathcare

> Meaning of Diagnosis

> ML in Healthcare

> Geometric Mean Length of Stay

> Features for the prediction of LOS",subtop case studyml heathcar,"['subtop', 'case', 'studyml', 'heathcar']",Subtopics of Case Study-ML in Heathcare,mean diagnosi machin learn healthcar geometr mean length stay featur predict length stay
133,"Subtopics of Case Study-ML in Fraud risk analytics

> Understanding Fraud

> WAYS TO CAPTURE FRAUDULENT BEHAVIOURS

> Social Engineering Scams",subtop case studyml fraud risk analyt,"['subtop', 'case', 'studyml', 'fraud', 'risk', 'analyt']",Subtopics of Case Study-ML in Fraud risk analytics,understand fraud way captur fraudul behaviour social engin scam
134,"Subtopics of Case Study-ML in Credit Risk

> Curious Case of Customer Credit

> Overall Objective of Credit Risk

> Predictive Analytics of Credit Risk

> Prescriptive Analytics",subtop case studyml credit risk,"['subtop', 'case', 'studyml', 'credit', 'risk']",Subtopics of Case Study-ML in Credit Risk,curious case custom credit overal object credit risk predict analyt credit risk prescript analyt
135,"Subtopics of Case Study-ML in E-commerce

> Curious Case of Customer Contacts

> Important Features in E-commerce

>  Predictive Analytics in  E-commerce",subtop case studyml ecommerc,"['subtop', 'case', 'studyml', 'ecommerc']",Subtopics of Case Study-ML in E-commerce,curious case custom contact import featur ecommerc predict analyt ecommerc
136,"Subtopics of Linux basics and terminal commands

> Understanding UNIX Operating System

> Kernel and shell of UNIX

> Understanding LINUX Operating System

> Ways to connect to an EC2 instance

> Steps to connect to an EC2 instance

> Security of Remote Computer

> Git Bash Understanding

> Introduction to Repository

> Understanding IP address

> Basic Bash or Linux commands",subtop linux basic termin command,"['subtop', 'linux', 'basic', 'termin', 'command']",Subtopics of Linux basics and terminal commands,understand unix oper system kernel shell unix understand linux oper system way connect ec2 instanc step connect ec2 instanc secur remot comput git bash understand introduct repositori understand ip address basic bash linux command
137,"Subtopics of Python modules and project setup

> Few UNIX commands during project setup

> UNIX command in colab notebook

> File Permission Handling for security

> Creating a Simple Module

> Basics of modular programming

> One-Off Script Layout

> Installable Package Layout

> App with Internal Packages Layout

> Data Science Project Layout

> Using cookiecutter for packaging

> stdin, stdout and stderr

> Reading a static data file from inside a Python package

> Code for Creating python library

> Testing during packaging of our own python library",subtop python modul project setup,"['subtop', 'python', 'modul', 'project', 'setup']",Subtopics of Python modules and project setup,unix command project setup unix command colab notebook file permiss handl secur creat simpl modul basic modular program oneoff script layout instal packag layout app intern packag layout data scienc project layout use cookiecutt packag stdin stdout stderr read static data file insid python packag code creat python librari test packag python librari
138,"Understanding Production Grade Programming

Production environment is the setting where  products are actually put into operation for their intended uses by end users.

1. Object Oriented Programming (OOP)

2. Handling Errors and Exceptions

Production grade code is where all chances of error are taken care with try block and classes are created suitably",understand product grade program,"['understand', 'product', 'grade', 'program']",Understanding Production Grade Programming,product environ set product actual put oper intend use end users1 object orient program oop2 handl error exceptionsproduct grade code chanc error taken care tri block class creat suitabl
139,"Object Oriented Programming 

OOP is a programming language model organized around object or class rather than functions and logic

> Object or class is an identifiable entity with some characteristic and behavior.

> Instantiation in OOP means creating an instance of class",object orient program,"['object', 'orient', 'program']",Object Oriented Programming ,oop program languag model organ around object class rather function logic object class identifi entiti characterist behavior instanti oop mean creat instanc class
140,"Understanding Attribute

An attribute is a characteristic of a class.",understand attribut,"['understand', 'attribut']",Understanding Attribute,attribut characterist class
141,"Defining a class

Apart from the standard classes, we can create new class as and when required. We can define different attributes and methods inside the class with proper indendation

By convention we give classes a name that starts with a capital letter.

> Similar functions are grouped together in a class

class Cylinder:
  
  def __init__(self, radius=1, height=1):
    self.radius=radius
    self.height=height
    
  def volume(self):
    return 3.14 * ((self.radius)**2)* self.height

  def surface_area(self):
    return 2 * 3.14 * self.radius* self.height


Cylinder(4,5).radius gives 4

Cylinder(4,5).height gives 5

Cylinder(4,5).volume() gives 251.2

Cylinder(4,5).surface_area() gives 125.6

_init_ is used to define the attribute

radius=1, height=1 are the default values of the variables to avoid null error.",defin class,"['defin', 'class']",Defining a class,apart standard class creat new class requir defin differ attribut method insid class proper indendationbi convent give class name start capit letter similar function group togeth classclass cylind def initself radius1 height1 selfradiusradius selfheightheight def volumeself return 314 selfradius2 selfheight def surfaceareaself return 2 314 selfradius selfheight cylinder45radius give 4cylinder45height give 5cylinder45volum give 2512cylinder45surfacearea give 1256init use defin attributeradius1 height1 default valu variabl avoid null error
142,"Understanding Polymorphism

The word polymorphism means having many forms. In programming, polymorphism means the same function name (but different signatures) being used for different types.

For class, polymorphism refers to the way in which different object classes can share the same method name. Example:

class India():
    def capital(self):
        print(""New Delhi is the capital of India."")

class USA():
    def capital(self):
        print(""Washington, D.C. is the capital of USA."")

",understand polymorph,"['understand', 'polymorph']",Understanding Polymorphism,word polymorph mean mani form program polymorph mean function name differ signatur use differ typesfor class polymorph refer way differ object class share method name exampleclass india def capitalself printnew delhi capit indiaclass usa def capitalself printwashington dc capit usa
143,"Exception handling   

Errors detected during execution are called exceptions. There is full list of built in python exceptions-

https://docs.python.org/3/library/exceptions.html

input=""prabir""
try:
  # we do our operations here

  print(f'Your input number is greater than 5 with {input-5} points')
    
except:
  # If there is an exception, then execute this block

  print(""Sorry ! there is an exception"")
  
else:
  # If there is no exception then execute this block

  print(""The code is successfully run"")

> There can be one or more than one except statements in a try-except block

try:

   Code block here
   ...
   Due to any exception, this code may be skipped!

finally:

   This code block would always be executed.

> Finally can also be used with try and except block",except handl,"['except', 'handl']",Exception handling   ,error detect execut call except full list built python exceptionshttpsdocspythonorg3libraryexceptionshtmlinputprabir tri oper printfyour input number greater 5 input5 point except except execut block printsorri except els except execut block printth code success run one one except statement tryexcept blocktri code block due except code may skippedfin code block would alway execut final also use tri except block
144,"Single Underscore in the variable name

Single Underscore in the variable name(var_ ): Sometimes used as a name for temporary or insignificant variables",singl underscor variabl name,"['singl', 'underscor', 'variabl', 'name']",Single Underscore in the variable name,singl underscor variabl namevar sometim use name temporari insignific variabl
145,"create, read, update and delete

CRUD Meaning: CRUD is an acronym that comes from the world of computer programming and refers to the four functions that are considered necessary to implement a persistent storage application: create, read, update and delete.",creat read updat delet,"['creat', 'read', 'updat', 'delet']","create, read, update and delete",crud mean crud acronym come world comput program refer four function consid necessari implement persist storag applic creat read updat delet
146,"Double Leading and Trailing Underscore in the variable name

Double Leading and Trailing Underscore in the variable name( __var__ ): Indicates special methods defined by the Python language. 

",doubl lead trail underscor variabl name,"['doubl', 'lead', 'trail', 'underscor', 'variabl', 'name']",Double Leading and Trailing Underscore in the variable name,doubl lead trail underscor variabl name var indic special method defin python languag
147,"Subtopics of Version control-Git

> CVCS and DVCS

> GitHub workflow

> Steps of creating local git repository

> Concept of Branch

> Synchronising local git repository with Github

> Merging operation in git

> Continuous Integration/ Continuous Deployment or Delivery",subtop version controlgit,"['subtop', 'version', 'controlgit']",Subtopics of Version control-Git,central version control system dvcs github workflow step creat local git repositori concept branch synchronis local git repositori github merg oper git continu integr continu deploy deliveri
148,"Subtopics of API basics with flask

> Understanding API

> Types of web pages

> Machine Learning Services

> Types of API

> Basics of Flask

> Benefits of using the Flask framework

> Popular HTTP Requests

> Use of Virtual Environment

> Creating a virtual environment

> Understanding pip install

> Creating a Web App in Flask

> Jinja techniques

> Ways to edit script code (.py file) in Colab notebook

> Flaskr as a basic blog application

> Hyper Text Markup Language",subtop applic program interfac basic flask,"['subtop', 'applic', 'program', 'interfac', 'basic', 'flask']",Subtopics of API basics with flask,understand api type web page machin learn servic type api basic flask benefit use flask framework popular http request use virtual environ creat virtual environ understand pip instal creat web app flask jinja techniqu way edit script code py file colab notebook flaskr basic blog applic hyper text markup languag
149,"Subtopics of FastAPI

> Understanding FastAPI

> WSGI vs ASGI

> Features of FastAPI

> Pydantic Data Model

> FastAPI Working with SQL",subtop fastapi,"['subtop', 'fastapi']",Subtopics of FastAPI,understand fastapi web server gateway interfac vs asgi featur fastapi pydant data model fastapi work structur queri languag
150,"Subtopics of Docker

> Docker Basics

> Image or Docker Image

> Container or Docker Container

> Introduction to dockerhub

> Ways to install docker in EC2 instance

> Basic Docker Commands

> Docker or Container volume

> Docker Bridge Networking and Port Mapping

> Docker Compose

> Docker swarm",subtop docker,"['subtop', 'docker']",Subtopics of Docker,docker basic imag docker imag contain docker contain introduct dockerhub way instal docker ec2 instanc basic docker command docker contain volum docker bridg network port map docker compos docker swarm
151,"Subtopics of Microservices and streamlit

> Understanding Microservices

> API Gateway

> Kubernetes

> Computer Networking

> Development of Microservices

> Python code for creating a Streamlit file

> Creating a file for streamlit inside the session

> Creating a Streamlit Project

> Deploying streamlit app in Google Cloud

> Deploying streamlit app in AWS EC2",subtop microservic streamlit,"['subtop', 'microservic', 'streamlit']",Subtopics of Microservices and streamlit,understand microservic applic program interfac gateway kubernet comput network develop microservic python code creat streamlit file creat file streamlit insid session creat streamlit project deploy streamlit app googl cloud deploy streamlit app aw ec2
152,"Subtopics of ML Lifecycle

> Machine learning engineering

> 4 Phases of ML Lifecycle

> Challenges with ML during development

> Challenges with ML in production

> Data drift and Model drift

> Operational monitoring and functional monitoring in MLE",subtop machin learn lifecycl,"['subtop', 'machin', 'learn', 'lifecycl']",Subtopics of ML Lifecycle,machin learn engin 4 phase machin learn lifecycl challeng machin learn develop challeng machin learn product data drift model drift oper monitor function monitor machin learning
153,"Understanding Competitive coding

Competitive coding is important for interview to test coding aptitude

30-40% companies keep this competitive coding round in their interview process

> Sometimes candidates are not allowed to run the code on python interpreter or editor. Pen and paper test to check how we run the code on our head.",understand competit code,"['understand', 'competit', 'code']",Understanding Competitive coding,competit code import interview test code aptitude3040 compani keep competit code round interview process sometim candid allow run code python interpret editor pen paper test check run code head
154,"google colab notebook

To run a python code on cloud we can use google colab notebook (is the Jupyter notebook that run in the cloud and are highly integrated with Google Drive).",googl colab notebook,"['googl', 'colab', 'notebook']",google colab notebook,run python code cloud use googl colab notebook jupyt notebook run cloud high integr googl drive
155,"Code editor

Code editor tend to go for a broader approach and able to edit all types of files, instead of specializing in a particular type or language. Example of Code Editors are VS Code, PyCharm",code editor,"['code', 'editor']",Code editor,code editor tend go broader approach abl edit type file instead special particular type languag exampl code editor vs code pycharm
156,"Compiler

Compiler scans the entire program and translates the whole of it into machine code at once. ",compil,['compil'],Compiler,compil scan entir program translat whole machin code
157,"Interpreter 

Interpreter translates just one statement of the program at a time into machine code and takes very less time to analyse the code. 

cmd.exe is the default command-line interpreter for the OS/2, eComStation, ArcaOS, Microsoft Windows, and ReactOS operating systems. The name refers to its executable filename. It is also commonly referred to as cmd or the Command Prompt, referring to the default window title on Windows

",interpret,['interpret'],Interpreter ,interpret translat one statement program time machin code take less time analys code cmdex default commandlin interpret os2 ecomst arcao microsoft window reacto oper system name refer execut filenam also common refer cmd command prompt refer default window titl window
158,"IDLE

IDLE is Integrated Development and Learning Environment for Python. Example of IDE's are Spyder, Jupyter notebook

",integr develop learn environ,"['integr', 'develop', 'learn', 'environ']",IDLE,integr develop learn environ integr develop learn environ python exampl ide spyder jupyt notebook
159,"Anaconda and python

Python language was created in 1991

Van Rossum thought he needed a name that was short, unique, and slightly mysterious, so he decided to call the language Python. Python is a high level language.

Anaconda is the heaviest and the biggest snake in the world (can weight upto 550 pound or 250kg or more and length 25 feets or more). 

On the other hand, the python is no doubt the longest snake in the world(can be of length 33 feet or more).

To run a python code in local machine, we may install anaconda which installs python with many more libraries like numpy, pandas, matplotlib etc. and we can use jupyter notebook. ",anaconda python,"['anaconda', 'python']",Anaconda and python,python languag creat 1991van rossum thought need name short uniqu slight mysteri decid call languag python python high level languageanaconda heaviest biggest snake world weight upto 550 pound 250kg length 25 feet hand python doubt longest snake worldcan length 33 feet moreto run python code local machin may instal anaconda instal python mani librari like numpi panda matplotlib etc use jupyt notebook
160,"
Arthur Samuel and Geoffrey Everest Hinton

Arthur Samuel coined the term “Machine Learning” in 1952.

 Father of Machine Learning: Geoffrey Everest Hinton (for ANN)
",arthur samuel geoffrey everest hinton,"['arthur', 'samuel', 'geoffrey', 'everest', 'hinton']","
Arthur Samuel and Geoffrey Everest Hinton",arthur samuel coin term “machin learning” 1952 father machin learn geoffrey everest hinton ann
161,"Cloud computing or Cluster computing, Cloud Storage

Cloud (or Cluster) computing means the use of remote computer having remote storage (called cloud storage), OS and application softwares.

Cloud Storage examples: Google Drive, Amazon Drive (AWS S3) and Azure Storage. Google Drive offers 15 GB for free including emails & Google Photos and Amazon Drive (S3) gives we 5 GB for free.

> This remote computer is also called cloud server. Server (local or remote) is a computer connected to multiple users.",cloud comput cluster comput cloud storag,"['cloud', 'comput', 'cluster', 'comput', 'cloud', 'storag']","Cloud computing or Cluster computing, Cloud Storage",cloud cluster comput mean use remot comput remot storag call cloud storag oper system applic softwarescloud storag exampl googl drive amazon drive aw s3 azur storag googl drive offer 15 gb free includ email googl photoper system amazon drive s3 give 5 gb free remot comput also call cloud server server local remot comput connect multipl user
162,"Types of computer languages

High-Level Language (programmer-friendly, requires a compiler/interpreter to be translated into machine code)

Low-level language (machine-friendly,requires an assembler that would translate instructions)",type comput languag,"['type', 'comput', 'languag']",Types of computer languages,highlevel languag programmerfriend requir compilerinterpret translat machin codelowlevel languag machinefriendlyrequir assembl would translat instruct
163,"Jupyter notebook

Jupyter is a very popular application used for data analysis. 

It's an IPython notebook (""interactive python""). We can run each block of code separately. 

Python has two basic modes: script and interactive. The script mode is the mode where the scripted and finished .py files are run in the Python interpreter. Interactive mode is a command line shell which gives immediate feedback for each statement

Jupyter, comes from the core supported programming languages that it supports: Julia, Python, and R. There are many more languages that it supports. ",jupyt notebook,"['jupyt', 'notebook']",Jupyter notebook,jupyt popular applic use data analysi ipython notebook interact python run block code separ python two basic mode script interact script mode mode script finish py file run python interpret interact mode command line shell give immedi feedback statementjupyt come core support program languag support julia python r mani languag support
164,"
Notebook documents

Notebook documents are both human-readable documents containing the analysis description and the results (figures, tables, etc..) as well as executable documents which can be run to perform data analysis.",notebook document,"['notebook', 'document']","
Notebook documents",notebook document humanread document contain analysi descript result figur tabl etc well execut document run perform data analysi
165,"collaboration site

A collaboration site is used to support project teams, research groups, and other collaborative work. 

Teams, committees, and student groups may make announcements, engage in online discussions, and share resources within their collaboration sites.",collabor site,"['collabor', 'site']",collaboration site,collabor site use support project team research group collabor work team committe student group may make announc engag onlin discuss share resourc within collabor site
166,"Understanding of language

>> Basic data, data-structure (int, float, boolean, str, list, tuple, set, dict) and variables can be thought of basic words of python language

>> Syntax is grammer for the language

>> expression, conditional and statements are the sentences of the language

>> Loops, functions, methods and classes are some set of sentences to perform an operation on the data (experience stored in a computer)

>> Subjects like Social Science or Humanities, Science, Commerce and Technical are nothing but different features/ dimensions/ parameters/streams for understanding life experience logically. Every subject is again an experience for human being and thus have some special words or features or concepts

>> Libraries are subject specific. Thus every libraries have some special words, functions and methods and class.

>> Data scientists are mainly concerned with statistics related libraries to understand any experience (data) from logical dimension

>> The improvement of human intelligence is also based on understanding an experience logically. The first experience of human being is social science or humanites",understand languag,"['understand', 'languag']",Understanding of language,basic data datastructur int float boolean str list tupl set dict variabl thought basic word python languag syntax grammer languag express condit statement sentenc languag loop function method class set sentenc perform oper data experi store comput subject like social scienc human scienc commerc technic noth differ featur dimens parametersstream understand life experi logic everi subject experi human thus special word featur concept librari subject specif thus everi librari special word function method class data scientist main concern statist relat librari understand experi data logic dimens improv human intellig also base understand experi logic first experi human social scienc humanit
167,"Subtopics of General Knowledge for a Data Scientist

> google colab notebook

> Code editor

> Compiler

> Interpreter 

> IDLE

> Anaconda and python

> Arthur Samuel and Geoffrey Everest Hinton

> Cloud computing or Cluster computing, Cloud Storage

> Types of computer languages

> Jupyter notebook

> Notebook documents

> collaboration site

> Understanding of language",subtop general knowledg data scientist,"['subtop', 'general', 'knowledg', 'data', 'scientist']",Subtopics of General Knowledge for a Data Scientist,googl colab notebook code editor compil interpret idl anaconda python arthur samuel geoffrey everest hinton cloud comput cluster comput cloud storag type comput languag jupyt notebook notebook document collabor site understand languag
168,"Subtopics of Cloud Computing

> Cloud computing advantages

> Cloud service models

> Runtime system

> CPU vs GPU

> Azure machine learning platform

> Azure ML Workspace and Azure ML piplines

> Connecting to azure ml

> Different cloud deployment models",subtop cloud comput,"['subtop', 'cloud', 'comput']",Subtopics of Cloud Computing,cloud comput advantag cloud servic model runtim system central process unit vs gpu azur machin learn platform azur machin learn workspac azur machin learn piplin connect azur machin learn differ cloud deploy model
169,"Subtopics of MLOps-MLFlow

> DevOps and MLOps

> Agile software development

> Key components of MLOps

> Advantages of MLOps

> Key outcomes of MLOps

> Model registry and Metadata

> MLFLow Basics",subtop mlopsmlflow,"['subtop', 'mlopsmlflow']",Subtopics of MLOps-MLFlow,devop mlop agil softwar develop key compon mlop advantag mlop key outcom mlop model registri metadata mlflow basic
170,"Subtopics of PySpark

> Apache Spark (PySpark)

> Advantages and disadvantages of Spark

> Spark Modules

> RDD operations

> Anatomy of Spark Application

> Spark application coding

> Directed Acyclic Graph

> Azure Databricks

> Spark application deploy modes",subtop pyspark,"['subtop', 'pyspark']",Subtopics of PySpark,apach spark pyspark advantag disadvantag spark spark modul resili distribut dataset oper anatomi spark applic spark applic code direct acycl graph azur databrick spark applic deploy mode
171,"Subtopics of Airflow

> Apache Airflow

> Introduction to Jenkins

> Use of Apache Airflow

> Airflow Components

> Web Server and Scheduler

> Executor, Worker and Operator

> Metadata Database

> Applications of Airflow

> Understanding Celery",subtop airflow,"['subtop', 'airflow']",Subtopics of Airflow,apach airflow introduct jenkin use apach airflow airflow compon web server schedul executor worker oper metadata databas applic airflow understand celeri
172,"Understanding Library

A library is a collection of python packages with multiple programming blocks

Library means main folder

Top Python Libraries for Data Scientists : 

Numpy (stands for Numerical Python, released in 2005), 
Pandas (in 2008), 
SciPy (in 2001), 
Matplotlib (in 2003),
Seaborn (in 2018), 
Scikit-Learn (in 2007), 
TensorFlow (in 2015), 
Keras (in 2015), 
Theano, 
PyTorch,  
sys, 
time",understand librari,"['understand', 'librari']",Understanding Library,librari collect python packag multipl program blockslibrari mean main foldertop python librari data scientist numpi stand numer python releas 2005 panda 2008 scipi 2001 matplotlib 2003 seaborn 2018 scikitlearn 2007 tensorflow 2015 kera 2015 theano pytorch sys time
173,"Understanding Package

A package is a collection of Python modules

Package means sub folder

We may import the entire library (the main folder) or a particular package (a sub folder) or a particular module (a file) with dot(.) operator",understand packag,"['understand', 'packag']",Understanding Package,packag collect python modulespackag mean sub folderw may import entir librari main folder particular packag sub folder particular modul file dot oper
174,"Understanding Module

A module is a single Python file that consists of classes, functions, attributes and methods

Module means .py files

The term Library, Package or module are often used interchangeably, especially since many libraries only consist of a single module

Modules provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers. 

Modules also provide standardized solutions for many problems that occur in everyday programming.

some popular categories of Python modules are: 
1. operating system
2. math
3. time
4. data visualization ",understand modul,"['understand', 'modul']",Understanding Module,modul singl python file consist class function attribut methodsmodul mean py filesth term librari packag modul often use interchang especi sinc mani librari consist singl modulemodul provid access system function file io would otherwis inaccess python programm modul also provid standard solut mani problem occur everyday programmingsom popular categori python modul 1 oper system 2 math 3 time 4 data visual
175,"Writing softmax function with numpy (used in deep learning)

def softmax(x):

    x_exp = np.exp(x)

    x_sum = np.sum(x_exp, axis = 1, keepdims = True)

    s = np.divide(x_exp, x_sum)

    return s",write softmax function numpi use deep learn,"['write', 'softmax', 'function', 'numpi', 'use', 'deep', 'learn']",Writing softmax function with numpy (used in deep learning),def softmaxx xexp npexpx xsum npsumxexp axi 1 keepdim true npdividexexp xsum return
176,"Modular programming 

Modular programming refers to the process of breaking a large, unwieldy programming task into separate, smaller, more manageable subtasks or modules. 

Advantages are Simplicity, Maintainability, Reusability",modular program,"['modular', 'program']",Modular programming ,modular program refer process break larg unwieldi program task separ smaller manag subtask modul advantag simplic maintain reusabl
177,"N-dimensional array

The most important object defined in NumPy is an N-dimensional array type called ndarray.

> One dimensional array with 22 elements: size=(22)

> Two dimensional array with 4 lists each having 22 elements: size=(4,22)- similar like table with 4 rows and 22 columns

> Three dimensional array with 5 lists (dimension 1) each having 4 lists (dimension 2) each having 22 elements (dimension 3): size(5,4,22)",ndimension array,"['ndimension', 'array']",N-dimensional array,import object defin numpi ndimension array type call ndarray one dimension array 22 element size22 two dimension array 4 list 22 element size422 similar like tabl 4 row 22 column three dimension array 5 list dimens 1 4 list dimens 2 22 element dimens 3 size5422
178,"Creating a two-dimensional array 

import numpy as np

another_array = np.array([[1,2,5],[3,4,7]]) 

another_array.shape is (2,3). 

A full experience table can be stored in a 2D Numpy array. It may be of shape 1000k(rows),1k(column). The same may be done with nested list means a list having 1000k lists and each list haviing 1k elements",creat twodimension array,"['creat', 'twodimension', 'array']",Creating a two-dimensional array ,import numpi npanotherarray nparray125347 anotherarrayshap 23 full experi tabl store 2d numpi array may shape 1000krows1kcolumn may done nest list mean list 1000k list list havi 1k element
179,"attributes of numpy array

my_array.shape

my_array.ndim

my_array.size",attribut numpi array,"['attribut', 'numpi', 'array']",attributes of numpy array,myarrayshapemyarrayndimmyarrays
180,"Advantages of numpy array 

1. Numpy array takes very less space in memory as compared to list

2. Execution time for any operation with Numpy array is also very less as compared to list 

3. Functionality wise Numpy array is better than list ",advantag numpi array,"['advantag', 'numpi', 'array']",Advantages of numpy array ,1 numpi array take less space memori compar list2 execut time oper numpi array also less compar list 3 function wise numpi array better list
181,"Creating ndarray

>> create ndarray 

1. np.empty((5,3),dtype=int)

2. np.zeros((3,3))

3. np.ones((3,3))

4. np.random.random(size=(4,3))

5. np.random.randint(0,5,size=(2,3,4))

np.random.randn(2, 3) # Return a sample (or samples) from the “standard normal” distribution with shape (2,3)

6. np.full((3,3),30)- creates a (3,3) array full of 30's

>> create 1d array

np.arange(10,20,2)- 
np.linspace(1,50,10)",creat ndarray,"['creat', 'ndarray']",Creating ndarray,creat ndarray 1 npempty53dtypeint2 npzeros333 npones334 nprandomrandomsize435 nprandomrandint05size234nprandomrandn2 3 return sampl sampl “standard normal” distribut shape 236 npfull3330 creat 33 array full 30s creat 1d arraynparange10202 nplinspace15010
182,"Defining Array size

Size=(2) means an array or list with 2 elements

size=(2,3) means a list of 2 lists each having 3 elements

size=(2,3,4) means a list of 2 lists each having 3 lists each having 4 elements

array([[[1, 4, 3, 2],
        [1, 0, 2, 0],
        [0, 1, 3, 4]],

       [[4, 1, 2, 3],
        [0, 2, 0, 2],
        [2, 2, 4, 0]]])

(2,3,4) is actually the array shape and the size is 2*3*4 means 24, the total no. of elements

>> Array shape is defined as size in the syntax",defin array size,"['defin', 'array', 'size']",Defining Array size,size2 mean array list 2 elementssize23 mean list 2 list 3 elementssize234 mean list 2 list 3 list 4 elementsarray1 4 3 2 1 0 2 0 0 1 3 4 4 1 2 3 0 2 0 2 2 2 4 0234 actual array shape size 234 mean 24 total element array shape defin size syntax
183,"Normalizing rows (used in deep learning algorithm)

x = np.array([[1,2,5],[2,3,4]])

x_norm = np.linalg.norm(x, ord=None, axis=1, keepdims=True)

x_normalized = np.divide(x, x_norm)

> With keepdims=True the result will broadcast correctly against the original x.

>  axis=1 means you are going to get the norm in a row-wise manner. If you need the norm in a column-wise way, you would need to set axis=0.

> numpy.linalg.norm has another parameter ord where we specify the type of normalization to be done

> ord=None means 2-norm",normal row use deep learn algorithm,"['normal', 'row', 'use', 'deep', 'learn', 'algorithm']",Normalizing rows (used in deep learning algorithm),x nparray125234xnorm nplinalgnormx ordnon axis1 keepdimstruexnorm npdividex xnorm keepdimstru result broadcast correct origin x axis1 mean go get norm rowwis manner need norm columnwis way would need set axis0 numpylinalgnorm anoth paramet ord specifi type normal done ordnon mean 2norm
184,"Conversion of list and tuple to ndarray

np.array(my_list or my_tuple)

 np.array(my_list, ndmin = 2)",convers list tupl ndarray,"['convers', 'list', 'tupl', 'ndarray']",Conversion of list and tuple to ndarray,nparraymylist mytupl nparraymylist ndmin 2
185,"Indexing and Slicing of ndarray

> Indexing and Slicing ndarray is same as list

slicing of 2D array

my_array[2:5,0:3]

my_array[2:5,3]

my_array[2:5]",index slice ndarray,"['index', 'slice', 'ndarray']",Indexing and Slicing of ndarray,index slice ndarray listslic 2d arraymyarray2503myarray253myarray25
186,"
Grabbing element by index of 2D array

my_array[0,9] or my_array[0][9]

Grabbing element with .flat
my_array.flat[5]",grab element index 2d array,"['grab', 'element', 'index', '2d', 'array']","
Grabbing element by index of 2D array",myarray09 myarray09grab element flat myarrayflat5
187,"Array Manipulation

np.reshape(my_array,new_shape) or my_array.reshape(new_shape)

> my_array.reshape(-1, 1) -It mean, if we have an array of shape (2,4) then reshaping it with (-1, 1), then the array will get reshaped in such a way that the resulting array has only 1 column and this is only possible by having 8 rows, hence, (8,1)

np.resize(my_array,shape_with_new_size) or my_array.resize(shape_with_new_size)

np.transpose(my_array) or my_array.transpose() or my_array.T

my_array.flatten()

np.stack((array_1, array_2), axis=0)

np.fill_diagonal(my_matrix,5)

my_array.dtype=""float64""",array manipul,"['array', 'manipul']",Array Manipulation,npreshapemyarraynewshap myarrayreshapenewshap myarrayreshape1 1 mean array shape 24 reshap 1 1 array get reshap way result array 1 column possibl 8 row henc 81npresizemyarrayshapewithnews myarrayresizeshapewithnewsizenptransposemyarray myarraytranspos myarraytmyarrayflattennpstackarray1 array2 axis0npfilldiagonalmymatrix5myarraydtypefloat64
188,"Important functions in numpy

1. np.insert(my_array, location_index, insert_value, axis=0),

2. np.append(my_array, array_to_append, axis=1)- array_to_append should match no. of elements in all  dimension except axis=1

3. np.delete(my_array, location_index, axis=2),

4. np.unique(my_array, return_index=True, return_counts=True)--returns a tuple of arrays

>> axis=0 is the 1st dim or minor axis and signifies rows, axis=1 is the 2nd dim or major axis and signifies columns",import function numpi,"['import', 'function', 'numpi']",Important functions in numpy,1 npinsertmyarray locationindex insertvalu axis02 npappendmyarray arraytoappend axis1 arraytoappend match element dimens except axis13 npdeletemyarray locationindex axis24 npuniquemyarray returnindextru returncountstruereturn tupl array axis0 1st dim minor axi signifi row axis1 2nd dim major axi signifi column
189,"Difference between view and copy in numpy

view

a = np.array([2,34,12])
b = a
b[0] = -999 
print(b)
print(a)

[-999   34   12]
[-999   34   12]

copy

a = np.array([2,34,12])
b = a.copy()
b[0] = -999
print(b)
print(a)

[-999   34   12]
[ 2 34 12]",differ view copi numpi,"['differ', 'view', 'copi', 'numpi']",Difference between view and copy in numpy,viewa nparray23412 b b0 999 printb printa999 34 12 999 34 12copya nparray23412 b acopi b0 999 printb printa999 34 12 2 34 12
190,"Basic Operations & Functions in numpy

a=array_1

b=array_2

np.add(a,b),

np.subtract(a,b),

np.absolute(a-b)

np.multiply(a,b),

np.divide(a,b),

np.sum(a),

np.square(a),

np.min(a,axis=2),

np.max(a,axis=1),

np.mean(a,axis=0),

np.median(a,axis=0)

np.var(a,axis=0),

np.sort(a,axis=0),

my_array.astype('float64')

np.argsort(a,axis=0)-returns the indices of sorted array,

np.argmin(a,axis=0)-returns the index of the minimum element 

np.where(conditional_statement) # This can be used for finding index of any element in numpy

np.where(a == a.max())[0][0]

my_array.tolist()- or list(my_array)

np.percentile(my_df['column'],50)--returns the value for 50th percentile or median
",basic oper function numpi,"['basic', 'oper', 'function', 'numpi']",Basic Operations & Functions in numpy,aarray1barray2npaddabnpsubtractabnpabsoluteabnpmultiplyabnpdivideabnpsumanpsquareanpminaaxis2npmaxaaxis1npmeanaaxis0npmedianaaxis0npvaraaxis0npsortaaxis0myarrayastypefloat64npargsortaaxis0return indic sort arraynpargminaaxis0return index minimum element npwhereconditionalstat use find index element numpynpwherea amax00myarraytolist listmyarraynppercentilemydfcolumn50return valu 50th percentil median
191,"Exponentiation of vectors (used in deep learning algorithm)

>> For vectors we use numpy library for exponentiation

import numpy as np

t_x = np.array([1, 2, 3])

print(np.exp(t_x)) # result is (exp(1), exp(2), exp(3))

sigmoid_t_x = 1/(1+ np.exp(-t_x))

>> For real numbers we use math library for exponentiation

import math
from public_tests import *

x = 0 
sigmoid_x = 1/(1+math.exp(-x)) will return 0.5",exponenti vector use deep learn algorithm,"['exponenti', 'vector', 'use', 'deep', 'learn', 'algorithm']",Exponentiation of vectors (used in deep learning algorithm),vector use numpi librari exponentiationimport numpi nptx nparray1 2 3printnpexptx result exp1 exp2 exp3sigmoidtx 11 npexptx real number use math librari exponentiationimport math publictest import x 0 sigmoidx 11mathexpx return 05
192,"Basics of Pandas Dataframe

The name pandas is derived from the term “panel data”. It is for doing practical, real world data analysis in Python

Pandas is designed to make it easier to work with structured data i.e. organised experience set.

The DataFrame object in pandas is ""a two-dimensional tabular, column-oriented data structure with both row and column labels."" (excel is a similar datastructure)

A DataFrame is like a fixed-size dict in that we can get and set values by index label

Due to its inherent tabular structure, pandas dataframes also allow cells to have null values (i.e. no data value such as blank space, NaN(Not a Number), -999, etc).",basic panda datafram,"['basic', 'panda', 'datafram']",Basics of Pandas Dataframe,name panda deriv term “panel data” practic real world data analysi pythonpanda design make easier work structur data ie organis experi setth datafram object panda twodimension tabular columnori data structur row column label excel similar datastructurea datafram like fixeds dict get set valu index labeldu inher tabular structur panda datafram also allow cell null valu ie data valu blank space nannot number 999 etc
193,"Difference between list and numpy array

Python lists are flexible and can store data items of various types (e.g. integers, floats, text strings). List can be converted to numpy array.

However, numpy arrays generally deals with numerical data. Thus, numpy arrays can provide more functionality for running calculations such as element-by-element arithmetic operations",differ list numpi array,"['differ', 'list', 'numpi', 'array']",Difference between list and numpy array,python list flexibl store data item various type eg integ float text string list convert numpi arrayhowev numpi array general deal numer data thus numpi array provid function run calcul elementbyel arithmet oper
194,"Rows and columns of pandas dataframe

The columns in pandas DataFrames can be different types (e.g. the first column containing integers and the second column containing text strings). Each value in pandas dataframe is referred to as a cell that has a specific row index and column index within the tabular structure.",row column panda datafram,"['row', 'column', 'panda', 'datafram']",Rows and columns of pandas dataframe,column panda datafram differ type eg first column contain integ second column contain text string valu panda datafram refer cell specif row index column index within tabular structur
195,"Pandas series

We can creare pandas series.

number_series = pd.Series([2, 3, 5, 6, 8])

Pandas series is nothing but a 1D numpy array",panda seri,"['panda', 'seri']",Pandas series,crear panda seriesnumberseri pdseries2 3 5 6 8panda seri noth 1d numpi array
196,"Convertion of numpy array to pandas dataframe

We can convert a 2D numpy array, list of lists, dictionary, list of dicts, list of tuples into pandas DataFrame.

1. pd.DataFrame(some_2D_array , columns = ['Name', 'Age'])

2. pd.DataFrame(list_of_lists, columns = ['Name', 'Age'])

3. pd.DataFrame([employee_dict])

4. pd.DataFrame(list_of_dicts)

5. pd.DataFrame(list_of_tuples, columns = ['Name', 'Age'])",convert numpi array panda datafram,"['convert', 'numpi', 'array', 'panda', 'datafram']",Convertion of numpy array to pandas dataframe,convert 2d numpi array list list dictionari list dict list tupl panda dataframe1 pddataframesome2darray column name age2 pddataframelistoflist column name age3 pddataframeemployeedict4 pddataframelistofdicts5 pddataframelistoftupl column name age
197,"For uploading any file from local computer to the colab session

from google.colab import files
upload=files.upload()",upload file local comput colab session,"['upload', 'file', 'local', 'comput', 'colab', 'session']",For uploading any file from local computer to the colab session,googlecolab import file uploadfilesupload
198,"Methods and attributes of pandas DataFrame 

1. my_df.index

2. my_df.columns

3. my_df.head()

4. my_df.tail()

5. my_df.info()

6. my_df.rename(columns = {'Name':'Actor Name','Age' :'Actor Age'}, inplace=True)

or,
my_df.columns = list_of_new column_names

7. my_df.shape

8. my_df.describe() 

or my_df.groupby('column_name').describe()

9. my_df.values

10. my_df.keys() or  my_df.columns 

11.  
my_df.replace(r'?', np.NaN)  

my_df['col_name'].replace([np.inf, -np.inf], 0, inplace=True)

my_df['col_name'].replace(np.nan, my_df['col_name'].median(), inplace=True)

>> r string stands for 'raw string'

>> u string stands for 'unicode string'

>> np.inf means numpy infinite number

12. my_df.dropna(inplace=True) is called for removing the rows which contains the null values

13. my_df.sample(frac=0.3) -for 30% random sampling of the data",method attribut panda datafram,"['method', 'attribut', 'panda', 'datafram']",Methods and attributes of pandas DataFrame ,1 mydfindex2 mydfcolumns3 mydfhead4 mydftail5 mydfinfo6 mydfrenamecolumn nameactor nameag actor age inplacetrueor mydfcolumn listofnew columnnames7 mydfshape8 mydfdescrib mydfgroupbycolumnnamedescribe9 mydfvalues10 mydfkey mydfcolumn 11 mydfreplac npnan mydfcolnamereplacenpinf npinf 0 inplacetruemydfcolnamereplacenpnan mydfcolnamemedian inplacetru r string stand raw string u string stand unicod string npinf mean numpi infinit number12 mydfdropnainplacetru call remov row contain null values13 mydfsamplefrac03 30 random sampl data
199,"Connecting google drive to colab notebook

from google.colab import drive
drive.mount('/content/drive')

-After mounting the drive in colab, we can  read a file/create a file/write in a file/append in a file/edit a file(csv/excel) 

",connect googl drive colab notebook,"['connect', 'googl', 'drive', 'colab', 'notebook']",Connecting google drive to colab notebook,googlecolab import drive drivemountcontentdriveaft mount drive colab read filecr filewrit fileappend fileedit filecsvexcel
200,"Converting a csv file to pandas DataFrame

my_df=pd.read_csv('file_path')

or 
data = pd.read_csv(open_source_csv_url)

> url is also a file_path

> When data is in latin language,

my_df=pd.read_csv('file_path', encoding='latin-1')

For creating a duplicate copy of the dataframe

new_df = my_df.copy()",convert csv file panda datafram,"['convert', 'csv', 'file', 'panda', 'datafram']",Converting a csv file to pandas DataFrame,mydfpdreadcsvfilepathor data pdreadcsvopensourcecsvurl url also filepath data latin languagemydfpdreadcsvfilepath encodinglatin1for creat duplic copi dataframenewdf mydfcopi
201,"Creating an empty csv file on a specific folder

my_df = pd.DataFrame(list())

my_df.to_csv('folder_path/name_of_empty_csv.csv')
",creat empti csv file specif folder,"['creat', 'empti', 'csv', 'file', 'specif', 'folder']",Creating an empty csv file on a specific folder,mydf pddataframelistmydftocsvfolderpathnameofemptycsvcsv
202,"Writing data in a csv file

1. Create the csv writer

import csv

writer = csv.writer(open('file_path', 'w'))

2. Define variables

header=['Name','Weight','Height']

data=[['Ram',600,10],['Sham',400,20],['Jadu',200,10]]

3. Write multiple rows to the csv file

writer.writerow(header)

writer.writerows(data)",write data csv file,"['write', 'data', 'csv', 'file']",Writing data in a csv file,1 creat csv writerimport csvwriter csvwriteropenfilepath w2 defin variablesheadernameweightheightdataram60010sham40020jadu200103 write multipl row csv filewriterwriterowheaderwriterwriterowsdata
203,"Appending new row to an exsisting csv file

import csv

writer = csv.writer(open('file_path', 'a'))

data=['Madhu',800,90]

writer.writerow(data)",append new row exsist csv file,"['append', 'new', 'row', 'exsist', 'csv', 'file']",Appending new row to an exsisting csv file,import csvwriter csvwriteropenfilepath adatamadhu80090writerwriterowdata
204,"Editing a csv file

> Read the csv as pandas df and then do all the editing and then save it to csv

>> Reading a json file as pandas df

my_df=pd.read_json('file_path')",edit csv file,"['edit', 'csv', 'file']",Editing a csv file,read csv panda df edit save csv read json file panda dfmydfpdreadjsonfilepath
205,"Excel file with multiple worksheets

xls = pd.ExcelFile('file_path')

xls.sheet_names

df1 = pd.read_excel(xls, 'sheet_name')

Reading an excel file first sheet directly

df = pd.read_excel ('file_path')",excel file multipl worksheet,"['excel', 'file', 'multipl', 'worksheet']",Excel file with multiple worksheets,xls pdexcelfilefilepathxlssheetnamesdf1 pdreadexcelxl sheetnameread excel file first sheet directlydf pdreadexcel filepath
206,"Writing in a new excel file

import xlwt
  
my_workbook = xlwt.Workbook() 

sheet1 = my_workbook.add_sheet(""sheet_name"")
  
font_style = xlwt.easyxf('font: bold 1,color red;')
  
sheet1.write(0, 0, 'JADU', font_style)

my_workbook.save(""file_path/file_name.xls"")",write new excel file,"['write', 'new', 'excel', 'file']",Writing in a new excel file,import xlwt myworkbook xlwtworkbook sheet1 myworkbookaddsheetsheetnam fontstyl xlwteasyxffont bold 1color red sheet1write0 0 jadu fontstylemyworkbooksavefilepathfilenamexl
207,"Editing in pandas df and then save them to csv/excel

my_df = pd.DataFrame([['Ram',600,10],['Sham',400,20],['Jadu',200,10]], columns=['Name','Weight','Height'])

my_df.to_csv('/content/drive/MyDrive/my_data.csv')

or,

my_df.to_excel('/content/drive/MyDrive/my_data.xlsx')",edit panda df save csvexcel,"['edit', 'panda', 'df', 'save', 'csvexcel']",Editing in pandas df and then save them to csv/excel,mydf pddataframeram60010sham40020jadu20010 columnsnameweightheightmydftocsvcontentdrivemydrivemydatacsvormydftoexcelcontentdrivemydrivemydataxlsx
208,"Slicing operation on Pandas DataFrame

my_df.iloc[:,-4:] or my_df.iloc[:, 7]  slicing operation both on rows and columns by index

my_df.loc[list_of_index_name] -slicing by index name

my_df.loc[:, list_of_columns] -slicing operation on rows by index and on columns by column names

my_df[0:10] -slicing operation only on rows

my_df[list_of_columns] -slicing operation only on columns",slice oper panda datafram,"['slice', 'oper', 'panda', 'datafram']",Slicing operation on Pandas DataFrame,mydfiloc4 mydfiloc 7 slice oper row column indexmydfloclistofindexnam slice index namemydfloc listofcolumn slice oper row index column column namesmydf010 slice oper rowsmydflistofcolumn slice oper column
209,"Slicing operation on a particular column  in Pandas DataFrame

my_df['column_name'][0:5]

type(my_df['column_name']) is pandas series

Writing any value at a particular cell

my_df['column_name'][row_index]=value -This is similar like cell reference in excel",slice oper particular column panda datafram,"['slice', 'oper', 'particular', 'column', 'panda', 'datafram']",Slicing operation on a particular column  in Pandas DataFrame,mydfcolumnname05typemydfcolumnnam panda serieswrit valu particular cellmydfcolumnnamerowindexvalu similar like cell refer excel
210,"Conditional Slicing or Conditional Filtering in  Pandas DataFrame

>> Normal Filtering with single selection on single or multiple columns

my_df.loc[(my_df['new_cast']=='Robert De Niro')]

my_df.loc[(my_df['new_cast']=='Robert De Niro') & (my_df['original_language']=='en')]

>> Normal Filtering with multiple selection on single column

my_df.loc[(my_df['original_language']=='en') | (my_df['original_language']=='fr')]

>> Conditional Filtering on single or multiple columns in  Pandas DataFrame

long_movies = my_df[my_df['runtime'] > 120]

long_english_movies = my_df[(my_df['original_language']=='en') & (df['runtime'] > 120)]",condit slice condit filter panda datafram,"['condit', 'slice', 'condit', 'filter', 'panda', 'datafram']",Conditional Slicing or Conditional Filtering in  Pandas DataFrame,normal filter singl select singl multipl columnsmydflocmydfnewcastrobert de niromydflocmydfnewcastrobert de niro mydforiginallanguageen normal filter multipl select singl columnmydflocmydforiginallanguageen mydforiginallanguagefr condit filter singl multipl column panda dataframelongmovi mydfmydfruntim 120longenglishmovi mydfmydforiginallanguageen dfruntim 120
211,"Adding new column in existing df

my_df['half_runtime'] = 250 
or 
my_df.half_runtime = [list_with_same_len]

my_df['half_runtime'] = my_df['runtime'] * 0.5

my_df['movie_profit'] = my_df['revenue'] - df['budget']

>> We can also add rows/columns by concatination

>> Adding new row in existing df

my_df.loc[(my_df.index.max()+1)] =list_of_values_for columns ",ad new column exist df,"['ad', 'new', 'column', 'exist', 'df']",Adding new column in existing df,mydfhalfruntim 250 mydfhalfruntim listwithsamelenmydfhalfruntim mydfruntim 05mydfmovieprofit mydfrevenu dfbudget also add rowscolumn concatin ad new row exist dfmydflocmydfindexmax1 listofvaluesfor column
212,"Removing one or multiple columns

my_df.drop(list_of_columns, axis = 1, inplace=True)
",remov one multipl column,"['remov', 'one', 'multipl', 'column']",Removing one or multiple columns,mydfdroplistofcolumn axi 1 inplacetru
213,"Removing one or multiple rows

my_df=my_df.drop(my_df.index[list_of_index]).reset_index()

my_df.drop(""index"", axis = 1, inplace=True) - This is to remove the extra 'index' column created during removing rows

>> We can also remove rows/columns by slicing",remov one multipl row,"['remov', 'one', 'multipl', 'row']",Removing one or multiple rows,mydfmydfdropmydfindexlistofindexresetindexmydfdropindex axi 1 inplacetru remov extra index column creat remov row also remov rowscolumn slice
214,"Setting a column as row index

my_df.set_index('column_name', inplace=True)

inplace=True for parmanent change
",set column row index,"['set', 'column', 'row', 'index']",Setting a column as row index,mydfsetindexcolumnnam inplacetrueinplacetru parman chang
215,"Methods for particular column

1. my_df['column_name'].apply(any_function)

2. list(my_df['column_name'].unique())

3. my_df.column_name.nunique()

4. my_df['column_name'].value_counts()

5. my_df['column_name'].isnull()

6. my_df['column_name']= my_df['column_name'].fillna(0)- works when there is np.NaN values

>> .isna() and .notna() functions are also useful

>>  Checking zero values
if 0 in my_df.values returns T/F

To get the total null values in the entire dataset

print(my_df.isnull().sum())",method particular column,"['method', 'particular', 'column']",Methods for particular column,1 mydfcolumnnameapplyanyfunction2 listmydfcolumnnameunique3 mydfcolumnnamenunique4 mydfcolumnnamevaluecounts5 mydfcolumnnameisnull6 mydfcolumnnam mydfcolumnnamefillna0 work npnan valu isna notna function also use check zero valu 0 mydfvalu return tfto get total null valu entir datasetprintmydfisnullsum
216,"Sort values in  Pandas DataFrame

my_df.sort_values('column_name', ascending=False)

my_df.sort_values(['column_name1','column_name2'], ascending=[False,True])

Sorting by index
my_df.sort_index(inplace=True)

> For sorting series

my_df.groupby([col])[col].median().sort_values(ascending=False)",sort valu panda datafram,"['sort', 'valu', 'panda', 'datafram']",Sort values in  Pandas DataFrame,mydfsortvaluescolumnnam ascendingfalsemydfsortvaluescolumnname1columnname2 ascendingfalsetruesort index mydfsortindexinplacetru sort seriesmydfgroupbycolcolmediansortvaluesascendingfals
217,"Datetime operations

from datetime import datetime

from datetime import date

from datetime import timedelta

strptime means string parser, this will convert a string format to datetime.

strftime means string formatter, this will convert a datetime to string format.",datetim oper,"['datetim', 'oper']",Datetime operations,datetim import datetimefrom datetim import datefrom datetim import timedeltastrptim mean string parser convert string format datetimestrftim mean string formatt convert datetim string format
218,"Creating a new column with lambda function (applied on pandas series)

my_df['new_column_name']=my_df['exsisting_column_name'].apply(lambda x : datetime.strptime(x,'%m/%d/%Y'))

my_df['new_column_name'] = my_df['column_name'].apply(lambda x : 1 if x == ""Yes"" else 0)

A lambda function is just like any normal python function, except that it has no name when defining it.

lambda input_variables : output expression

add_lambda = lambda a, b: a + b

add_lambda(4,5) will give the result

> To extract the year

pd.DatetimeIndex(my_df['datetime_col']).year

>>
df['column_name'] = df['column_name'].apply(lambda x:[i['name'] for i in x] if isinstance(x,list) else [])

>> isinstance() function returns True if the specified object is of the specified type, otherwise False ",creat new column lambda function appli panda seri,"['creat', 'new', 'column', 'lambda', 'function', 'appli', 'panda', 'seri']",Creating a new column with lambda function (applied on pandas series),mydfnewcolumnnamemydfexsistingcolumnnameapplylambda x datetimestrptimexmdymydfnewcolumnnam mydfcolumnnameapplylambda x 1 x yes els 0a lambda function like normal python function except name defin itlambda inputvari output expressionaddlambda lambda b baddlambda45 give result extract yearpddatetimeindexmydfdatetimecolyear dfcolumnnam dfcolumnnameapplylambda xinam x isinstancexlist els isinst function return true specifi object specifi type otherwis fals
219,"Pandas data display options

pd.options.display.float_format = '{:.4f}'.format",panda data display option,"['panda', 'data', 'display', 'option']",Pandas data display options,pdoptionsdisplayfloatformat 4fformat
220,"Difference among and & |

if a==5 and b==10:
  print('mango') - This ""and"" operator is used along with if statement and returns boolean value

[(column_a==5) & (coumn_b==10)] - During conditional slicing of Pandas DataFrame, it returns the result when both the conditions are true

[(column_a==5) | (coumn_b==10)] - During conditional slicing of Pandas DataFrame, it returns the result when first condition is true and then returns the result when second condition is true",differ among,"['differ', 'among']",Difference among and & |,a5 b10 printmango oper use along statement return boolean valuecolumna5 coumnb10 condit slice panda datafram return result condit truecolumna5 coumnb10 condit slice panda datafram return result first condit true return result second condit true
221,"Use of Unary operator, ~

Unary means involving a single component

~ reverses an integer or boolean value",use unari oper,"['use', 'unari', 'oper']","Use of Unary operator, ~",unari mean involv singl compon revers integ boolean valu
222,"Another way to convert dict to df

pd.DataFrame.from_dict(dictionary_object, orient='index')",anoth way convert dict df,"['anoth', 'way', 'convert', 'dict', 'df']",Another way to convert dict to df,pddataframefromdictdictionaryobject orientindex
223,"Converting pandas df to numpy array

my_df.to_numpy()",convert panda df numpi array,"['convert', 'panda', 'df', 'numpi', 'array']",Converting pandas df to numpy array,mydftonumpi
224,"String to numeric in  Pandas DataFrame

my_df[list_of columns]=my_df[list_of columns].astype('float64')

or

pd.to_numeric(my_df[list_of columns])",string numer panda datafram,"['string', 'numer', 'panda', 'datafram']",String to numeric in  Pandas DataFrame,mydflistof columnsmydflistof columnsastypefloat64orpdtonumericmydflistof column
225,"Numpy array or pandas df to matrix convertion

my_matrix = np.asmatrix(my_array or my_df)",numpi array panda df matrix convert,"['numpi', 'array', 'panda', 'df', 'matrix', 'convert']",Numpy array or pandas df to matrix convertion,mymatrix npasmatrixmyarray mydf
226,"Subtopics of computer science and machine learning

> Understanding computer science

> Understanding convensional programming

> Types of Computer languages

> Programming language  vs scripting language

> AI technique

> Basics of machine learning

> Basics of deep learning

> Basics of data science

> Role of a data scientist

> Difference between Business Analyst and Data Scientist

> Predictive ML model

> Meaning of heuristic technique

> Comparison between Heuristic technique and ML technique

> Types of learning models

> List of ML models

> Application of regression model

> Application of classification model

> Application of Clustering model

> Components of reinforcement learning

> Application of Reinforcement learning model",subtop comput scienc machin learn,"['subtop', 'comput', 'scienc', 'machin', 'learn']",Subtopics of computer science and machine learning,understand comput scienc understand convension program type comput languag program languag vs script languag artifici intellig techniqu basic machin learn basic deep learn basic data scienc role data scientist differ busi analyst data scientist predict machin learn model mean heurist techniqu comparison heurist techniqu machin learn techniqu type learn model list machin learn model applic regress model applic classif model applic cluster model compon reinforc learn applic reinforc learn model
227,"Basics of Data wrangling

 Data wrangling is also known as data munging. Data wrangling is the process of cleaning and unifying messy and complex data sets for easy access and analysis.

Even though the methodologies are similar, data wrangling and data cleaning are two distinct procedures. Data cleaning focuses on removing erroneous data from your data set. In contrast, data-wrangling focuses on changing the data format by translating “raw” data into a more usable form. ",basic data wrangl,"['basic', 'data', 'wrangl']",Basics of Data wrangling,data wrangl also known data mung data wrangl process clean unifi messi complex data set easi access analysiseven though methodolog similar data wrangl data clean two distinct procedur data clean focus remov erron data data set contrast datawrangl focus chang data format translat “raw” data usabl form
228,"Concatenating pandas DataFrame

pd.concat(list_of_df)-concats both axis

pd.concat(list_of_df, axis=1)-Concatenate along axis 1- means columnwise",concaten panda datafram,"['concaten', 'panda', 'datafram']",Concatenating pandas DataFrame,pdconcatlistofdfconcat axispdconcatlistofdf axis1concaten along axi 1 mean columnwis
229,"DataFrame merging operation through joins

1. Inner join

2. Left join

3. Right join

4. Outer join or Full join

pd.merge(english_movies, long_movies, how='left',left_on='imdb_id',right_on='imdb_id')

> Self Join

A self join allows you to join a table to itself. It helps query hierarchical data or compare values in a column to other values in the same column same table. A self join uses the inner join or left join clause.

pd.merge(my_df, my_df, how='inner',left_on=col_1,right_on=col_1)

> default join is inner for pd.merge

> df1.join(df2)  is a left join by default",datafram merg oper join,"['datafram', 'merg', 'oper', 'join']",DataFrame merging operation through joins,1 inner join2 left join3 right join4 outer join full joinpdmergeenglishmovi longmovi howleftleftonimdbidrightonimdbid self joina self join allow join tabl help queri hierarch data compar valu column valu column tabl self join use inner join left join clausepdmergemydf mydf howinnerleftoncol1rightoncol1 default join inner pdmerg df1joindf2 left join default
230,"Groupby operation on pandas dataframe for data analysis

my_df.groupby('Year_of_release')['runtime'].max().reset_index()

my_df.groupby('Year_of_release')['imdb_id'].count().median()

grouped_actors = new_df.groupby('new_cast').agg({'new_genre':'sum','profit':'mean'}).reset_index()

agg() function is used to pass a function or list of functions to be applied

df.groupby([list_of_columns])['column_name'].median()",groupbi oper panda datafram data analysi,"['groupbi', 'oper', 'panda', 'datafram', 'data', 'analysi']",Groupby operation on pandas dataframe for data analysis,mydfgroupbyyearofreleaseruntimemaxresetindexmydfgroupbyyearofreleaseimdbidcountmediangroupedactor newdfgroupbynewcastaggnewgenresumprofitmeanresetindexagg function use pass function list function applieddfgroupbylistofcolumnscolumnnamemedian
231,"Detailed EDA

1. Connection with the Data:

2. First Feelings of the Data:

We can not see the whole of an extreamly large dataset. 

Therefore to understand the dataset we see the head(), tail() and shape of the dataset.

3. Deeper Understanding of the Data:

Then we try to understand the dataset from info() (missing values or np.NaN values, others strings like '?', '-', 'Not available' etc. and also check the type of values, wheather list, tuple, set or dict is available as string) and describe() methods

4. Cleaning the Data:

Then we perform data cleaning and simplify the data (converting others strings like '?', '-', 'Not available' etc. to np.NaN) and may do concatinating or merging operation in case of multiple datasets

> Deleting the column with missing data

updated_df = df.dropna(axis=1, how='all')

> Deleting the row with missing data

updated_df = newdf.dropna(axis=0)

> Filling the Missing Values – Imputation

updated_df['Age']=updated_df['Age'].fillna(updated_df['Age'].median())

updated_df['Age']=updated_df['Age'].replace(0, updated_df['Age'].median())

>> imputation with mode for categorical column

> To check duplication of data
len(my_df[my_df.duplicated()]) shall be 0

If found duplicates,
my_df.drop_duplicates(inplace=True)

> Searching for the columns on which the dataset can be filtered. If not found then creating/extracting the columns/features on which the dataset can be filtered 

> We do filtering on different conditions (extracting subset of data) and groupby operation (classfying the whole dataset on all possible groups) for further analysis

5. Detecting Anomalies in the Data:

6. Visualizing the Data:

Then we plot the  data on different aspects to analyse it more deeply

> We can plot the observed values for one feature with respect to observation numbers or we can plot relation between two features

7. The Conclusion from the Data:",detail exploratori data analysi,"['detail', 'exploratori', 'data', 'analysi']",Detailed EDA,1 connect data2 first feel dataw see whole extream larg dataset therefor understand dataset see head tail shape dataset3 deeper understand datathen tri understand dataset info miss valu npnan valu other string like avail etc also check type valu wheather list tupl set dict avail string describ methods4 clean datathen perform data clean simplifi data convert other string like avail etc npnan may concatin merg oper case multipl dataset delet column miss dataupdateddf dfdropnaaxis1 howal delet row miss dataupdateddf newdfdropnaaxis0 fill miss valu – imputationupdateddfageupdateddfagefillnaupdateddfagemedianupdateddfageupdateddfagereplace0 updateddfagemedian imput mode categor column check duplic data lenmydfmydfdupl shall 0if found duplic mydfdropduplicatesinplacetru search column dataset filter found creatingextract columnsfeatur dataset filter filter differ condit extract subset data groupbi oper classfi whole dataset possibl group analysis5 detect anomali data6 visual datathen plot data differ aspect analys deepli plot observ valu one featur respect observ number plot relat two features7 conclus data
232,"Quick EDA

For serious exploratory data analysis without writing code for different statistics and visualization, we can import  
pandas_profiling  for quick data analysis.

Pandas profiling in colab notebook

1. Run the following code to install the new version of pandas profiling in colab

pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip 

2. Restart the kernel (connecting again to the hosted runtime)

3. Re-import the libraries

import pandas as pd
import numpy as np
from pandas_profiling import ProfileReport

4. Reread the data set as pandas dataframe

5. Generate profile report
 data_profile=ProfileReport(my_dataframe,title='file_name',html={'style':{'full_width':True}})

6. Viewing profile report in colabnotebook
 data_profile.to_notebook_iframe()

7. Saving the report as html

data_profile.to_file(output_file='folder_path/file_name.html')",quick exploratori data analysi,"['quick', 'exploratori', 'data', 'analysi']",Quick EDA,serious exploratori data analysi without write code differ statist visual import pandasprofil quick data analysispanda profil colab notebook1 run follow code instal new version panda profil colabpip instal httpsgithubcompandasprofilingpandasprofilingarchivemasterzip 2 restart kernel connect host runtime3 reimport librariesimport panda pd import numpi np pandasprofil import profilereport4 reread data set panda dataframe5 generat profil report dataprofileprofilereportmydataframetitlefilenamehtmlstylefullwidthtrue6 view profil report colabnotebook dataprofiletonotebookiframe7 save report htmldataprofiletofileoutputfilefolderpathfilenamehtml
233,"EDA practice

sklearn is a machinelearning library that provides following datasets for practice

1. Toy datasets (Boston house prices,Iris plants,Diabetis,hand writen digits,Breast Cancer etc.), 

2. Real world datasets, 

3. Generated datasets and 

4. Image Data

from sklearn.datasets import load_diabetes

my_data=load_diabetes()",exploratori data analysi practic,"['exploratori', 'data', 'analysi', 'practic']",EDA practice,sklearn machinelearn librari provid follow dataset practice1 toy dataset boston hous pricesiri plantsdiabetishand writen digitsbreast cancer etc 2 real world dataset 3 generat dataset 4 imag datafrom sklearndataset import loaddiabetesmydataloaddiabet
234,"Use of ast library

import ast

ast stands for Abstract Syntax Trees

ast.literal_eval(): A function that  evaluates a string containing a Python literal (basic words) as list, tuple, set or dictionaries

or,
df['column_name'] = df.apply(lambda row: eval(row['column_name']), axis=1)",use ast librari,"['use', 'ast', 'librari']",Use of ast library,import astast stand abstract syntax treesastliteralev function evalu string contain python liter basic word list tupl set dictionariesor dfcolumnnam dfapplylambda row evalrowcolumnnam axis1
235,"Use of explode function

explode() function is used to transform each element of a list to a row, replicating the index values

s = pd.Series(['ff', [], [5, 6]])

s.explode() returns 'ff', NaN, 5, 6",use explod function,"['use', 'explod', 'function']",Use of explode function,explod function use transform element list row replic index valuess pdseriesff 5 6sexplod return ff nan 5 6
236,"Subtopics of Overview of mathematics

> Main branches of pure mathematics

> Sub-branches of Algebra

> Linear algebra

> Application of mathematics in machine learning",subtop overview mathemat,"['subtop', 'overview', 'mathemat']",Subtopics of Overview of mathematics,main branch pure mathemat subbranch algebra linear algebra applic mathemat machin learn
237,"libraries for data visualization

A Picture is worth a thousand words!

Main libraries for data visualization are 

1. matplotlib and 

2. seaborn",librari data visual,"['librari', 'data', 'visual']",libraries for data visualization,pictur worth thousand wordsmain librari data visual 1 matplotlib 2 seaborn
238,"Matplotlib library

MATLAB is a proprietary multi-paradigm programming language and numeric computing environment developed by MathWorks.

Matplotlib work like MATLAB.

One of the core aspects of Matplotlib is matplotlib.pyplot.

Different plots are: 

1. line plot

2. bar plot, 

3. box plot,

4. scatter plot,

5. step plot,

6. histogram, 

7. Time Series, 

8. Fill Between 

> A time series graph is a line graph of repeated measurements taken over regular time intervals. Time is always shown on the horizontal axis.",matplotlib librari,"['matplotlib', 'librari']",Matplotlib library,matlab proprietari multiparadigm program languag numer comput environ develop mathworksmatplotlib work like matlabon core aspect matplotlib matplotlibpyplotdiffer plot 1 line plot2 bar plot 3 box plot4 scatter plot5 step plot6 histogram 7 time seri 8 fill time seri graph line graph repeat measur taken regular time interv time alway shown horizont axi
239,"Matplotlib  pyplot function

Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.

import matplotlib.pyplot as plt

%matplotlib inline

from matplotlib import rcParams

%matplotlib inline sets the backend of matplotlib to the 'inline' backend. The resulting plots will then also be stored in the notebook document. This is line oriented magic function (Cell oriented magic functions are prefixed with a double %%)

If we are not using matplotlib in interactive mode at all, figures will only appear if we invoke plt.show().

plt.axis('equal') is used to show multiple plots under same axis

plt.legend(list_of_features) shows the legend",matplotlib pyplot function,"['matplotlib', 'pyplot', 'function']",Matplotlib  pyplot function,pyplot function make chang figur eg creat figur creat plot area figur plot line plot area decor plot label etcimport matplotlibpyplot pltmatplotlib inlinefrom matplotlib import rcparamsmatplotlib inlin set backend matplotlib inlin backend result plot also store notebook document line orient magic function cell orient magic function prefix doubl use matplotlib interact mode figur appear invok pltshowpltaxisequ use show multipl plot axispltlegendlistoffeatur show legend
240,"Matplotlib Line Plot

plt.plot(pandas_series_for_x_label, pandas_series_for_y_label,'bo--',linewidth=2, markersize=8)

> 'bo-' means blue line, 0 as marker, -- is line style",matplotlib line plot,"['matplotlib', 'line', 'plot']",Matplotlib Line Plot,pltplotpandasseriesforxlabel pandasseriesforylabelbolinewidth2 markersize8 bo mean blue line 0 marker line style
241,"Matplotlib Horizontal Bar Plot

plt.barh(y,x,width=0.4,color='rosybrown')",matplotlib horizont bar plot,"['matplotlib', 'horizont', 'bar', 'plot']",Matplotlib Horizontal Bar Plot,pltbarhyxwidth04colorrosybrown
242,"Matplotlib Box plot and Scatter Plot

Boxplot is a method for graphically demonstrating the locality, spread and skewness groups of numerical data through their quartiles.

plt.boxplot(my_new_df['column_name'])

Scatter Plot

The most common visual technique for bivariate analysis is a scatter plot, where one variable is on the x-axis and the other on the y-axis.

plt.scatter(x, y,s=80)

> s=80 is the marker size

",matplotlib box plot scatter plot,"['matplotlib', 'box', 'plot', 'scatter', 'plot']",Matplotlib Box plot and Scatter Plot,boxplot method graphic demonstr local spread skew group numer data quartilespltboxplotmynewdfcolumnnamescatt plotth common visual techniqu bivari analysi scatter plot one variabl xaxi yaxispltscatterx ys80 s80 marker size
243,"drawing a trend line in the scatter plot

fit = np.polyfit(x, y, deg=1) 

p = np.poly1d(fit) 

plt.plot(x,p(x),""r--"") 
",draw trend line scatter plot,"['draw', 'trend', 'line', 'scatter', 'plot']",drawing a trend line in the scatter plot,fit nppolyfitx deg1 p nppoly1dfit pltplotxpxr
244,"drawing horizontal or vertial lines

plt.axvline(x=1.8, ymin=0, ymax=1, color ='red', linestyle =""--"",  linewidth = 1)

plt.axhline(y=2.2, xmin=0, xmax=1, color ='red', linestyle =""--"", linewidth = 1)

>> plt.gcf().autofmt_xdate() -to show x-axis labels inclined",draw horizont vertial line,"['draw', 'horizont', 'vertial', 'line']",drawing horizontal or vertial lines,pltaxvlinex18 ymin0 ymax1 color red linestyl linewidth 1pltaxhliney22 xmin0 xmax1 color red linestyl linewidth 1 pltgcfautofmtxd show xaxi label inclin
245,"Seaborn library

Seaborn is a library for making statistical graphics in Python. It is built on top of matplotlib.

import seaborn as sns

Based on matplotlib but many special features are available in these plots

sns.color_palette() to see the available colors",seaborn librari,"['seaborn', 'librari']",Seaborn library,seaborn librari make statist graphic python built top matplotlibimport seaborn snsbase matplotlib mani special featur avail plotssnscolorpalett see avail color
246,"seaborn Line plot, Scatter plot, Distribution/Density Plot

1. Line plot

sns.lineplot(x=""index"",y=i1,data=eps,label=i1)

2. Scatter plot with regression line

sns.lmplot(x='feature_name', y='feature_name', data=my_df,height=10, aspect=1)

> lmplot is advance than sns.regplot(x='feature_name', y='feature_name', data=my_df)

3. Distribution/Density Plot

rcParams['figure.figsize'] = 7,7
sns.distplot(my_df['column_name'], hist=True)

> From the distplot we can find wheather the variables are normally distributed

> A histogram is a bar graph-like representation of data that buckets(or bins) a range of outcomes into columns along the x-axis. The y-axis represents the number count or percentage of occurrences in the data. Alternate is sns.distplot",seaborn line plot scatter plot distributiondens plot,"['seaborn', 'line', 'plot', 'scatter', 'plot', 'distributiondens', 'plot']","seaborn Line plot, Scatter plot, Distribution/Density Plot",1 line plotsnslineplotxindexyi1dataepslabeli12 scatter plot regress linesnslmplotxfeaturenam yfeaturenam datamydfheight10 aspect1 lmplot advanc snsregplotxfeaturenam yfeaturenam datamydf3 distributiondens plotrcparamsfigurefigs 77 snsdistplotmydfcolumnnam histtru distplot find wheather variabl normal distribut histogram bar graphlik represent data bucketsor bin rang outcom column along xaxi yaxi repres number count percentag occurr data altern snsdistplot
247,"seaborn Joint Distribution Plot, Heatmap, Bar Plot

4. Joint Distribution Plot

sns.jointplot(x='', y='', data=my_df,height=10)

5. Heatmap (for finding correlation between variables)

sns.heatmap(my_df.corr(), vmin=-1, cmap='Greens', annot=True)- visually pleasing

6. Bar Plot

plt.figure(figsize=(7,7))
 or
plt.rcParams['figure.figsize'] = (7,7)

sns.barplot(x='feature name', y='feature name', data=my_df)

",seaborn joint distribut plot heatmap bar plot,"['seaborn', 'joint', 'distribut', 'plot', 'heatmap', 'bar', 'plot']","seaborn Joint Distribution Plot, Heatmap, Bar Plot",4 joint distribut plotsnsjointplotx datamydfheight105 heatmap find correl variablessnsheatmapmydfcorr vmin1 cmapgreen annottru visual pleasing6 bar plotpltfigurefigsize77 pltrcparamsfigurefigs 77snsbarplotxfeatur name yfeatur name datamydf
248,"seaborn Histogram, Factor Plot, Box plot

7. Histogram (plot of data group vs. frequency)

Common visual technique used for univariate analysis is a histogram, which is a frequency distribution graph. We could also use a box plot or violin plot to compare the spread of the variables and provides an insight into outliers. 

rcParams['figure.figsize'] = 10,5

sns.histplot(my_df['column_name'], kde=True)

8. Factor Plot

sns.factorplot(x='sample' ,y= 'value',data=my_df,height=10)

9. Box plot

rcParams['figure.figsize'] = 7,7

sns.boxplot(x='lsample', y='value', data=my_df)

sns.boxplot(new_df['RM'])

> We can check both distribution and outliers by boxplot",seaborn histogram factor plot box plot,"['seaborn', 'histogram', 'factor', 'plot', 'box', 'plot']","seaborn Histogram, Factor Plot, Box plot",7 histogram plot data group vs frequencycommon visual techniqu use univari analysi histogram frequenc distribut graph could also use box plot violin plot compar spread variabl provid insight outlier rcparamsfigurefigs 105snshistplotmydfcolumnnam kdetrue8 factor plotsnsfactorplotxsampl valuedatamydfheight109 box plotrcparamsfigurefigs 77snsboxplotxlsampl yvalu datamydfsnsboxplotnewdfrm check distribut outlier boxplot
249,"10. Seaborn Pairplot

It is a very useful tool to carry out univariate, bivariate and multivariate analysis in a single plot

sns.pairplot(data=my_df, kind='scatter', diag_kind='kde')
plt.show()

>> Kind of diagonal graphs are Kernel Density Estimates(kde) and kind of non-diagonal graphs are scatter

>> Diagonal graphs are for univariate analysis

>> If dependent variable is the first column of dataset, then the first row graphs are for bivariate analysis

>> In pairgrid plot, we can pass various parameter for more flexibility",10 seaborn pairplot,"['10', 'seaborn', 'pairplot']",10. Seaborn Pairplot,use tool carri univari bivari multivari analysi singl plotsnspairplotdatamydf kindscatt diagkindkd pltshow kind diagon graph kernel densiti estimateskd kind nondiagon graph scatter diagon graph univari analysi depend variabl first column dataset first row graph bivari analysi pairgrid plot pass various paramet flexibl
250,"Ploting directly from pandas dataframe

# set the required column as index(parameter to be plotted in x axis)

my_df=my_df.set_index('column_name')

It is very useful to compare multiple time series. Then 'Time' shall be set as index and then

my_df.plot.line(figsize=(7,7))

my_df.groupby(['iyear'])['nkill', 'nwound'].sum().plot.line(figsize=(10,8))

>> requirement of scaler transformation may be investigated

# We can plot one parameter vs one parameter

my_df.plot.line(x='MS Flow through Turbine',y='Load\n', figsize=(15,10))

# We can also plot bar, scatter, box etc. 

df.groupby(['month_number'])[['facewash','facecream']].mean().plot.bar()

my_df.hist(bins=10, figsize=(7,7))

my_df.boxplot(column=list(my_df.describe().columns))

> The plot method on Series and DataFrame is just a simple wrapper around plt.plot()",plote direct panda datafram,"['plote', 'direct', 'panda', 'datafram']",Ploting directly from pandas dataframe,set requir column indexparamet plot x axismydfmydfsetindexcolumnnameit use compar multipl time seri time shall set index thenmydfplotlinefigsize77mydfgroupbyiyearnkil nwoundsumplotlinefigsize108 requir scaler transform may investig plot one paramet vs one parametermydfplotlinexm flow turbineyloadn figsize1510 also plot bar scatter box etc dfgroupbymonthnumberfacewashfacecreammeanplotbarmydfhistbins10 figsize77mydfboxplotcolumnlistmydfdescribecolumn plot method seri datafram simpl wrapper around pltplot
251,"Editing the plot area with matplotlib and seaborn

plt.title('my_chart_title')
plt.ylabel('chart_y_label')
plt.xlabel('chart_x_label')
plt.grid()

plt.rcParams['figure.figsize'] = (7,7) 

> size=7 inches/ 7 inches

sns.set_style(""darkgrid"")

>  pandas data structures, matplotlib and seaborn are closely integrated",edit plot area matplotlib seaborn,"['edit', 'plot', 'area', 'matplotlib', 'seaborn']",Editing the plot area with matplotlib and seaborn,plttitlemycharttitl pltylabelchartylabel pltxlabelchartxlabel pltgridpltrcparamsfigurefigs 77 size7 inch 7 inchessnssetstyledarkgrid panda data structur matplotlib seaborn close integr
252,"Combining two plots in a single graph

plt.figure(figsize=(7,7))
sns.distplot(np.log10(my_df['feature_name']),color=""y"")

-combination of bar plot and distribution plot

Create multiple plots

1. Using for loop

2. Using subplot

plt.suptitle(""Title for whole figure"", fontsize=16)

plt.subplot(total_subplot_rows,total_subplot_columns, subplot_number)

sns.countplot(my_df['feature_name'])",combin two plot singl graph,"['combin', 'two', 'plot', 'singl', 'graph']",Combining two plots in a single graph,pltfigurefigsize77 snsdistplotnplog10mydffeaturenamecolorycombin bar plot distribut plotcreat multipl plots1 use loop2 use subplotpltsuptitletitl whole figur fontsize16pltsubplottotalsubplotrowstotalsubplotcolumn subplotnumbersnscountplotmydffeaturenam
253,"Saving any print output as word file in google drive

with open(""folder_path/file_name.docx"", ""a"") as f:
    print(""Hello World!"", file=f)
    print(""I have a question."", file=f)",save print output word file googl drive,"['save', 'print', 'output', 'word', 'file', 'googl', 'drive']",Saving any print output as word file in google drive,openfolderpathfilenamedocx f printhello world filef printi question filef
254,"saving the plot as png in google drive

plt.savefig('folder_path/my_fig.png')",save plot png googl drive,"['save', 'plot', 'png', 'googl', 'drive']",saving the plot as png in google drive,pltsavefigfolderpathmyfigpng
255,"Excel basics

Excel is a datastructure with columns and rows. It is also a programming platform

>> In excel, cell reference (a cell or a range of cells) like A5,H6 etc. are feature names or variable names or identifiers

>> Thus, excel stores all the values (input value or output of any function) mentioned in different cell in different variables (A5,H6 etc.) 

>> Thus we can call the variables inside any function (=A5*H6) as and when required and it is stored in another variable (say, K1)

For many organisation presentation in excel is needed",excel basic,"['excel', 'basic']",Excel basics,excel datastructur column row also program platform excel cell refer cell rang cell like a5h6 etc featur name variabl name identifi thus excel store valu input valu output function mention differ cell differ variabl a5h6 etc thus call variabl insid function a5h6 requir store anoth variabl say k1for mani organis present excel need
256,"Data Handling in excel

> Paste special option may be used whenever only one feature of a cell or a group of cells are required to paste.

We can also do arithmetic operations and transpose with paste special. 

> During presentation, we shall present the workbook with multiple worksheets

> During arithmatic operation with a particular cell, we can use $ (press F4 key) for freezing of cell (e,g, $A63)",data handl excel,"['data', 'handl', 'excel']",Data Handling in excel,past special option may use whenev one featur cell group cell requir pastew also arithmet oper transpos past special present shall present workbook multipl worksheet arithmat oper particular cell use press f4 key freez cell eg a63
257,"Sections in excel file (top to bottom)

1. Menu bar

2. Standard toolbar

3. Formula bar (including Name box)

4. Headings (A,B,C…)

5. Spreadsheet

6. Sheet tabs

7. Status bar",section excel file top bottom,"['section', 'excel', 'file', 'top', 'bottom']",Sections in excel file (top to bottom),1 menu bar2 standard toolbar3 formula bar includ name box4 head abc…5 spreadsheet6 sheet tabs7 status bar
258,"Menu Bar Tabs in excel

1. Home tab 

i. Font alignment, type, size, colour

ii. Cell colour

iii. Merge Cells

iv. Hide columns and rows (cell> format)

v. Adjusting row height and column width  (cell> format)

vi. Clear format

2. Insert tab 

i. Pictures or logo

ii. Hyperlinks (mainly used for multiple worksheets)

3 Page Layout tab

4. Formula (Functions) tab

i. Logical (AND, OR, IF, NOT etc.)

ii. Date & Time

iii. Text (Left, Right, Concatenate(&), Find, Text to Column)

iv. Lookups (VLOOKUP& HLOOKUP)

v. Math

vi. More Functions (Financial, Statistical, Engineering and more)

vii. Formula Auditing> Error Checking> circular ref, Data>edit links, 
IFERROR
evaluate formula, 
trace dependent etc.

5. Data tab 

i. Grouping Rows or Columns

6. Review tab

7. View tab

i. Freeze panes

ii. Gridlines
",menu bar tab excel,"['menu', 'bar', 'tab', 'excel']",Menu Bar Tabs in excel,1 home tab font align type size colourii cell colouriii merg cellsiv hide column row cell formatv adjust row height column width cell formatvi clear format2 insert tab pictur logoii hyperlink main use multipl worksheets3 page layout tab4 formula function tabi logic etcii date timeiii text left right concaten find text columniv lookup vlookup hlookupv mathvi function financi statist engin morevii formula audit error check circular ref dataedit link iferror evalu formula trace depend etc5 data tab group row columns6 review tab7 view tabi freez panesii gridlin
259,"Data types in excel

It is very important to check the data types in excel to avoid mistakes

General,

Date,

Number,

Time,

Text,

Percentage
",data type excel,"['data', 'type', 'excel']",Data types in excel,import check data type excel avoid mistakesgeneraldatenumbertimetextpercentag
260,"To view the groupby statistics like pandas df in excel

> First hide the not necessary columns or group the columns 

> Then sort the values of the coulmns on which groupby operation to be performed

> Then click on Data tab> Outline> Subtotal and use functions like sum, average, count, min, max etc. in the required column

> Mode statistic in pivot table is available in powerpivot of office 2013. But mode can be calculated through mode() function in lower version

> Then copy different  subtotal statistics and paste in separate worksheet (Select the required rows and columns and then press Alt+; for selecting the visible cells only) for creating the dashboard
 
> PivotTable is better option than Subtotal for creating the dashboard",view groupbi statist like panda df excel,"['view', 'groupbi', 'statist', 'like', 'panda', 'df', 'excel']",To view the groupby statistics like pandas df in excel,first hide necessari column group column sort valu coulmn groupbi oper perform click data tab outlin subtot use function like sum averag count min max etc requir column mode statist pivot tabl avail powerpivot offic 2013 mode calcul mode function lower version copi differ subtot statist past separ worksheet select requir row column press alt select visibl cell creat dashboard pivott better option subtot creat dashboard
261,"Filter in excel

(Home/Data tab)

1. Filter

> The most important function for data analysis and experiment

> We can filter data by custom filter

> We can filter data by font colour or cell colour

> Shortcut for selecting the filter: Alt+down arrow

Few other keyboard shortcuts

> When doing any arithmatic operation with the filtered cells, we need to select the cells and then press ALT+;

> selecting entire column: Ctrl+space

> Protect sheet: Alt+T+P+P

> Sheet change: Ctrl+Pgdn

> Hiding entire row: Ctrl+9

> Unhide row: Ctrl + Shift + 9

> Hiding column: Ctrl+0

> Getting to cell A1: Ctrl+home

> To copy cell contents using drag and drop press the Ctrl key",filter excel,"['filter', 'excel']",Filter in excel,homedata tab1 filter import function data analysi experi filter data custom filter filter data font colour cell colour shortcut select filter altdown arrowfew keyboard shortcut arithmat oper filter cell need select cell press alt select entir column ctrlspace protect sheet alttpp sheet chang ctrlpgdn hide entir row ctrl9 unhid row ctrl shift 9 hide column ctrl0 get cell a1 ctrlhome copi cell content use drag drop press ctrl key
262,"Conditional Formatting in excel

(Home tab)

2. Conditional Formatting 

> This is for automatic colour coding, bar coding or icon coding of the data 

> It helps in data analysis by  highlighting cells with certain rule like certain text or duplicate values",condit format excel,"['condit', 'format', 'excel']",Conditional Formatting in excel,home tab2 condit format automat colour code bar code icon code data help data analysi highlight cell certain rule like certain text duplic valu
263,"Sorting in excel

(Home/ Data tab)

3. Sorting

> Like filter, sorting can also be done by both values and colour (text or cell)

> Sorting can also be done for multiple levels (go to add level option in the sort dialog box)

> Sorting may be done for rows means for column headings",sort excel,"['sort', 'excel']",Sorting in excel,home data tab3 sort like filter sort also done valu colour text cell sort also done multipl level go add level option sort dialog box sort may done row mean column head
264,"Removing Duplicates in excel

(Data tab)

4. Remove Dups

> Directly removes duplicate observation with one column or multiple columns",remov duplic excel,"['remov', 'duplic', 'excel']",Removing Duplicates in excel,data tab4 remov dup direct remov duplic observ one column multipl column
265,"Pivot and Slicers in excel

(Insert tab)

5. Pivot and Slicers

> Pivot is a way to summarize data

> It helps us to create a summary table playing with Row labels, Column labels, Filters and Summarizing Values

> We can copy-paste the  summary table multiple times and create different combinations of results

> By right click on values we can change the summarizing data type (sum,count,avg,max,min,etc.)

> For pivot table, excel shows option tab in which we can do more arrangement of the summary table

> We can show additional calculated column (option tab> formulas> calculated field) if required

> If we double click on the summarized value, excel shows the exact data for that summarization on different worksheet

> We can also do groupby or other operations from other tabs apart from pivot table option tab if required

Pivot Slicer

> In pivot table option tab, we have slicer (a type of filter which operates from outside the table). We can connect multiple pivot tables in a single slicer.

Slicers is not available in Excel 2007.",pivot slicer excel,"['pivot', 'slicer', 'excel']",Pivot and Slicers in excel,insert tab5 pivot slicer pivot way summar data help us creat summari tabl play row label column label filter summar valu copypast summari tabl multipl time creat differ combin result right click valu chang summar data type sumcountavgmaxminetc pivot tabl excel show option tab arrang summari tabl show addit calcul column option tab formula calcul field requir doubl click summar valu excel show exact data summar differ worksheet also groupbi oper tab apart pivot tabl option tab requiredpivot slicer pivot tabl option tab slicer type filter oper outsid tabl connect multipl pivot tabl singl slicerslic avail excel 2007
266,"Refreshing all pivot tables

How do we refresh data in all pivot tables or entire worksheet at a time

> Pivot table  options> Refresh all and pivot chart analyze> Refresh all",refresh pivot tabl,"['refresh', 'pivot', 'tabl']",Refreshing all pivot tables,refresh data pivot tabl entir worksheet time pivot tabl option refresh pivot chart analyz refresh
267,"Charts in excel

(Insert tab)

6. Charts

> After selecting the table, we can insert chart of our choice

> On getting the chart, we can select the chart and will get design, layout and format tabs for the chart

> In layout tab> analysis> Trendline

- we have More trendline options which allows us to adjust the trendline and see the equation of the trend line on the chart

>> For comparing values over categories Column Chart (vertical bar graph) is useful

>> To track the progress of the stock market on a daily basis or to track or compare performance of employees in a year, Line chart is useful",chart excel,"['chart', 'excel']",Charts in excel,insert tab6 chart select tabl insert chart choic get chart select chart get design layout format tab chart layout tab analysi trendlin trendlin option allow us adjust trendlin see equat trend line chart compar valu categori column chart vertic bar graph use track progress stock market daili basi track compar perform employe year line chart use
268,"Dashboard in excel

> Dashboard is the panel facing the driver of a vehicle or the pilot of an aircraft, containing instruments and controls

> A data dashboard is an information management tool that visually tracks, analyzes and displays key performance indicators (KPI), metrics and key data points to monitor the health of a business, department or specific process

> Data dashboard is created with pivot table, slicer, additional tables and charts, title, legend

> At first we need to do all data crunching in python and create a clean csv or excel. 

> If management want the visualization (dashboard) in excel, then we do all  onward excel operations.

> If the management want the dashboard in Tableau (or Power BI), then we can connect the clean excel file in Tableau (or Power BI) for onward operations.",dashboard excel,"['dashboard', 'excel']",Dashboard in excel,dashboard panel face driver vehicl pilot aircraft contain instrument control data dashboard inform manag tool visual track analyz display key perform indic kpi metric key data point monitor health busi depart specif process data dashboard creat pivot tabl slicer addit tabl chart titl legend first need data crunch python creat clean csv excel manag want visual dashboard excel onward excel oper manag want dashboard tableau power bi connect clean excel file tableau power bi onward oper
269,"Waterfall or birdge graph analysis

> This graph shows, how things have changed between two periods(or two states) 

> Insert a stacked column chart and adjust the invisible colour",waterfal birdg graph analysi,"['waterfal', 'birdg', 'graph', 'analysi']",Waterfall or birdge graph analysis,graph show thing chang two periodsor two state insert stack column chart adjust invis colour
270,"Excel functions

COUNT(B2:C8) function calculates the number of cells that have numeric entries

The COUNTA(H2:O16) Function is categorized under Excel Statistical functions. It will calculate the number of cells that have numeric or text entries (i.e not blank)

LEN(H2) returns the string lenth of the cell

IF(LEN(H2)>50,1,0) # returns 1 if string length is greater than 50 elase 0

INDEX(A1:B5,2,2) function returns the value at a given location in a range or array (table ) by index (here row index and column index starts from 1)-// This actually returns value in B2

VLOOKUP is used for vertically arranged table means data is arranged in different columns. It searches the value in a particular column against the value in any other column. (column indexing starts from 1 in excel)

VLOOKUP(search_by_value, A1:B4, required_value_column_index, False) -False is used for finding exact match

MATCH(41,B2:B5,0) function returns the index after exact matching of the value with the given range (vertical or horizontal)

-MATCH function is similar like VLOOKUP or HLOOKUP but match function returns index and VLOOKUP or HLOOKUP returns value

=A1=B1 returns True or False

ROUND(235.415, -1) returns 240

ROUND(235.415, -2) ruturns 200

ROUND(235.415, 2) returns 235.42

ROUND(235.415, 0) or ROUNDDOWN(235.415, 0) returns 235

> Positive number (1,2) indicates digits after the decimal point and negative number (-1,-2) indicates digits before 0 point (unit place or one's place of the number)

0,1,2,-1,-2 etc indicates the rounding location in ROUND function",excel function,"['excel', 'function']",Excel functions,countb2c8 function calcul number cell numer entriesth countah2o16 function categor excel statist function calcul number cell numer text entri ie blanklenh2 return string lenth celliflenh25010 return 1 string length greater 50 elas 0indexa1b522 function return valu given locat rang array tabl index row index column index start 1 actual return valu b2vlookup use vertic arrang tabl mean data arrang differ column search valu particular column valu column column index start 1 excelvlookupsearchbyvalu a1b4 requiredvaluecolumnindex fals fals use find exact matchmatch41b2b50 function return index exact match valu given rang vertic horizontalmatch function similar like vlookup hlookup match function return index vlookup hlookup return valuea1b1 return true falseround235415 1 return 240round235415 2 ruturn 200round235415 2 return 23542round235415 0 rounddown235415 0 return 235 posit number 12 indic digit decim point negat number 12 indic digit 0 point unit place one place number01212 etc indic round locat round function
271,"Concatenation of text in excel

using Ampersand (&)

= A5&A6

or using CONCATENATE(A5,A6)

>> Function To insert current date and time

now()",concaten text excel,"['concaten', 'text', 'excel']",Concatenation of text in excel,use ampersand a5a6or use concatenatea5a6 function insert current date timenow
272,"label value

A numeric value can be treated as label value if Apostrophe (‘) precedes it.

Microsoft Excel uses the table function to calculate the results in the data table.",label valu,"['label', 'valu']",label value,numer valu treat label valu apostroph ‘ preced itmicrosoft excel use tabl function calcul result data tabl
273,"Tableau basics

Tableau is a data visualization software and also a business intelligence (BI) software (may also be used for data crunching)

By definition, Tableau displays measures over time as a Line.

Alternative is Power BI (by microsoft)

Connect Tableau with static data (excel, text etc.) or dynamic data (SQL server)

Like worksheets in excel, Tableau has sheets and Dashboard. In one sheet there can be one visualization. In Dashboard we can have multiple visualization.  

Tableau operation is similar like pivot chart operation in excel

> Disaggregation returns all records in the underlying data source.

> The symbol related with the field that has been assembled is a 
Paper Clasp (An orange check mark indicates that the data source is the secondary data source in the workbook)

> Cell Size Option In Format Menu is TO CUSTOMIZE THE SIZE OF THE CELLS DISPLAYING THE DATA

> We can publish the workbook in the server whenever required",tableau basic,"['tableau', 'basic']",Tableau basics,tableau data visual softwar also busi intellig bi softwar may also use data crunchingbi definit tableau display measur time linealtern power bi microsoftconnect tableau static data excel text etc dynam data sql serverlik worksheet excel tableau sheet dashboard one sheet one visual dashboard multipl visual tableau oper similar like pivot chart oper excel disaggreg return record under data sourc symbol relat field assembl paper clasp orang check mark indic data sourc secondari data sourc workbook cell size option format menu custom size cell display data publish workbook server whenev requir
274,"Best practices for visualization

> Keep things simple and digestible (more important than beauty)

> There must be a meaningful association with colour

> In Bar graphs, size is more iterpretable than circular graphs

> Attention map for visualization: center is the most emphasized one, top left is the second emphasized part, bottom right is least emphasized one

In visualization we can play with following parameters

> Colour

> Size

> labels

> Tooltip

> Details",best practic visual,"['best', 'practic', 'visual']",Best practices for visualization,keep thing simpl digest import beauti must meaning associ colour bar graph size iterpret circular graph attent map visual center emphas one top left second emphas part bottom right least emphas onein visual play follow paramet colour size label tooltip detail
275,"Panes of Tableau

In Tableau, There are two panes in the left side:

1. Data

2. Analysis

under data pane there are:

i. Dimensions (Dimensions contain qualitative values such as names, dates, or geographical data. Categorical columns enter this section.)

ii. Measures (Measures contain numeric, quantitative values that we can measure. Numerical columns enter this section.)

iii. Calculated fields (these are actually engineered features)

iv. Sets (Sets are custom fields that define a subset of data based on some conditions)

v. Parameters (Parameters give us a way to dynamically modify a reference line, band, or box)

>> Data pane fields are as per sql schema when connected to sql server

under analysis pane there are:

i. Summarize (constant line, avg. line, median and quartile)

ii. Model (Trend line, forecast)

> Maximum of 32 tables can be join in tableau",pane tableau,"['pane', 'tableau']",Panes of Tableau,tableau two pane left side1 data2 analysisund data pane arei dimens dimens contain qualit valu name date geograph data categor column enter sectionii measur measur contain numer quantit valu measur numer column enter sectioniii calcul field actual engin featuresiv set set custom field defin subset data base conditionsv paramet paramet give us way dynam modifi refer line band box data pane field per structur queri languag schema connect structur queri languag serverund analysi pane arei summar constant line avg line median quartileii model trend line forecast maximum 32 tabl join tableau
276,"Trend line models in Tableau

Tableau provides users with five trend line models: 

Linear, 

Logarithmic, 

Exponential, 

Polynomial and 

Power.",trend line model tableau,"['trend', 'line', 'model', 'tableau']",Trend line models in Tableau,tableau provid user five trend line model linear logarithm exponenti polynomi power
277,"Creating Calculated field in Tableau

> Select “Analysis” present in the menu bar.

> Select “Create Calculated Field” from the list.

> Then calculated Field window will open. Name it.

> Type the estimated value of the measure.

For creating variable size bins, we use Calculated fields (In the Data pane, right-click a measure and select Create > Bins)",creat calcul field tableau,"['creat', 'calcul', 'field', 'tableau']",Creating Calculated field in Tableau,select “analysis” present menu bar select “creat calcul field” list calcul field window open name type estim valu measurefor creat variabl size bin use calcul field data pane rightclick measur select creat bin
278,"Chart area of Tableau

In Tableau, the chart area is divided into grid (helps to show multiple dimensions and multiple measures in a single chart). The columns of the grid shows the dimensions and the rows of the grid shows the measures. (can be reversed)

On dragging dimension to columns and measure to rows, Tableau automatically creates a bar graph (vertical).

Click on Show Me button (top right corner) to change the visualization


>> As an alternative to Dashboard, we can create tableau stories

>> We can drag and drop a variable in the Detail Mark or any of the marks, whenever detailing on visualization is required.

>> We can disable or enable the highlighting action for the workbook or sheets from the toolbar.",chart area tableau,"['chart', 'area', 'tableau']",Chart area of Tableau,tableau chart area divid grid help show multipl dimens multipl measur singl chart column grid show dimens row grid show measur reversedon drag dimens column measur row tableau automat creat bar graph verticalclick show button top right corner chang visual altern dashboard creat tableau stori drag drop variabl detail mark mark whenev detail visual requir disabl enabl highlight action workbook sheet toolbar
279,"Chart types in Tableau

1. Area Chart

2. Bar Chart

3. Box and Whisker Plots

4. Bullet Chart (like horizontal bar garph, also called projectile chart)-This is to compare multiple measures for a single category

5. Scatter Plot

6. Pie Chart

7. Bubble Chart

8. Line Chart

> The chart wizard term data series refers to a collection of chart data markers",chart type tableau,"['chart', 'type', 'tableau']",Chart types in Tableau,1 area chart2 bar chart3 box whisker plots4 bullet chart like horizont bar garph also call projectil chartthi compar multipl measur singl category5 scatter plot6 pie chart7 bubbl chart8 line chart chart wizard term data seri refer collect chart data marker
280,"Tableau Pills

The fields drag and dropped in the columns and rows look like pills and so they are generally called pills

> Pill colors

Most of the dimensions we use are blue and most of the measures are green. 

Blue pills however in fact correspond to discrete data and green pills to continuous data. If we see a red pill at any point, it means there is an error of some kind – a dimension/measure is missing or the calculation we did hasn't been recognised.

e.g. 

Discrete Dimensions: Product Name

Continuous Dimensions: Year(order date)

Discrete Measures: Sum(discrete profit)

Continuous Measures:  Sum(continuous profit)",tableau pill,"['tableau', 'pill']",Tableau Pills,field drag drop column row look like pill general call pill pill colorsmost dimens use blue measur green blue pill howev fact correspond discret data green pill continu data see red pill point mean error kind – dimensionmeasur miss calcul hasnt recognisedeg discret dimens product namecontinu dimens yearord datediscret measur sumdiscret profitcontinu measur sumcontinu profit
281,"Tableau Desktop applications

> Tableau Desktop 

> Tableau Public (can not save data locally like Tableau Desktop, only handles static data)

 > Tableau Online

> Tableau Server (similar like tableau online)

> Tableau Prep

> Tableau Reader

> Tableau Viewer

> Tableau Explorer",tableau desktop applic,"['tableau', 'desktop', 'applic']",Tableau Desktop applications,tableau desktop tableau public save data local like tableau desktop handl static data tableau onlin tableau server similar like tableau onlin tableau prep tableau reader tableau viewer tableau explor
282,"Components of a Dashboard

1. Horizontal bar graph

2. Vertical bar graph

3. Image Extract",compon dashboard,"['compon', 'dashboard']",Components of a Dashboard,1 horizont bar graph2 vertic bar graph3 imag extract
283,"COUNTD function

COUNTD function is used for displaying a distinct or unique value of the dimension. It will count the distinct value of the number of items in a group and will display it. It will ignore NULL values.",countd function,"['countd', 'function']",COUNTD function,countd function use display distinct uniqu valu dimens count distinct valu number item group display ignor null valu
284,"Reference line and Reference band

Say we are analyzing the monthly sales for several products, we can include a reference line at the average sales mark so we can see how each product performed against the average.

A Reference Band can be based on two fixed points.",refer line refer band,"['refer', 'line', 'refer', 'band']",Reference line and Reference band,say analyz month sale sever product includ refer line averag sale mark see product perform averagea refer band base two fix point
285,"File extensions in Tableau

Tableau Workbook (.twb)

Tableau Packaged Workbook (.twbx)

Tableau Data Source(.tds)",file extens tableau,"['file', 'extens', 'tableau']",File extensions in Tableau,tableau workbook twbtableau packag workbook twbxtableau data sourcetd
286,"Filters in Tableau

Filters are very helpful to create dashboards in Tableau. Filters can help to minimize the size of data sets for efficient use, eliminate irrelevant dimension elements, clean up underlying data, set date ranges and measures as required, simplify and organize data, etc.

There are several different kinds of filters in Tableau and they get executed in the following order from top to bottom

1. Extract filters

2. Data Source filters

3. Context filters

4. Dimension filters

5. Measure filters

6. Table Calc filters",filter tableau,"['filter', 'tableau']",Filters in Tableau,filter help creat dashboard tableau filter help minim size data set effici use elimin irrelev dimens element clean under data set date rang measur requir simplifi organ data etcther sever differ kind filter tableau get execut follow order top bottom1 extract filters2 data sourc filters3 context filters4 dimens filters5 measur filters6 tabl calc filter
287,"Data blending 

Data blending is a method for combining data from multiple sources.

Blends, unlike relationships or joins, never truly combine the data. Instead, blends query each data source independently, the results are aggregated to the appropriate level, then the results are presented visually together in the view.

> We can perform all kinds of joins using data blending

> Blending uses Left join by default",data blend,"['data', 'blend']",Data blending ,data blend method combin data multipl sourcesblend unlik relationship join never truli combin data instead blend queri data sourc independ result aggreg appropri level result present visual togeth view perform kind join use data blend blend use left join default
288,"Data Types in Tableau

> String

> Numeric 

> Date and Time

> Boolean 

> Geographic

> Cluster or Mixed ",data type tableau,"['data', 'type', 'tableau']",Data Types in Tableau,string numer date time boolean geograph cluster mix
289,"KPI Basics

A popular saying in organizations is “what gets measured gets managed“. -by management guru Peter Drucker

Don’t just Measure. Measure what matters. Therefore we use KPI

KPI stands for key performance indicator, a quantifiable measure of performance over time for a specific objective. KPIs provide targets for teams to shoot for, milestones to gauge progress, and insights that help people across the organization make better decisions. 

There are KPIs specific to marketing, development, and support etc.",kpi basic,"['kpi', 'basic']",KPI Basics,popular say organ “what get measur get managed“ manag guru peter druckerdon't measur measur matter therefor use kpikpi stand key perform indic quantifi measur perform time specif object kpis provid target team shoot mileston gaug progress insight help peopl across organ make better decis kpis specif market develop support etc
290,"KPI vs Metric

While key performance indicators and metrics are related, they’re not the same. 

Metrics are not the most critical measures. Some examples include “monthly store visits” or “white paper downloads”.

To build business intelligence, we should track both strategic key performance indicators and tactical metrics.  ",kpi vs metric,"['kpi', 'vs', 'metric']",KPI vs Metric,key perform indic metric relat they'r metric critic measur exampl includ “month store visits” “white paper downloads”to build busi intellig track strateg key perform indic tactic metric
291,"Need of KPI for the company

1. we Can Measure our Targets

2. Create an Atmosphere of Learning

3. Receive Important Information

4. Encourage Accountability

5. Boost Morale

6. Keep our teams aligned

7. Provide a health check

8. Make adjustments

9. Hold our teams accountable",need kpi compani,"['need', 'kpi', 'compani']",Need of KPI for the company,1 measur targets2 creat atmospher learning3 receiv import information4 encourag accountability5 boost morale6 keep team aligned7 provid health check8 make adjustments9 hold team account
292,"Types of Indicators

1. Quantitative indicators

2. Qualitative indicators

3. Leading indicators

4. Lagging indicators

5. Input indicators

6. Process indicators

7. Output indicators

8. Practical indicators

9. Directional indicators

10. Actionable indicators

11. Financial indicators",type indic,"['type', 'indic']",Types of Indicators,1 quantit indicators2 qualit indicators3 lead indicators4 lag indicators5 input indicators6 process indicators7 output indicators8 practic indicators9 direct indicators10 action indicators11 financi indic
293,"Effectiveness and Efficiency

Effectiveness = atual output/ expected output

It is the comparison between actual output and expected output

Efficiency = output (actual cost of product)/ input (actual cost of resource consumed)

It is the comparison between  actual output and actual input

> If we maximize the expectation equal to the input, then effectiveness becomes equal to efficiency",effect effici,"['effect', 'effici']",Effectiveness and Efficiency,effect atual output expect outputit comparison actual output expect outputeffici output actual cost product input actual cost resourc consumedit comparison actual output actual input maxim expect equal input effect becom equal effici
294,"Ways to develop KPI

1. Define how KPIs will be used

2. Tie them to strategic goals

3. Write SMART KPIs

4. Keep them clear-cut

5. Plan to iterate

6. Avoid KPI overload",way develop kpi,"['way', 'develop', 'kpi']",Ways to develop KPI,1 defin kpis used2 tie strateg goals3 write smart kpis4 keep clearcut5 plan iterate6 avoid kpi overload
295,"Three steps to a stronger KPI strategy

1. Select KPIs that matter most

2. Create a KPI-driven culture

3. Iterate
",three step stronger kpi strategi,"['three', 'step', 'stronger', 'kpi', 'strategi']",Three steps to a stronger KPI strategy,1 select kpis matter most2 creat kpidriven culture3 iter
296,"Key Performance Indicators in practice

1. Result indicators – what have we done?

2. Key result indicators – how well have we done?

3. Performance indicators – what shall be done?

4. Key Performance indicators – what needs to be done?

> Performance Measures  is an indicator of the measurement of success in any organization

> On-Time To Promise is a KPI measure (the percentage of time we're successful at delivering the product when we promised our customers we would deliver it.)

> In Industrial performance, resource means Material resources, Human resources and Financial resources",key perform indic practic,"['key', 'perform', 'indic', 'practic']",Key Performance Indicators in practice,1 result indic – done2 key result indic – well done3 perform indic – shall done4 key perform indic – need done perform measur indic measur success organ ontim promis kpi measur percentag time success deliv product promis custom would deliv industri perform resourc mean materi resourc human resourc financi resourc
297,"Executive Dashboard 

An Executive Dashboard is a reporting tool that provides a visual display of organizational KPIs, metrics, and data. The objective of executive dashboards is to give CEOs an at-a-glance visibility into business performance across all units and projects.",execut dashboard,"['execut', 'dashboard']",Executive Dashboard ,execut dashboard report tool provid visual display organiz kpis metric data object execut dashboard give ceo atagl visibl busi perform across unit project
298,"Possible dangers of industrial performance indicators

1. Do not balance all the goals

2. Perform unnecessary indicators

3. Difficulty of defining objectives by service",possibl danger industri perform indic,"['possibl', 'danger', 'industri', 'perform', 'indic']",Possible dangers of industrial performance indicators,1 balanc goals2 perform unnecessari indicators3 difficulti defin object servic
299,"SQL Basics

SQL means Structured Query Language

> In csv file there can be only one worksheet means one table

> In excel file there can be multiple worksheets means multiple tables

> Similar like excel, in .db or .tar files (SQL) there are  multiple inter-related tables (relational database) but used for very large amount of data

> Sql needs a server because it is dynamic database

> SQL statement is also called clause

> SQL is case insensitive, but are often written in all caps.",structur queri languag basic,"['structur', 'queri', 'languag', 'basic']",SQL Basics,structur queri languag mean structur queri languag csv file one worksheet mean one tabl excel file multipl worksheet mean multipl tabl similar like excel db tar file structur queri languag multipl interrel tabl relat databas use larg amount data structur queri languag need server dynam databas structur queri languag statement also call claus structur queri languag case insensit often written cap
300,"List of relational database

-Oracle Database 12c

-MySQL (Oracle)

-Microsoft SQL Server

-PostgreSQL

-SQLite

-DB2 (owner IBM)

-SAP HANA (owner SAP)

> many of them are open source",list relat databas,"['list', 'relat', 'databas']",List of relational database,oracl databas 12cmystructur queri languag oraclemicrosoft structur queri languag serverpostgrestructur queri languagestructur queri languageitedb2 owner ibmsap hana owner sap mani open sourc
301,"Difference between SQL and Python

SQL is a query language primarily aimed to store, manipulate and retrieve data

Python is a general-purpose programming language that enables experimentation with the data.

But SQL is faster than Python, thus used for extreamly large data",differ structur queri languag python,"['differ', 'structur', 'queri', 'languag', 'python']",Difference between SQL and Python,structur queri languag queri languag primarili aim store manipul retriev datapython generalpurpos program languag enabl experiment databut structur queri languag faster python thus use extream larg data
302,"Basics of Relational Database (RDBMS)

A relational database is a type of database that stores and provides access to data points that are related to one another (from inter-related tables).

 The relationship between tables and field types is called a schema. Schema is a logical collection of database objects

> The columns of the table hold attributes of the data, and each record usually has a value for each attribute.

> When the values of minimum one attribute of two tables are same, they are called relational tables or relational database 
e.g. 
Say two tables contains one same attribute (types of job) with same values (service,business), then the two tables are relational",basic relat databas rdbms,"['basic', 'relat', 'databas', 'rdbms']",Basics of Relational Database (RDBMS),relat databas type databas store provid access data point relat one anoth interrel tabl relationship tabl field type call schema schema logic collect databas object column tabl hold attribut data record usual valu attribut valu minimum one attribut two tabl call relat tabl relat databas eg say two tabl contain one attribut type job valu servicebusi two tabl relat
303,"Basics of Non-Relational Database

A non-relational database is any database that does not use the tabular schema of rows and columns like in relational databases. Rather, its storage model is optimized for the type of data it’s storing

For extreamly large dataset non-relational databases (ms word is a simple non-relational database for static data) are used.

NoSQL databases

1. Document-oriented databases

2. Key-Value Stores

3. Wide-Column Stores

4. Graph Stores

-MongoDB (non relational database or NoSQL-semi structured data)",basic nonrel databas,"['basic', 'nonrel', 'databas']",Basics of Non-Relational Database,nonrel databas databas use tabular schema row column like relat databas rather storag model optim type data it storingfor extream larg dataset nonrel databas ms word simpl nonrel databas static data usednosql databases1 documentori databases2 keyvalu stores3 widecolumn stores4 graph storesmongodb non relat databas nosqlsemi structur data
304,"Parts of SQL

1) DDL - Data Definition Language

2) DML - Data Manipulation Language

3) DCL - Data Control Language (View Definition)

4) TCL - Transaction Control Language

> Group of operations that form a single logical unit of work is known as Transaction",part structur queri languag,"['part', 'structur', 'queri', 'languag']",Parts of SQL,1 ddl data definit language2 dml data manipul language3 dcl data control languag view definition4 tcl transact control languag group oper form singl logic unit work known transact
305,"SQL Statements

DDL statements are used to define the database structure or schema. 

1) CREATE

2) ALTER

3) DROP

4) RENAME

5) TRUNCATE

DML statements are used for managing data within schema objects. 

1) SELECT

2) INSERT

3) UPDATE

4) DELETE

5) LOCK

6) CALL

7) EXPLAIN PLAN

DCL statements

1) GRANT (to allow specified users to perform specified tasks)

2) REVOKE (to remove the user accessibility to database object)

TCL statements

1) COMMIT

2) ROLLBACK",structur queri languag statement,"['structur', 'queri', 'languag', 'statement']",SQL Statements,ddl statement use defin databas structur schema 1 create2 alter3 drop4 rename5 truncatedml statement use manag data within schema object 1 select2 insert3 update4 delete5 lock6 call7 explain plandcl statements1 grant allow specifi user perform specifi tasks2 revok remov user access databas objecttcl statements1 commit2 rollback
306,"Practicing sql queries (with static data)

> In PgAdmin schema, we can find all the table names and column names for each table. 

> But while practicing sql queries in colab, we need to see all the table names and column names of each table to understand the schema of the database

import sqlite3

connnection_engine=sqlite3.connect('.db_file_path_in_google_drive') # After mounting drive in colab

my_cursor = connnection_engine.cursor()

>  To view all the table names

my_cursor.execute(""SELECT name FROM sqlite_master WHERE type='table';"")

my_cursor.fetchall()

>  To view all the column names along with datatype of a table

my_cursor.execute(""PRAGMA table_info(table_name); "")

info_list=my_cursor.fetchall()

[k[1] for k in info_list] # To view the column names

> To check the shape of the table

my_cursor.execute(""SELECT COUNT(*) FROM table_name;"")

my_cursor.fetchall() # Returns the number of rows that match a specific condition of a query

> To see the head of the table

my_cursor.execute(""SELECT * FROM table_name LIMIT 5;"")

my_cursor.fetchall()

> To create a new table

connnection_engine=sqlite3.connect('test_database.db')

my_cursor = connnection_engine.cursor()

my_cursor.execute(""CREATE TABLE table_name (
    column1 datatype,
    column2 datatype,
    column3 datatype);"")

> Inserting data into the table

my_cursor.execute('''INSERT INTO table_name VALUES(51,'James',5);''')

> Inserting new column

my_cursor.execute(""ALTER TABLE table_name ADD column_name data_type;"")",practic structur queri languag queri static data,"['practic', 'structur', 'queri', 'languag', 'queri', 'static', 'data']",Practicing sql queries (with static data),pgadmin schema find tabl name column name tabl practic structur queri languag queri colab need see tabl name column name tabl understand schema databaseimport structur queri languageite3connnectionenginestructur queri languageite3connectdbfilepathingoogledr mount drive colabmycursor connnectionenginecursor view tabl namesmycursorexecuteselect name structur queri languageitemast typetablemycursorfetchal view column name along datatyp tablemycursorexecutepragma tableinfotablenam infolistmycursorfetchallk1 k infolist view column name check shape tablemycursorexecuteselect count tablenamemycursorfetchal return number row match specif condit queri see head tablemycursorexecuteselect tablenam limit 5mycursorfetchal creat new tableconnnectionenginestructur queri languageite3connecttestdatabasedbmycursor connnectionenginecursormycursorexecutecr tabl tablenam column1 datatyp column2 datatyp column3 datatyp insert data tablemycursorexecuteinsert tablenam values51james5 insert new columnmycursorexecutealt tabl tablenam add columnnam datatyp
307,"SQL Query/ Statements/ commands

1. Structural commands: SELECT, DISTINCT, ALL,  AS, FROM, WHERE

SELECT * FROM table_name WHERE column_name=5;

2. Comparison commands: IN, BETWEEN, LIKE, ILIKE

3. Grouping commands: GROUP BY, HAVING

SELECT COUNT(DISTINCT column_name) FROM table_name;

> The GROUP BY statement groups rows that have the same values

> HAVING and WHERE are similar operation. “WHERE” is used to filter rows but “HAVING” is used to filter groups

> WHERE” is always used before “GROUP BY” and HAVING after “GROUP BY”

4. Display commands: ORDER BY, ASC/ DESC, LIMIT

""SELECT column_name FROM table_name ORDER BY column_name DESC LIMIT 10; (default for ORDER BY is ASC)

5. Logical Commands: AND, OR, NOT

6. Output commands: INTO TABLE/ CURSOR, TO SCREEN

7. Union Commands: UNION",structur queri languag queri statement command,"['structur', 'queri', 'languag', 'queri', 'statement', 'command']",SQL Query/ Statements/ commands,1 structur command select distinct whereselect tablenam columnname52 comparison command like ilike3 group command group havingselect countdistinct columnnam tablenam group statement group row valu similar oper “where” use filter row “having” use filter group where” alway use “group by” “group by”4 display command order asc desc limitselect columnnam tablenam order columnnam desc limit 10 default order asc5 logic command not6 output command tabl cursor screen7 union command union
308,"SELECT and SELECT DISTINT statements

In order to select the entire table SELECT * is used

SELECT DISTINCT column_name1, column_name2 FROM table_name;

-returns only the unique values from the columns
",select select distint statement,"['select', 'select', 'distint', 'statement']",SELECT and SELECT DISTINT statements,order select entir tabl select usedselect distinct columnname1 columnname2 tablenamereturn uniqu valu column
309,"Conditional operators

=
<
>
<=
>=
!=

SELECT city, temperature, condition FROM weather WHERE condition = 'sunny' OR condition = 'cloudy' AND temperature >= 60",condit oper,"['condit', 'oper']",Conditional operators,select citi temperatur condit weather condit sunni condit cloudi temperatur 60
310,"Reading SQLite Database file (.sqlite3, .sqlite, .db) as pandas df 

my_df = pd.read_sql_query(""select * from table_name;"", connnection_engine)",read sqlite databas file sqlite3 sqlite db panda df,"['read', 'sqlite', 'databas', 'file', 'sqlite3', 'sqlite', 'db', 'panda', 'df']","Reading SQLite Database file (.sqlite3, .sqlite, .db) as pandas df ",mydf pdreadsqlqueryselect tablenam connnectionengin
311,"SQLite

SQLite is very lightweight and comes as already installed dbms server in colab and requires no username and password. 

Thus, SQLite is very useful for practicing SQL queries.

> We can convert sql dadabase file to .sqlite file (rebasedata.com) whenever required",sqlite,['sqlite'],SQLite,structur queri languageit lightweight come alreadi instal dbms server colab requir usernam password thus structur queri languageit use practic structur queri languag queri convert structur queri languag dadabas file structur queri languageit file rebasedatacom whenev requir
312,"File extensions

> Common database file extensions: .tar, .db, .accdb, .nsf, and .fp7

> Postgresql, mysql, mssql and other database file extension: .tar

.db- Mobile Device Database File

.accdb- Access 2007 Database File (mainly used for offline applications)

",file extens,"['file', 'extens']",File extensions,common databas file extens tar db accdb nsf fp7 postgresql mysql mssql databas file extens tardb mobil devic databas fileaccdb access 2007 databas file main use offlin applic
313,"LIKE and ILIKE

LIKE and ILIKE allows us to perform pattern matching against string data with the use of wildcard characters (%, _)

percentage(%)- matches any sequence of characters

underscore(_)- matches any group of characters

LIKE is case-sensitive while ILIKE is case-insensitive

> Wildcard characters are useful When an exact match is not possible in a SELECT statement

WHERE column_name LIKE A% # returns names that begins with A

WHERE column_name LIKE %a # returns names that ends with a

WHERE column_name LIKE 'any_string_' # returns names with any_string

> both the wildcard characters can also be used in combination",like ilik,"['like', 'ilik']",LIKE and ILIKE,like ilik allow us perform pattern match string data use wildcard charact percentag match sequenc charactersunderscor match group characterslik casesensit ilik caseinsensit wildcard charact use exact match possibl select statementwher columnnam like return name begin awher columnnam like return name end awher columnnam like anystr return name anystr wildcard charact also use combin
314,"Subtopics of ADVANCED SQL COMMANDS

> TIMESTAMP & EXTRACT

> SQL TIMESTAMP and DATETIME

> SUB-QUERY 

> SELF JOIN

> CREATING DATABASES & TABLES

> Simple View and Complex View

> Syntax for the CREATE VIEW

> CREATE DATABASE, DROP DATABASE and BACKUP DATABASE

> CREATE TABLE in SQL

> DELETE in SQL

> SQL TRUNCATE TABLE

> SQL ALTER TABLE

> INSERT INTO in SQL

> UPDATE in SQL

> SQL CHECK

> SQL trigger

> Temporary table and Heap table

> SQL indexing

> Writing function in SQL",subtop advanc structur queri languag command,"['subtop', 'advanc', 'structur', 'queri', 'languag', 'command']",Subtopics of ADVANCED SQL COMMANDS,timestamp extract structur queri languag timestamp datetim subqueri self join creat databas tabl simpl view complex view syntax creat view creat databas drop databas backup databas creat tabl structur queri languag delet structur queri languag structur queri languag truncat tabl structur queri languag alter tabl insert structur queri languag updat structur queri languag structur queri languag check structur queri languag trigger temporari tabl heap tabl structur queri languag index write function structur queri languag
315,"TIMESTAMP & EXTRACT

1)TIME : Contains only Time

2)DATE : Contains only Date

3)TIMESTAMP : Contains Time and Date

4)TIMESTAMPTZ : Contains Time, Date and Time Zone

EXTRACT : Extracts YEAR/ MONTH/ DAY/ WEEK/ QUARTER from a date_column

AGE : Calculates and returns the current age given a timestamp

TO_CHAR : Converts date types to text and is useful for formatting",timestamp extract,"['timestamp', 'extract']",TIMESTAMP & EXTRACT,1time contain time2d contain date3timestamp contain time date4timestamptz contain time date time zoneextract extract year month day week quarter datecolumnag calcul return current age given timestamptochar convert date type text use format
316,"SUB-QUERY 

(allows us to construct complex queries, essentially performing a query on the results of another query)

>> This is one of the basic approaches for joining tables. Other approaches are Union Join and  Natural join

>> When we have a subquery inside of the main query, subquery  is executed first",subqueri,['subqueri'],SUB-QUERY ,allow us construct complex queri essenti perform queri result anoth queri one basic approach join tabl approach union join natur join subqueri insid main queri subqueri execut first
317,"SELF JOIN

(Query in which a table is joined to itself. They are useful for comparing values in a column within the same table)

> EQUI JOIN is also called an INNER JOIN",self join,"['self', 'join']",SELF JOIN,queri tabl join use compar valu column within tabl equi join also call inner join
318,"DATA/ DATA STRUCTURE TYPES IN SQL

1)Boolean : True or False

2)Character : Char, Varchar or Text

3)Numeric : Integer or Floating Point Numbers

4)Temporary :  Date, Time, Timestamp and Interval

5)UUID : Universally Unique Identifier

6)Array : Stores an array of strings or numbers

7)JSON (array like data structure)

8)Hstore : Key Value pair

9)Special : Geometrical Data or Network Address",data data structur type structur queri languag,"['data', 'data', 'structur', 'type', 'structur', 'queri', 'languag']",DATA/ DATA STRUCTURE TYPES IN SQL,1boolean true false2charact char varchar text3numer integ float point numbers4temporari date time timestamp interval5uuid univers uniqu identifier6array store array string numbers7json array like data structure8hstor key valu pair9speci geometr data network address
319,"Primary Key and Foreign Key

In SQL, keys are the set of attributes that are used to identify the specific row in a table and to find or create the relationship between two or more tables.

1)Primary Key : Column or Group of Columns that are used to uniquely identify a row in a table. They allow us to easily discern what columns are to be used when joining tables

> We can have only one primary key in a table 

2)Foreign Key: column or group of columns that refers to the primary key/unique key of other table.

i)  Parent Table : Table to which FK references

ii) Child Table : Table that contains the FK

> Primary Key and Foreign Key are used to establish and enforce a link between the data in two tables

Say, we have a column ‘A’ in table1 which is a primary key and it refers to column ‘B’ in table2. Further, column ‘A’ has only three values (1,2,3). Then

> Inserting a value 4 in column ‘B’ of table2 will result in an error

> Inserting a value 4 in column ‘A’ of table1 will be successful",primari key foreign key,"['primari', 'key', 'foreign', 'key']",Primary Key and Foreign Key,sql key set attribut use identifi specif row tabl find creat relationship two tables1primari key column group column use uniqu identifi row tabl allow us easili discern column use join tabl one primari key tabl 2foreign key column group column refer primari keyuniqu key tablei parent tabl tabl fk referencesii child tabl tabl contain fk primari key foreign key use establish enforc link data two tablessay column a table1 primari key refer column b table2 column a three valu 123 insert valu 4 column b table2 result error insert valu 4 column a table1 success
320,"CONSTRAINTS IN SQL

Constraints means the rules enforced on data columns in tables and are used to prevent invalid data being entered in a table. 

Constraints can be divided into 2 types : COLUMN CONSTRAINTS and TABLE CONSTRAINTS

1)NOT NULL (enforces a column to NOT accept NULL values)

2)UNIQUE (ensures that all values in a column are different)

3)PRIMARY KEY (automatically has a UNIQUE constraint and cannot contain NULL values)

4)FOREIGN KEY (to prevent actions that would destroy links between tables)

5)CHECK

Example of CONSTRAINTS

CREATE TABLE Orders (
    OrderID int NOT NULL,
    OrderNumber int NOT NULL,
    PersonID int,
    PRIMARY KEY (OrderID),
    FOREIGN KEY (PersonID) REFERENCES Persons(PersonID)
);
",constraint structur queri languag,"['constraint', 'structur', 'queri', 'languag']",CONSTRAINTS IN SQL,constraint mean rule enforc data column tabl use prevent invalid data enter tabl constraint divid 2 type column constraint tabl constraints1not null enforc column accept null values2uniqu ensur valu column different3primari key automat uniqu constraint cannot contain null values4foreign key prevent action would destroy link tables5checkexampl constraintscr tabl order orderid int null ordernumb int null personid int primari key orderid foreign key personid refer personspersonid
321,"CREATING DATABASES & TABLES

1)CREATE

2)INSERT

3)UPDATE

4)DELETE

5)ALTER 

6)DROP

7)CHECK (CONSTRAINTS)",creat databas tabl,"['creat', 'databas', 'tabl']",CREATING DATABASES & TABLES,1create2insert3update4delete5alt 6drop7check constraint
322,"CONDITIONAL EXPRESSIONS & PROCEDURES

1. CASE

The CASE statement goes through conditions and returns a value when the first condition is met (like an if-else statement)

GENERAL CASE : Allows us to do all kinds of conditional checks

CASE EXPRESSION : Allows us to do only certain checks

SELECT OrderID, Quantity,
CASE
    WHEN Quantity > 30 THEN 'The quantity is greater than 30'
    WHEN Quantity = 30 THEN 'The quantity is 30'
    ELSE 'The quantity is under 30'
END AS QuantityText
FROM OrderDetails;

2. COALESCE

Accepts unlimited  number of arguments and returns the first argument which is not NULL

COALESCE(arg_1, arg_2......arg_n)

-substituting the NULL values with a variable

SELECT (PRICE - COALESCE(DISCOUNT,0))  AS FINAL_PRICE FROM table

3. CAST
Let’s us convert one Data Type into another.

4. NULLIF
This returns a NULL Value if the arguments inside NULLIF() are equal.

5. VIEWS
View can be accessed as a Virtual Table and it does not store the data physically. It simply stores the query. ",condit express procedur,"['condit', 'express', 'procedur']",CONDITIONAL EXPRESSIONS & PROCEDURES,1 caseth case statement goe condit return valu first condit met like ifels statementgener case allow us kind condit checkscas express allow us certain checksselect orderid quantiti case quantiti 30 quantiti greater 30 quantiti 30 quantiti 30 els quantiti 30 end quantitytext orderdetails2 coalesceaccept unlimit number argument return first argument nullcoalescearg1 arg2argnsubstitut null valu variableselect price coalescediscount0 finalpric table3 cast let us convert one data type another4 nullif return null valu argument insid nullif equal5 view view access virtual tabl store data physic simpli store queri
323,"
Simple View and Complex View

There are 2 types of Views in SQL: Simple View and Complex View. 

Simple View: It is the view created by involving only single table.

Complex View: A view based on multiple tables, which contain GROUP BY clause and functions.",simpl view complex view,"['simpl', 'view', 'complex', 'view']","
Simple View and Complex View",2 type view sql simpl view complex view simpl view view creat involv singl tablecomplex view view base multipl tabl contain group claus function
324,"Syntax for the CREATE VIEW

CREATE VIEW view_name AS SELECT columns FROM tables [WHERE conditions];

",syntax creat view,"['syntax', 'creat', 'view']",Syntax for the CREATE VIEW,creat view viewnam select column tabl condit
325,"SQL queries in Python

Step 1: Importing SQLAlchemy, psycopg2 etc.

from sqlalchemy import create_engine

Step 2: Creating a SQL engine

engine = create_engine('mysql://user:password@server')

engine.execute(""CREATE DATABASE dbname"") #create db

engine.execute(""USE dbname"") # select new db

Step 3: Running queries using SQL statements

Step 4: Writing to DB

Step 5: Creating a Table in DB",structur queri languag queri python,"['structur', 'queri', 'languag', 'queri', 'python']",SQL queries in Python,step 1 import structur queri languagealchemi psycopg2 etcfrom structur queri languagealchemi import createenginestep 2 creat structur queri languag engineengin createenginemystructur queri languageuserpasswordserverengineexecutecr databas dbname creat dbengineexecuteus dbname select new dbstep 3 run queri use structur queri languag statementsstep 4 write dbstep 5 creat tabl db
326,"> First install postgreesql and then pgadmin (needs windows 64 bit)

set Port: 5432
set password:  prabir
user: postgres (default)

> Open pgadmin

> load the .tar database

> Then we can see different tables names under schemas

> Now we can write queries to check all the tables 
",first instal postgreesql pgadmin need window 64 bit,"['first', 'instal', 'postgreesql', 'pgadmin', 'need', 'window', '64', 'bit']",> First install postgreesql and then pgadmin (needs windows 64 bit),set port 5432 set password prabir user postgr default open pgadmin load tar databas see differ tabl name schema write queri check tabl
327,"Ways to see the files inside a .tar or .tar.gz file (zip file) without uncompressing by winzip

import tarfile

tar = tarfile.open('.tar file path')

file_names=tar.getnames() 

# most of the files are actually different relational tables in .dat format (.dat files can only be accessed by the application that created them)",way see file insid tar targz file zip file without uncompress winzip,"['way', 'see', 'file', 'insid', 'tar', 'targz', 'file', 'zip', 'file', 'without', 'uncompress', 'winzip']",Ways to see the files inside a .tar or .tar.gz file (zip file) without uncompressing by winzip,import tarfiletar tarfileopentar file pathfilenamestargetnam file actual differ relat tabl dat format dat file access applic creat
328,"SQL aliases

SQL aliases are used to give a table, or a column in a table, a temporary name. Aliases are often used to make column names more readable. 

Syntax

SELECT column_name AS alias_name FROM table_name;

Example

SELECT CustomerID AS ID, CustomerName AS Customer FROM Customers;",structur queri languag alias,"['structur', 'queri', 'languag', 'alias']",SQL aliases,structur queri languag alias use give tabl column tabl temporari name alias often use make column name readabl syntaxselect columnnam aliasnam tablenameexampleselect customerid id customernam custom custom
329,"Different Types of SQL JOINs

(INNER) JOIN:  Returns a table by combining rows/records that have matching values in two or more tables

LEFT (OUTER) JOIN: Returns all rows from the left table, and the matched rows/records from the right table

RIGHT (OUTER) JOIN: Returns all records from the right table, and the matched records from the left table

FULL (OUTER) JOIN: Returns all records when there is a match in either left or right table

e.g.

SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate FROM Orders INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID;",differ type structur queri languag join,"['differ', 'type', 'structur', 'queri', 'languag', 'join']",Different Types of SQL JOINs,inner join return tabl combin rowsrecord match valu two tablesleft outer join return row left tabl match rowsrecord right tableright outer join return record right tabl match record left tableful outer join return record match either left right tableegselect ordersorderid customerscustomernam ordersorderd order inner join custom orderscustomeridcustomerscustomerid
330,"MOD() function

> MOD() function in MySQL is used to find the remainder of one number divided by another.",mod function,"['mod', 'function']",MOD() function,mod function mysql use find remaind one number divid anoth
331,"CREATE DATABASE, DROP DATABASE and BACKUP DATABASE

CREATE DATABASE Syntax

CREATE DATABASE databasename;

DROP DATABASE Syntax

DROP DATABASE databasename;

BACKUP DATABASE Syntax

BACKUP DATABASE databasename TO DISK = 'filepath';",creat databas drop databas backup databas,"['creat', 'databas', 'drop', 'databas', 'backup', 'databas']","CREATE DATABASE, DROP DATABASE and BACKUP DATABASE",creat databas syntaxcr databas databasenamedrop databas syntaxdrop databas databasenamebackup databas syntaxbackup databas databasenam disk filepath
332,"CREATE TABLE in SQL

CREATE TABLE table_name (
    column1 datatype,
    column2 datatype,
    column3 datatype,
   ....
);

Example

CREATE TABLE Persons (
    PersonID int,
    LastName varchar(255),
    FirstName varchar(255),
    Address varchar(255),
    City varchar(255)
);",creat tabl structur queri languag,"['creat', 'tabl', 'structur', 'queri', 'languag']",CREATE TABLE in SQL,creat tabl tablenam column1 datatyp column2 datatyp column3 datatyp examplecr tabl person personid int lastnam varchar255 firstnam varchar255 address varchar255 citi varchar255
333,"DELETE in SQL

DELETE FROM table_name WHERE condition;

Example

DELETE FROM Customers WHERE CustomerName='Alfreds Futterkiste';",delet structur queri languag,"['delet', 'structur', 'queri', 'languag']",DELETE in SQL,delet tablenam conditionexampledelet custom customernamealfr futterkist
334,"SQL TRUNCATE TABLE 

In SQL TRUNCATE TABLE statement always first drops, then re-creates a new table.  It removes all rows from a table without logging the individual row deletions

DROP TABLE command deletes complete table but it would remove complete table structure form the database and we would need to re-create this table once again if we wish we store some data.

> DELETE command can be rolled back but TRUNCATE and DROP command cannot be rolled back

> TRUNCATE is usually faster than DELETE command",structur queri languag truncat tabl,"['structur', 'queri', 'languag', 'truncat', 'tabl']",SQL TRUNCATE TABLE ,structur queri languag truncat tabl statement alway first drop recreat new tabl remov row tabl without log individu row deletionsdrop tabl command delet complet tabl would remov complet tabl structur form databas would need recreat tabl wish store data delet command roll back truncat drop command cannot roll back truncat usual faster delet command
335,"SQL ALTER TABLE 

The SQL ALTER TABLE clause modifies a table definition by altering, adding, or deleting table columns and/or constraints

1. To add a column in a table

ALTER TABLE table_name ADD column_name datatype;

2. To delete a column in a table

ALTER TABLE table_name DROP COLUMN column_name;

3. To change the data type of a column in a table

ALTER TABLE table_name ALTER COLUMN column_name datatype;

> In place of ALTER COLUMN, MODIFY COLUMN may be used",structur queri languag alter tabl,"['structur', 'queri', 'languag', 'alter', 'tabl']",SQL ALTER TABLE ,structur queri languag alter tabl claus modifi tabl definit alter ad delet tabl column andor constraints1 add column tablealt tabl tablenam add columnnam datatype2 delet column tablealt tabl tablenam drop column columnname3 chang data type column tablealt tabl tablenam alter column columnnam datatyp place alter column modifi column may use
336,"SQL string datatypes

> CHAR, 

> VARCHAR, 

> BINARY, 

> VARBINARY, 

> BLOB, 

> TEXT, 

> ENUM, and 

> SET",structur queri languag string datatyp,"['structur', 'queri', 'languag', 'string', 'datatyp']",SQL string datatypes,char varchar binari varbinari blob text enum set
337,"INSERT INTO in SQL

INSERT INTO table_name (column1, column2, column3, ...) VALUES (value1, value2, value3, ...);

INSERT INTO table_name VALUES (value1, value2, value3, ...); # When inserting values for all the columns",insert structur queri languag,"['insert', 'structur', 'queri', 'languag']",INSERT INTO in SQL,insert tablenam column1 column2 column3 valu value1 value2 value3 insert tablenam valu value1 value2 value3 insert valu column
338,"UPDATE in SQL

UPDATE table_name SET column1 = value1, column2 = value2, … WHERE condition;

Example

UPDATE Customers SET ContactName = 'Alfred Schmidt', City= 'Frankfurt' WHERE CustomerID = 1;

> we can UPDATE a single table at a time but we can UPDATE multiple values of multiple columns
",updat structur queri languag,"['updat', 'structur', 'queri', 'languag']",UPDATE in SQL,updat tablenam set column1 value1 column2 value2 … conditionexampleupd custom set contactnam alfr schmidt citi frankfurt customerid 1 updat singl tabl time updat multipl valu multipl column
339,"SQL CHECK 

SQL CHECK can be used along with CREATE, ALTER, DROP commands

CHECK on CREATE TABLE

CREATE TABLE Persons (
    ID int NOT NULL,
    LastName varchar(255) NOT NULL,
    FirstName varchar(255),
    Age int CHECK (Age>=18)
);

CHECK on ALTER TABLE

ALTER TABLE Persons ADD CHECK (Age>=18);
",structur queri languag check,"['structur', 'queri', 'languag', 'check']",SQL CHECK ,structur queri languag check use along creat alter drop commandscheck creat tablecr tabl person id int null lastnam varchar255 null firstnam varchar255 age int check age18 check alter tablealt tabl person add check age18
340,"UNION in SQL

JOIN combines data columnwise and UNION combines data rowwise.

UNION/UNION ALL simply adds data vertically.

SELECT Name 
FROM Boys 
WHERE Rollno < 16 

UNION

SELECT Name 
FROM Girls 
WHERE Rollno > 9 

UNION ALL

The difference between UNION and UNION ALL is that UNION ALL will not eliminate duplicate rows, instead it just pulls all rows from all tables fitting our query specifics and combines them into a table.

SELECT column_name(s) FROM table1 UNION ALL SELECT column_name(s) FROM table2;

> SELECT on a MERGE table is like UNION ALL 

MERGE() is the combination of three statements (INSERT, DELETE and UPDATE)

In MERGE, we specify a ""Source"" record set and a ""Target"" table and the JOIN condition between the two.",union structur queri languag,"['union', 'structur', 'queri', 'languag']",UNION in SQL,join combin data columnwis union combin data rowwiseunionunion simpli add data verticallyselect name boy rollno 16 unionselect name girl rollno 9 union allth differ union union union elimin duplic row instead pull row tabl fit queri specif combin tableselect columnnam table1 union select columnnam table2 select merg tabl like union merg combin three statement insert delet updatein merg specifi sourc record set target tabl join condit two
341,"SQL TIMESTAMP and DATETIME 

SQL TIMESTAMP and DATETIME both store without time zone.

TIMESTAMP is stored in UTC (Coordinated Universal Time) values, and DATETIME is stored in without time zone.

UTC is effectively a successor to Greenwich Mean Time (GMT).

Both the data type can be used for values that contain both date and time parts.

In ' YYYY-MM-DD hh:mm:ss ' format, the supported range DATETIME is '1000-01-01 00:00:00' to '9999-12-31 23:59:59' . 

 TIMESTAMP has a range of '1970-01-01 00:00:01' UTC to '2038-01-19 03:14:07' UTC.",structur queri languag timestamp datetim,"['structur', 'queri', 'languag', 'timestamp', 'datetim']",SQL TIMESTAMP and DATETIME ,structur queri languag timestamp datetim store without time zonetimestamp store utc coordin univers time valu datetim store without time zoneutc effect successor greenwich mean time gmtboth data type use valu contain date time partsin yyyymmdd hhmmss format support rang datetim 10000101 000000 99991231 235959 timestamp rang 19700101 000001 utc 20380119 031407 utc
342,"SQL trigger

SHOW TRIGGERS; returns a list of triggers in the current database

A SQL trigger is a database object which fires when an event occurs in a database. It consist of procedural code (includes procedural and SQL statements)

For example, a trigger can be set on a record insert in a database table.",structur queri languag trigger,"['structur', 'queri', 'languag', 'trigger']",SQL trigger,show trigger return list trigger current databasea structur queri languag trigger databas object fire event occur databas consist procedur code includ procedur structur queri languag statementsfor exampl trigger set record insert databas tabl
343,"SQL NOT IN

NOT IN operator can be used with a multiple-row subquery

SELECT column_name1, column_name2, etc
FROM table_name
WHERE  column_name1 NOT IN (value1, value2, etc);",structur queri languag,"['structur', 'queri', 'languag']",SQL NOT IN,oper use multiplerow subqueryselect columnname1 columnname2 etc tablenam columnname1 value1 value2 etc
344,"Operation with Null values in SQL

Null + 1 = Null
Null * 2 = Null

> IS NULL operator is used to compare the NULL values in SQL

SELECT column_names
FROM table_name
WHERE column_name IS NULL;

IS NOT NULL Syntax

SELECT column_names
FROM table_name
WHERE column_name IS NOT NULL;",oper null valu structur queri languag,"['oper', 'null', 'valu', 'structur', 'queri', 'languag']",Operation with Null values in SQL,null 1 null null 2 null null oper use compar null valu sqlselect columnnam tablenam columnnam nulli null syntaxselect columnnam tablenam columnnam null
345,"SQL CONCAT

In SQL, the CONCAT() takes 2 to 255 input strings and joins them into one

SELECT CONCAT('W3Schools', '.com');

To convert 212-555-1212 to ***-555-1212 where 212-555-1212 is a data from the column named ""phone""

concat(left(phone,3),'***')",structur queri languag concat,"['structur', 'queri', 'languag', 'concat']",SQL CONCAT,sql concat take 2 255 input string join oneselect concatw3school comto convert 2125551212 5551212 2125551212 data column name phoneconcatleftphone3
346,"SQL and String

String  index starts from 1 in SQL

INSTR ('ALMAX POINT', 'P') returns 7 as the index of 'P' is 7 in the given string

LEFT function allows us to extract a substring from a string, starting from the left-most character.

Syntax

LEFT(string_value , number_of_characters_to_be_returned)",structur queri languag string,"['structur', 'queri', 'languag', 'string']",SQL and String,string index start 1 sqlinstr almax point p return 7 index p 7 given stringleft function allow us extract substr string start leftmost charactersyntaxleftstringvalu numberofcharacterstobereturn
347,"Temporary table and Heap table

In SQL, a temporary table is a base table that is not stored in the database, but instead exists only while the database session in which it was created is active.

Heap tables are tables without a Clustered Index. In a heap table, the data is not sorted in any way, it's just a pile of unordered, unstructured records",temporari tabl heap tabl,"['temporari', 'tabl', 'heap', 'tabl']",Temporary table and Heap table,sql temporari tabl base tabl store databas instead exist databas session creat activeheap tabl tabl without cluster index heap tabl data sort way pile unord unstructur record
348,"Sequence in SQL

Sequence is a set of integers 1, 2, 3, … that are generated and supported by some database systems to produce unique values on demand.

A sequence in SQL can generate a maximum 38 digits number

Sequence can generate Numeric value or Alphanumeric value

CREATE SEQUENCE sequence_name
START WITH initial_value
INCREMENT BY increment_value
MINVALUE minimum value
MAXVALUE maximum value
CYCLE|NOCYCLE ;",sequenc structur queri languag,"['sequenc', 'structur', 'queri', 'languag']",Sequence in SQL,sequenc set integ 1 2 3 … generat support databas system produc uniqu valu demanda sequenc structur queri languag generat maximum 38 digit numbersequ generat numer valu alphanumer valuecr sequenc sequencenam start initialvalu increment incrementvalu minvalu minimum valu maxvalu maximum valu cyclenocycl
349,"In SQL When we need an exact copy of a table with all columns and indexes

SHOW CREATE TABLE tablename

",structur queri languag need exact copi tabl column index,"['structur', 'queri', 'languag', 'need', 'exact', 'copi', 'tabl', 'column', 'index']",In SQL When we need an exact copy of a table with all columns and indexes,show creat tabl tablenam
350,"Super Key and Candidate Key

> A superkey is a group of single or multiple keys which identifies rows in a table. A Super key may have additional attributes that are not needed for unique identification.

> Candidate Key in SQL is a set of attributes that uniquely identify tuples in a table. Candidate Key is a super key with no repeated attributes.

I. Minimal super key is a candidate key

II. Only one candidate key can be a primary key",super key candid key,"['super', 'key', 'candid', 'key']",Super Key and Candidate Key,superkey group singl multipl key identifi row tabl super key may addit attribut need uniqu identif candid key structur queri languag set attribut uniqu identifi tupl tabl candid key super key repeat attributesi minim super key candid keyii one candid key primari key
351,"Main branches of pure mathematics

1. Arithmetic (It deals with numbers and the basic operations are +-*/)

2. Algebra (It is a kind of arithmetic where we use unknown quantities along with numbers)

3. Geometry (It is the most practical branch of mathematics that deals with shapes and sizes of figures and their properties)

4. Trigonometry (It is the study of relationships between angles and sides of triangles)

5. Calculus (It is the branch that deals with the study of the rate of change in different quantities. Calculus forms the base of analysis)

6. Probability and Statistics (Probability is all about chance of any future event. Whereas statistics is all about understanding a data or dataset based on different measurements or scores.)",main branch pure mathemat,"['main', 'branch', 'pure', 'mathemat']",Main branches of pure mathematics,1 arithmet deal number basic oper 2 algebra kind arithmet use unknown quantiti along numbers3 geometri practic branch mathemat deal shape size figur properties4 trigonometri studi relationship angl side triangles5 calculus branch deal studi rate chang differ quantiti calculus form base analysis6 probabl statist probabl chanc futur event wherea statist understand data dataset base differ measur score
352,"Application of mathematics in machine learning

""What gets measured gets managed""

>> From the knowledge of arithmetic, we measure different feature values of different observations of any experience and store in a dataset or table

>> From the knowledge of statistics, we understand the dataset through different scores of the features

>> From the knowledge of probability, we understand the dataset through probability distributions

>> From the knowledge of algebra, we form different equations and find the values of the dependent variable

>> From the knowledge of calculus, we analyse the dataset with respect to different feature variables. It is also used (in GD) to find values of the dependent variable for large dataset

>> From the knowledge of geometry and trigonometry, we understand the dataset and calculations through visualization

>> Finally, from the knowledge of vector algebra and matrix algebra, we understand any data as a point in the vector space. Thus handling and use of data becomes simple and efficient.",applic mathemat machin learn,"['applic', 'mathemat', 'machin', 'learn']",Application of mathematics in machine learning,get measur get manag knowledg arithmet measur differ featur valu differ observ experi store dataset tabl knowledg statist understand dataset differ score featur knowledg probabl understand dataset probabl distribut knowledg algebra form differ equat find valu depend variabl knowledg calculus analys dataset respect differ featur variabl also use gd find valu depend variabl larg dataset knowledg geometri trigonometri understand dataset calcul visual final knowledg vector algebra matrix algebra understand data point vector space thus handl use data becom simpl effici
353,"Radian measure

> Calculus is always done in radian measure. Radians make it possible to relate a linear measure and an angle measure.

> Radian is a unit of measurement of angles equal to about 57.3° and 
π (3.14) radians = 180°

> π is the is the ratio of the circumference of any circle to the diameter of that circle and the value of π is 3.14",radian measur,"['radian', 'measur']",Radian measure,calculus alway done radian measur radian make possibl relat linear measur angl measur radian unit measur angl equal 573° π 314 radian 180° π ratio circumfer circl diamet circl valu π 314
354,"Machine Learning Use Cases of Calculus

1. Numerical Optimization

2. Gradient Computations

3. Probability Density Functions

4. Variational Inference and Related Techniques ",machin learn use case calculus,"['machin', 'learn', 'use', 'case', 'calculus']",Machine Learning Use Cases of Calculus,1 numer optimization2 gradient computations3 probabl densiti functions4 variat infer relat techniqu
355,"Basics of derivative 

Derivative means,

> Instantaneous rate of change (in Physics)

> Slope of a line at a specific point (in Geometry)

If a function is differentiable, it must it be continuous

Derivative of y = f(x) with respect to x is represented as dy/dx or f’(x)

A function is differentiable at a < c < b if and only if the left hand derivatives (LHD-slope calculated at the left side of the point)  and right hand derivatives (RHD- slope calculated at the right side of the point) at c both exist and are equal",basic deriv,"['basic', 'deriv']",Basics of derivative ,deriv mean instantan rate chang physic slope line specif point geometryif function differenti must continuousderiv fx respect x repres dydx f'xa function differenti c b left hand deriv lhdslope calcul left side point right hand deriv rhd slope calcul right side point c exist equal
356,"The chain rule

The chain rule is a formula for calculating the derivatives of composite functions (functions composed of functions inside another function(s))

f(x) = h(g(x))
df/dx  = df/dg*dg/dx",chain rule,"['chain', 'rule']",The chain rule,chain rule formula calcul deriv composit function function compos function insid anoth functionsfx hgx dfdx dfdgdgdx
357,"Point of maxima and point of minima

In a smoothly changing function, a maximum or minimum is always where the function flattens out (except for a saddle point). Where the slope is zero.

f'(x)=0
=>x= x0 , x1

The second derivative will tell us which value is maxima and which one is minima.

If f’’(x0) < 0 => x0 is point of maxima

If f’’(x0) > 0 => x0 is point of minima",point maxima point minima,"['point', 'maxima', 'point', 'minima']",Point of maxima and point of minima,smooth chang function maximum minimum alway function flatten except saddl point slope zerofx0 x x0 x1the second deriv tell us valu maxima one minimaif f''x0 0 x0 point maximaif f''x0 0 x0 point minima
358,"Partial Derivatives

Let’s take an example: 
z = f(x, y)

If we change x, but hold all other variables constant, how does f(x, y) change? That’s one partial derivative. 

The next variable is y. If we change y but hold x constant, how does f(x, y) change?",partial deriv,"['partial', 'deriv']",Partial Derivatives,let take exampl z fx yif chang x hold variabl constant fx chang that one partial deriv next variabl chang hold x constant fx chang
359,"Ways to find the slope of f(x,y)

slope of f(x) is a line but slope of z=f(x,y) will be a plane

slope of a plane is represented by a vector 

<dz/dx, dz/dy>",way find slope fxi,"['way', 'find', 'slope', 'fxi']","Ways to find the slope of f(x,y)",slope fx line slope zfxi planeslop plane repres vector dzdx dzdi
360,"Jacobian Matrix

Jacobian matrices are used to transform the infinitesimal vectors from one coordinate system to another. We will mostly be interested in the Jacobian matrices that allow transformation from the Cartesian to a different coordinate system.

The determinant of the jacobian matrix is called jacobian",jacobian matrix,"['jacobian', 'matrix']",Jacobian Matrix,jacobian matric use transform infinitesim vector one coordin system anoth most interest jacobian matric allow transform cartesian differ coordin systemth determin jacobian matrix call jacobian
361,"Derivatives Formulas 

f(x)= x^n, 

f’(x)= nx^(n-1)

f(x)= e^g(x)

f’(x)= e^g(x)* g'(x)

f(x)= m(x)*g(x)

f’(x)= m(x)*g'(x)+ m'(x)*g(x)

f(x)=  ln(x)

f’(x)= 1/x

f(x)=  ln(g(x))

f’(x)= 1/g(x)*g'(x)

f(x)=  x^x

f’(x)= x^x*(1+ln(x))

f(x)= sin(x)

f'(x) = cos(x)

f(x)= tan(x)

f'(x) = sec2(x)

f(x)= cot(x)=1/tan(x)

f'(x) = -cosec2(x)",deriv formula,"['deriv', 'formula']",Derivatives Formulas ,fx xn f'x nxn1fx egxf'x egx gxfx mxgxf'x mxgx mxgxfx lnxf'x 1xfx lngxf'x 1gxgxfx xxf'x xx1lnxfx sinxfx cosxfx tanxfx sec2xfx cotx1tanxfx cosec2x
362,"Definite Integrals

A definite integral is the area under a curve between two fixed limits. 

The integral of f(x) corresponds to the computation of the area under the graph of f(x).",definit integr,"['definit', 'integr']",Definite Integrals,definit integr area curv two fix limit integr fx correspond comput area graph fx
363,"Formulas for integration

∫x^n dx =x^(n+1)/(n+1) + C

∫e^g(x)dx = e^g(x)*1/g'(x) + C

∫m(x)*g(x) dx = m(x)*∫g(x)dx -∫g(x)*m'(x)dx + C

∫(1/x) dx = ln(x) + C

∫sin(x)dx = - cos(x) + C",formula integr,"['formula', 'integr']",Formulas for integration,∫xn dx xn1n1 c∫egxdx egx1gx c∫mxgx dx mx∫gxdx ∫gxmxdx c∫1x dx lnx c∫sinxdx cosx c
364,"Basics of Limit

Limit is used to solve a function for a particular value where it becomes indeterminate (0/0, 1/0, ∞/∞)

e.g. f(x)=(x^2-4)/(x-2) is indeterminate at x=2

Then we need to find Lim f(x) at x approaches 2

> x can tend to 2 from two sides : positive higher values or from negative higher values",basic limit,"['basic', 'limit']",Basics of Limit,limit use solv function particular valu becom indetermin 00 10 ∞∞eg fxx24x2 indetermin x2then need find lim fx x approach 2 x tend 2 two side posit higher valu negat higher valu
365,"Solution method of limit of indeterminate form

1. Factorization method

In factorization method, we convert indeterminate form to determinate form by factorizing the numerator and denominator with negative limit value first

Lim f(x) at x approaches 2=(x^2-4)/(x-2)=(x-2)(x+2)/(x-2)=x+2=4

2. By L' Hospital's Rule (also known as Bernoulli's rule)
 
Differentiate the numerator and differentiate the denominator and then take the limit.

Lim f(x)/g(x) at x approaches a = Lim f'(x)/g'(x) at x approaches a

Lim f(x) at x approaches 2 =(x^2-4)/(x-2)=2x/1=2x=4",solut method limit indetermin form,"['solut', 'method', 'limit', 'indetermin', 'form']",Solution method of limit of indeterminate form,1 factor methodin factor method convert indetermin form determin form factor numer denomin negat limit valu firstlim fx x approach 2x24x2x2x2x2x242 l hospit rule also known bernoulli rule differenti numer differenti denomin take limitlim fxgx x approach lim fxgx x approach alim fx x approach 2 x24x22x12x4
366," fraction

A fraction has two parts. The number on the top of the line is called the numerator. The number below the line is called the denominator.",fraction,['fraction'], fraction,fraction two part number top line call numer number line call denomin
367,"Sub-branches of Algebra 

Algebra is divided into different sub-branches such as
 
1. Elementary algebra 

2. Advanced algebra 

3. Abstract algebra 

4. Linear algebra and 

5. Commutative algebra
",subbranch algebra,"['subbranch', 'algebra']",Sub-branches of Algebra ,algebra divid differ subbranch 1 elementari algebra 2 advanc algebra 3 abstract algebra 4 linear algebra 5 commut algebra
368,"Linear algebra

It concerns the linear equations for the linear functions with their representation in vector spaces (=sample spaces) and through the matrices.

To fully comprehend machine learning, linear algebra fundamentals are the essential prerequisite.

In machine learning, the majority of data is most often represented as vectors, matrices or tensors. Therefore, the machine learning heavily relies on the linear algebra.

> Numpy is a linear algebra library for python. 

> Pandas is built on Numpy through Numpy API for Data Science area.

> Sklearn is built on Numpy through Numpy API for Machine Learning area.

> TensorFlow is built on Numpy through Numpy API for deep learning area.",linear algebra,"['linear', 'algebra']",Linear algebra,concern linear equat linear function represent vector space sampl space matricesto fulli comprehend machin learn linear algebra fundament essenti prerequisitein machin learn major data often repres vector matric tensor therefor machin learn heavili reli linear algebra numpi linear algebra librari python panda built numpi numpi applic program interfac data scienc area sklearn built numpi numpi applic program interfac machin learn area tensorflow built numpi numpi applic program interfac deep learn area
369,"Scaler, Vector, Matrix and Tensor

Scaler: a single number.

A vector is a 1D array: a list of numbers

A matrix is a 2D array of numbers, that has a fixed number of rows and columns: a list of lists or a list of vectors or a list of experiences like Pandas DataFrame 

The term tensor is generally used for >2D array. But actually tensor can be of 1 to n dmensions.

A tensor of dimension one is a vector. 

A vector <1,2,3...,n> can be called a matrix because a list n elements may be represented as a list of n number of lists each having one element [[1],[2],[3],...[n]]

A Tensor of Dimension three may be flattened to dimension two and then it can again be flattened to dimension one. 

Thus tensor and array are same.

> An array can be of one dimension to n dimensions",scaler vector matrix tensor,"['scaler', 'vector', 'matrix', 'tensor']","Scaler, Vector, Matrix and Tensor",scaler singl numbera vector 1d array list numbersa matrix 2d array number fix number row column list list list vector list experi like panda datafram term tensor general use 2d array actual tensor 1 n dmensionsa tensor dimens one vector vector 123n call matrix list n element may repres list n number list one element 123na tensor dimens three may flatten dimens two flatten dimens one thus tensor array array one dimens n dimens
370,"Dimension and Direction

Dimension is the mathematical structure for storing or plotting data. This structure can be of one dimensional to n dimensional. 

Whereas direction is the movement of an experience in that structure or environment.

> With two dimensions we can create four(2^2) spaces (x,y), (x,-y), (-x,y), (-x,-y) during plotting of data

> With three dimensions we can create eight(2^3) spaces (x,y,z), (x,-y,z), (-x,y,z), (-x,-y,z), (x,y,-z), (x,-y,-z), (-x,y,-z), (-x,-y,-z) during plotting of data ",dimens direct,"['dimens', 'direct']",Dimension and Direction,dimens mathemat structur store plot data structur one dimension n dimension wherea direct movement experi structur environ two dimens creat four22 space xy xy xy xy plot data three dimens creat eight23 space xyz xyz xyz xyz xyz xyz xyz xyz plot data
371,"position vector/ location vector/ radius vector

The magnitude of movement of an experience in any direction is the scalar quantity or numbers.

A vector is a mathematical quantity with both magnitude and direction. 

> In geometry, a position or position vector, also known as location vector or radius vector, is a Euclidean vector that represents the position of a point P (x,y,z) in space in relation to an arbitrary reference origin

> A position vector,r may be represented as <x,y> or <rcosθ, rsinθ> 

Here r is magnitude and θ is derection of the position vector",posit vector locat vector radius vector,"['posit', 'vector', 'locat', 'vector', 'radius', 'vector']",position vector/ location vector/ radius vector,magnitud movement experi direct scalar quantiti numbersa vector mathemat quantiti magnitud direct geometri posit posit vector also known locat vector radius vector euclidean vector repres posit point p xyz space relat arbitrari refer origin posit vectorr may repres xy rcosθ rsinθ r magnitud θ derect posit vector
372,"Applications of Linear Algebra in Data Science

1. Coordinate Transformations

2. Linear Regression

3. Dimensionality Reduction

4. Natural Language Processing

5. Computer Vision

6. Network Graphs etc.",applic linear algebra data scienc,"['applic', 'linear', 'algebra', 'data', 'scienc']",Applications of Linear Algebra in Data Science,1 coordin transformations2 linear regression3 dimension reduction4 natur languag processing5 comput vision6 network graph etc
373,"Vector operations

Vector operation makes the calculation simple and superfast. Thus we can directly perfom different operations on the column (vector) of a dataset without going for individual operation on every element using loop.

1. Scalar multiplication with vector

2. Vector Addition

If vector,
a.shape=(2,3) and b.shape=(3,1)

Then,  
> vector addition, a+b will give error because atleast one dimension (row/column ) should match

> vector addition with transpose, a+b.T will give result and shape of resultant vector will be (2,3)

3. Vector Subtraction

4. Vector dot product

5. Vector projection

6. Vector cross product

7. Vector norms",vector oper,"['vector', 'oper']",Vector operations,vector oper make calcul simpl superfast thus direct perfom differ oper column vector dataset without go individu oper everi element use loop1 scalar multipl vector2 vector additionif vector ashape23 bshape31then vector addit ab give error atleast one dimens rowcolumn match vector addit transpos abt give result shape result vector 233 vector subtraction4 vector dot product5 vector projection6 vector cross product7 vector norm
374,"Vector Dot Product

The dot product is a fundamental way we can combine two vectors. 

Intuitively, it tells us something about how much two vectors point in the same direction.

Geometric way, 
a.b=|a||b|cosθ, where θ is the angle between vector a and b

or, Arithmetic way,
a.b = (a1i + a2j + a3k) . (b1i + b2j + b3k)

> In coordinate system, as the vector i, j and k are perpendicular to each other, i.i=1, j.j=1 and k.k=1, i.j=0, j.k=0, i.k=0

Thus,
a.b=a1b1+a2b2+a3b3+…..+anbn, where vector a=(a1,a2,a3,..an) and vector b=(b1,b2,b3,…bn)

If vector,
a.shape=(2,3) and b.shape=(3,1)

> Vector dot product, np.dot(a,b) will give result and the shape of the resultant vector will be (2,1). 

> Vector dot product gives the result of matrix multiplication because dot products are done between the rows of the first matrix and the columns of the second matrix.",vector dot product,"['vector', 'dot', 'product']",Vector Dot Product,dot product fundament way combin two vector intuit tell us someth much two vector point directiongeometr way ababcosθ θ angl vector bor arithmet way ab a1i a2j a3k b1i b2j b3k coordin system vector j k perpendicular ii1 jj1 kk1 ij0 jk0 ik0thus aba1b1a2b2a3b3…anbn vector aa1a2a3an vector bb1b2b3…bnif vector ashape23 bshape31 vector dot product npdotab give result shape result vector 21 vector dot product give result matrix multipl dot product done row first matrix column second matrix
375,"Vector Projection

The projection of u onto v is another vector that is parallel to v and has a length equal to what vector u's shadow would be

projection of vector a onto the vector b: 
vector p = (a·b / b·b) * b 

>> The magnitude of any vector is also called the modulus or the length of the vector.",vector project,"['vector', 'project']",Vector Projection,project u onto v anoth vector parallel v length equal vector us shadow would beproject vector onto vector b vector p a·b b·b b magnitud vector also call modulus length vector
376,"Vector Cross product

The cross product of any two vectors is a vector that is perpendicular to the two vectors. It has both magnitude and direction. The magnitude of the resultant vector is equal to the area of the parallelogram, whose side lengths are equal to the magnitude of the two given vectors.

Geometric way, 
a x b=|a||b|sinθ

or,Arithmetic way,
a × b = (a1i + a2j + a3k) X (b1i + b2j + b3k)

> In coordinate system, as the unit vector i, j and k are perpendicular to each other, iXi=0, jXj=0 and kXk=0, iXj=k, jXk=i, kXi=j, j x i= -k. k x j= -i. i x k= -j.",vector cross product,"['vector', 'cross', 'product']",Vector Cross product,cross product two vector vector perpendicular two vector magnitud direct magnitud result vector equal area parallelogram whose side length equal magnitud two given vectorsgeometr way x babsinθorarithmet way × b a1i a2j a3k x b1i b2j b3k coordin system unit vector j k perpendicular ixi0 jxj0 kxk0 ixjk jxki kxij j x k k x j x k j
377,"Vector norms

L1 norm 

It is defined as the sum of magnitudes of each component

a =  ( a1 , a2 , a3 )

L1 norm of vector a  =  |a1| + |a2| + |a3|

> This is like manhattan distance (Minkowski distance with p = 1)

L2 norm 

It is defined as the square root of sum of squares of each component 

L2 norm of vector a =  √( a1^2  + a2^2 + a3^2 )

> This is like euclidean distance (Minkowski distance with p = 2)

> L1 and L2 norms are used for minimizing the loss function",vector norm,"['vector', 'norm']",Vector norms,l1 norm defin sum magnitud componenta a1 a2 a3 l1 norm vector a1 a2 a3 like manhattan distanc minkowski distanc p 1l2 norm defin squar root sum squar compon l2 norm vector √ a12 a22 a32 like euclidean distanc minkowski distanc p 2 l1 l2 norm use minim loss function
378,"Representing multivariable linear equation in vector space

1. Say we are considering a list type data structure with 3 numbers of integer data, i.e. [2,4,6]

2. [2,4,6] is a 1D array or a vector

3. So, we can write 

vector y= <2,4,6> or
vector y= vector2+ vector4+ vector6 or 
y=x1+x2+x3 considering x,y and z axis

4. Here the resultant vector y will be from point (0,0,0) to point (2,4,6) in x,y,z coordinate system",repres multivari linear equat vector space,"['repres', 'multivari', 'linear', 'equat', 'vector', 'space']",Representing multivariable linear equation in vector space,1 say consid list type data structur 3 number integ data ie 2462 246 1d array vector3 write vector 246 vector vector2 vector4 vector6 yx1x2x3 consid xy z axis4 result vector point 000 point 246 xyz coordin system
379,"Multivariable linear equations

y=1*x1+1*x2+1*x3, 1 is the slope of the line when x2 vector and x3 vector are zero.

y=5*x1+3*x2+4*x3, where 5 is the slope of the line when x2 vector and x3 vector are zero.

",multivari linear equat,"['multivari', 'linear', 'equat']",Multivariable linear equations,y1x11x21x3 1 slope line x2 vector x3 vector zeroy5x13x24x3 5 slope line x2 vector x3 vector zero
380,"Visualization of vector

Vector we can understand as straight pipeline.

The slopes 1,5,3,4 are scalar quantities and these are multiplied with vectors and we know that scalar multiplication changes the magnitude of a vector. Thus it is better to understand them as weights per unit length.

If the magnitude of a vector is length, then it is length vector and if the magnitude of a vector is weight, then it is a weight vector (after multiplication with weight per unit length)

Thus we can write an observation row (vector) having a list of elements as linear equation and visualize the experience in a vector space as a piping layout.

The changing weight per unit length in different direction can be understood as changing diameter of the pipeline (weight per unit length is the property of the dimension)",visual vector,"['visual', 'vector']",Visualization of vector,vector understand straight pipelineth slope 1534 scalar quantiti multipli vector know scalar multipl chang magnitud vector thus better understand weight per unit lengthif magnitud vector length length vector magnitud vector weight weight vector multipl weight per unit lengththus write observ row vector list element linear equat visual experi vector space pipe layoutth chang weight per unit length differ direct understood chang diamet pipelin weight per unit length properti dimens
381,"Types of Matrices

1. Symmetric matrix (in linear algebra, a symmetric matrix is a square matrix that is equal to its transpose)

2. Anti-symmetric matrix

3. Column matrix

4. Row matrix",type matric,"['type', 'matric']",Types of Matrices,1 symmetr matrix linear algebra symmetr matrix squar matrix equal transpose2 antisymmetr matrix3 column matrix4 row matrix
382,"Matrix operations

Addition, Subtraction, multiplication

> For addition or subtraction of matrix A and B, both the matrices needs to have same dimension",matrix oper,"['matrix', 'oper']",Matrix operations,addit subtract multipl addit subtract matrix b matric need dimens
383,"Matrix multiplication

For matrix multiplication, 
No. of elements in column of 1st matrix must be equal to no. of elements in row of 2nd matrix

Size of resultant matrix = Row of 1st matrix* Column of 2nd matrix

Matrix A = (aij) where i=rows, j=columns

Thus, 1st element of resultant matrix = (a11*b11+ a12*b21+ a13*b31+ a1n*bn1)

Matrix multiplication induces some transformation (rotation/ reflection/ shearing etc.) on other vector or matrix and gives the resultant vector or matrix",matrix multipl,"['matrix', 'multipl']",Matrix multiplication,matrix multipl element column 1st matrix must equal element row 2nd matrixs result matrix row 1st matrix column 2nd matrixmatrix aij irow jcolumnsthus 1st element result matrix a11b11 a12b21 a13b31 a1nbn1matrix multipl induc transform rotat reflect shear etc vector matrix give result vector matrix
384,"Equation in matrix form

K11X1+K12X2=R1,
K21X1+K22X2=R2 can be written as

(K11 K12)(X1)=(R1)
k21 K22   X2   R2

or, KX = R

or, X= (K−inverse)*R
 
Thus, we can find the values of x1 and x2",equat matrix form,"['equat', 'matrix', 'form']",Equation in matrix form,k11x1k12x2r1 k21x1k22x2r2 written ask11 k12x1r1 k21 k22 x2 r2or kx ror x k−invers thus find valu x1 x2
385,"Identity matrix

A square matrix in which all the elements of the principal diagonal are ones and all other elements are zeros. The effect of multiplying a given matrix by an identity matrix is to leave the given matrix unchanged.

> If a matrix is multiplied with its conjugate matrix and give an identity matrix, we call it an unitary matrix

> A diagonal matrix  has non-negative real numbers on the diagonal",ident matrix,"['ident', 'matrix']",Identity matrix,squar matrix element princip diagon one element zero effect multipli given matrix ident matrix leav given matrix unchang matrix multipli conjug matrix give ident matrix call unitari matrix diagon matrix nonneg real number diagon
386,"Determinant of a matrix

In mathematics, the determinant is a scalar value that is a function of the entries of a square matrix.

The determinant is useful for solving linear equations.

>> Determiants can only be found for a square matrix. Means, no. of equations (observations) must be equal to no. of unknowns (features)

Easy way is diagonal method

For 3X3 matrix,

A = (a b c)
     d e f
     g h i

|A| = a(ei − fh) − b(di − fg) + c(dh − eg)",determin matrix,"['determin', 'matrix']",Determinant of a matrix,mathemat determin scalar valu function entri squar matrixth determin use solv linear equat determi found squar matrix mean equat observ must equal unknown featureseasi way diagon methodfor 3x3 matrixa b c e f g h ia aei − fh − bdi − fg cdh − eg
387,"Transpose of a matrix

In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal; simply means we are interchanging the rows and columns.",transpos matrix,"['transpos', 'matrix']",Transpose of a matrix,linear algebra transpos matrix oper flip matrix diagon simpli mean interchang row column
388,"Adjoint of a matrix

The adjugate or classical adjoint of a square matrix is the transpose of its cofactor matrix. It is also occasionally known as adjunct matrix
",adjoint matrix,"['adjoint', 'matrix']",Adjoint of a matrix,adjug classic adjoint squar matrix transpos cofactor matrix also occasion known adjunct matrix
389,"cofactor matrix 

A cofactor matrix is formed by finding the cofactors for all elements. Cofactor of any element is obtained by eliminating the row and column of that particular element and then finding the determinant.",cofactor matrix,"['cofactor', 'matrix']",cofactor matrix ,cofactor matrix form find cofactor element cofactor element obtain elimin row column particular element find determin
390,"Inverse of a matrix 

Inverse of matrix A= adj(A)/det(A)

Why Do We Need an Inverse?

Because, there is no concept of dividing a matrix by another matrix. Thus we multiply by an inverse, which achieves the same thing.",invers matrix,"['invers', 'matrix']",Inverse of a matrix ,invers matrix adjadetawhi need inversebecaus concept divid matrix anoth matrix thus multipli invers achiev thing
391,"Eigenvalues and Eigenvectors of a square matrix

The eigenvector is a vector which does not change in direction after transformation (matrix) is applied

In simple words, when we find the eigenvector of any big matrix, number of features or columns are reduced substantially. Thus a complex data can be better understood.

> Eigenvalues are just the scaling or magnification factors of the eigenvector or the simple data

Av=λv

or, (A-λI)=0

or, |A-λI|=0

where v is the eigenvector of the matrix A and matrix A only does scaling of the eigenvector. λ is the eigen value or the scaling factor. I is the identity matrix of size A. 

Thus we can find the eigenvalues and eigenvectors of any matrix. 

For some matrices there may not be any eigenvectors. ",eigenvalu eigenvector squar matrix,"['eigenvalu', 'eigenvector', 'squar', 'matrix']",Eigenvalues and Eigenvectors of a square matrix,eigenvector vector chang direct transform matrix appliedin simpl word find eigenvector big matrix number featur column reduc substanti thus complex data better understood eigenvalu scale magnif factor eigenvector simpl dataavλvor aλi0or aλi0wher v eigenvector matrix matrix scale eigenvector λ eigen valu scale factor ident matrix size thus find eigenvalu eigenvector matrix matric may eigenvector
392,"Use of eigen values and eigen vectors

> To Compress the data

> To transform the original features into another feature subspace

> Optimal computational efficiency

> Cluster optimization in k-means clustering

> To better understand and visualize linear mappings

> To understand the stability of mechanical constructions

> For solving systems of differential equations, 

> To recognize images, 

> To interpret and visualize quadratic equations, 

> For image segmentation.",use eigen valu eigen vector,"['use', 'eigen', 'valu', 'eigen', 'vector']",Use of eigen values and eigen vectors,compress data transform origin featur anoth featur subspac optim comput effici cluster optim kmean cluster better understand visual linear map understand stabil mechan construct solv system differenti equat recogn imag interpret visual quadrat equat imag segment
393,"Nilpotent matrix 

Nilpotent matrix is a square matrix with n-dimensional triangular matrix with zeros along the main diagonal.",nilpot matrix,"['nilpot', 'matrix']",Nilpotent matrix ,nilpot matrix squar matrix ndimension triangular matrix zero along main diagon
394,"Importance of Probability theory 

Probability theory is very important in machine learning because, our ML model learns the rules/trend of the training data or training experiences and does prediction about future event. Prdiction is all about calculating the probability/chances of a event to occur.  ",import probabl theori,"['import', 'probabl', 'theori']",Importance of Probability theory ,probabl theori import machin learn machin learn model learn rulestrend train data train experi predict futur event prdiction calcul probabilitych event occur
395,"Set Theory

Set theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects.

An event is a subset of the sample space or universal set (with all possible outcomes).

Universal Set: Set containing all elements and of which all other sets are subsets. Denoted by U.",set theori,"['set', 'theori']",Set Theory,set theori branch mathemat logic studi set inform describ collect objectsan event subset sampl space univers set possibl outcomesunivers set set contain element set subset denot u
396,"Experiment, sample space, observation and experience

> An experiment (e.g. studying in any class) can be infinitely repeated and has a well-defined set of possible outcomes, called the sample space (e.g. column names of a table). 

> One observation means one outcome (e.g. one cell of a table) and one experience means a set of observations (e.g. one row of a table).

> Thus from an experiment, we get a dataset or a set of experiences (e.g. the entire table)",experi sampl space observ experi,"['experi', 'sampl', 'space', 'observ', 'experi']","Experiment, sample space, observation and experience",experi eg studi class infinit repeat welldefin set possibl outcom call sampl space eg column name tabl one observ mean one outcom eg one cell tabl one experi mean set observ eg one row tabl thus experi get dataset set experi eg entir tabl
397,"Intersection and Union of Sets

> Complement of A (denoted by A’ or A^c)- Everything that is not in A 

> Intersection of A and B (denoted by A ∩ B)- Everything in A and B

n(A ∩ B)= n(A)+ n(B)-n(A U B)

The number of elements in the union of set A and B,
n(A∪B) = n(A - B) + n(A ∩ B) + n(B - A) 

n(B) = n(A ∩ B) + n(B - A)

(A ∩ B) means (A and B)

> Union of A and B (denoted by A U B)-Everything in A or everything in B or both in A and B

(A U B) means (A or B)",intersect union set,"['intersect', 'union', 'set']",Intersection and Union of Sets,complement denot a’ ac everyth intersect b denot ∩ b everyth bna ∩ b na nbna u bthe number element union set b na∪b na b na ∩ b nb nb na ∩ b nb aa ∩ b mean b union b denot u beveryth everyth b ba u b mean b
398,"Mutually exclusive sets

If no element is common between two sets, we say that they are mutually exclusive.

A ∩ B = φ,",mutual exclus set,"['mutual', 'exclus', 'set']",Mutually exclusive sets,element common two set say mutual exclusivea ∩ b φ
399,"Permutation & Combination (Counting)

nPr=n!/(n-r)!

nCr=n!/{r!(n-r)!}

For counting, we can import libraries in python

from itertools import permutations

from itertools import combinations

combinations(my_list,4)",permut combin count,"['permut', 'combin', 'count']",Permutation & Combination (Counting),nprnnrncrnrnrfor count import librari pythonfrom itertool import permutationsfrom itertool import combinationscombinationsmylist4
400,"Basic Probability

> Probability is a numerical way of describing how likely (or not) an event is to happen. If each of the elements in the sample space are equally likely, then we can define the probability of event A as P(A) = n(A) / n(S)

n(S) is the no. of elements in the set or no. of possible outcomes

> Not event A, P(~A)=1-P(A)",basic probabl,"['basic', 'probabl']",Basic Probability,probabl numer way describ like event happen element sampl space equal like defin probabl event pa na nsns element set possibl outcom event pa1pa
401,"Probability Axiom#1

The relative frequency of event that is certain to occur must be 1.

The probability of getting any one of the numbers 1 to 6 on a dice is certain! 

P(S)=1, where S={1,2,3,4,5,6}",probabl axiom1,"['probabl', 'axiom1']",Probability Axiom#1,relat frequenc event certain occur must 1the probabl get one number 1 6 dice certain ps1 s123456
402,"Probability Axiom#2

The relative frequency of occurrence of any event must not be negative, that is, probabilities can never be negative. 

So the probability of an impossible event is 0.

Rule 1 and 2 together are telling us that probabilities lie between 0 (impossible) and 1 (certain). 
",probabl axiom2,"['probabl', 'axiom2']",Probability Axiom#2,relat frequenc occurr event must negat probabl never negat probabl imposs event 0rule 1 2 togeth tell us probabl lie 0 imposs 1 certain
403,"Probability Axiom#3 or Special Addition Rule

P(A U B) = P(A) + P(B)    if A ∩ B = φ

This property is known as additivity.",probabl axiom3 special addit rule,"['probabl', 'axiom3', 'special', 'addit', 'rule']",Probability Axiom#3 or Special Addition Rule,pa u b pa pb ∩ b φthis properti known addit
404,"Addition Rule

In general, where two sets are not necessarily mutually exclusive, Axiom#3 can be extended as follows: 

P(A U B) = P(A) + P(B) - P(A ∩ B)

> Event A and B can occur at the same time (not mutually exclussive)",addit rule,"['addit', 'rule']",Addition Rule,general two set necessarili mutual exclus axiom3 extend follow pa u b pa pb pa ∩ b event b occur time mutual excluss
405,"Conditional Probability

A conditional probability is the probability of an event, given some other event has already occurred. It is denoted by P(A|B) inferred as probability of A given B.

P(A | B) = P(A ∩ B) / P(B)
 
> If P(A | B) or P(A/B) =P(A), then A and B are independent events. 

Then probability of A not given B = P(A /~B) = P(A)",condit probabl,"['condit', 'probabl']",Conditional Probability,condit probabl probabl event given event alreadi occur denot pab infer probabl given bpa b pa ∩ b pb pa b pab pa b independ event probabl given b pa b pa
406,"Multiplication Rule

The probability that event A and B both occur is equal to the probability that Event B occurs times the probability that Event A occurs, given that B has occurred. 

P(A ∩ B) = P(B) * P(A | B)

If A and B are independent, 

P(A ∩ B) = P(B) * P(A)",multipl rule,"['multipl', 'rule']",Multiplication Rule,probabl event b occur equal probabl event b occur time probabl event occur given b occur pa ∩ b pb pa bif b independ pa ∩ b pb pa
407,"Solving any set or probability problem

> Identify the event, subsets or groups: A, B

> Identify n(A) or P(A)

> Identify n(B) or P(B)

>  Identify n(A U B) or P(A U B) or P(A or B)

> Identify n(A ∩ B) or P(A ∩ B) or P(A and B)

> Understand the concept in totality with respect to experiment, sample space, observation, experience and dataset ",solv set probabl problem,"['solv', 'set', 'probabl', 'problem']",Solving any set or probability problem,identifi event subset group b identifi na pa identifi nb pb identifi na u b pa u b pa b identifi na ∩ b pa ∩ b pa b understand concept total respect experi sampl space observ experi dataset
408,"Difference between mutually exclusive and independent events

> A mutually exclusive event can simply be defined as a situation when two events cannot occur at same time whereas independent event occurs when one event remains unaffected by the occurrence of the other event.

> When two events are mutually exclusive, then there is a condition between two events, thus they are not independent.

For example, when a coin is tossed then the result will be either head or tail, but we cannot get both the results. Therefore getting head and getting tail are two mutually exclusive events.

",differ mutual exclus independ event,"['differ', 'mutual', 'exclus', 'independ', 'event']",Difference between mutually exclusive and independent events,mutual exclus event simpli defin situat two event cannot occur time wherea independ event occur one event remain unaffect occurr event two event mutual exclus condit two event thus independentfor exampl coin toss result either head tail cannot get result therefor get head get tail two mutual exclus event
409,"Basics of Summarizing Data

It is used for understanding large to very large dataset through different statistics

There are mainly three measures or statistics:

1. Measure of central tendency

2. Measure of spread

3. Measures of Symmetry",basic summar data,"['basic', 'summar', 'data']",Basics of Summarizing Data,use understand larg larg dataset differ statisticsther main three measur statistics1 measur central tendency2 measur spread3 measur symmetri
410,"Numerical, Categorical, Dichotomous, and Ordinal Data

> A data can be of two types-Numerical and Categorical

> Numerical data are of two types-Discrete and Continuous

> Categorical data can be of three types-

1. Dichotomous(attribute like Yes or No), 

2. Nominal (Names like bus, train, car or red, yellow, blue) and 

3. Ordinal(order like hot, hotter, hotest)",numer categor dichotom ordin data,"['numer', 'categor', 'dichotom', 'ordin', 'data']","Numerical, Categorical, Dichotomous, and Ordinal Data",data two typesnumer categor numer data two typesdiscret continu categor data three types1 dichotomousattribut like yes 2 nomin name like bus train car red yellow blue 3 ordinalord like hot hotter hotest
411,"1. Measure of central tendency 

There are three measure of central tendency

Mean, Median, and Mode",1 measur central tendenc,"['1', 'measur', 'central', 'tendenc']",1. Measure of central tendency ,three measur central tendencymean median mode
412,"Understanding Mean

Mean is denoted by x bar (x̄) or mu(μ) and it is the most common method to measure central tendency

> It is influenced by outliers

> Not applicable to categorical data",understand mean,"['understand', 'mean']",Understanding Mean,mean denot x bar x̄ muμ common method measur central tendenc influenc outlier applic categor data
413,"Understanding Median

After sorting n observations in ascending or decending order,

If n is odd,then the median is the middle observation. 

If n is even, then the median is the mean of the middle two observations.

> It is robust or resistant to the effects of extreme observations (outliers)

> Not applicable to categorical data",understand median,"['understand', 'median']",Understanding Median,sort n observ ascend decend orderif n oddthen median middl observ n even median mean middl two observ robust resist effect extrem observ outlier applic categor data
414,"Understanding Mode

Mode is defined as the value which occurs with the greatest frequency or the most typical value.

> Its use in practice is limited

> Not influenced by outliers

> Applicable to all types of data

> More than one mode possible",understand mode,"['understand', 'mode']",Understanding Mode,mode defin valu occur greatest frequenc typic valu use practic limit influenc outlier applic type data one mode possibl
415,"2. Measure of spread

The measure of spread are:

Range, 

Variance, 

Standard Deviation, 

Interquartile Range",2 measur spread,"['2', 'measur', 'spread']",2. Measure of spread,measur spread arerang varianc standard deviat interquartil rang
416,"Understanding Range

Range is a very simple measure of spread defined as the difference between the largest and smallest observation in the data set.

Range is a poor measure of the spread of data as it relies on the extreme values, which aren’t necessarily representative of the data as a whole.",understand rang,"['understand', 'rang']",Understanding Range,rang simpl measur spread defin differ largest smallest observ data setrang poor measur spread data reli extrem valu aren't necessarili repres data whole
417,"Understanding Variance

Variance is denoted by σ^2 or S^2

Variance is the expectation of the squared deviation of a random variable from its mean. 

In other words, it measures how far a set of numbers is spread out from their average value or mean.
",understand varianc,"['understand', 'varianc']",Understanding Variance,varianc denot σ2 s2varianc expect squar deviat random variabl mean word measur far set number spread averag valu mean
418,"Understanding Standard Deviation

The standard deviation is the positive square root of the variance.

A quantity expressing by how much the members of a group differ from the mean value

> A low standard deviation indicates that the values tend to be close to the mean of the set, while a high standard deviation indicates that the values are spread out over a wider range",understand standard deviat,"['understand', 'standard', 'deviat']",Understanding Standard Deviation,standard deviat posit squar root variancea quantiti express much member group differ mean valu low standard deviat indic valu tend close mean set high standard deviat indic valu spread wider rang
419,"Understanding Interquartile Range

IQR is another measure of spread which is like the range but not affected by the data extremes

> Q1, Q2 and Q3 are the quartiles that divide a set of data into four quarters. 

> Note that Q2 is just the median, while Q1 is called the lower quartile and Q3 the upper quartile

> The interquartile range is defined as (Q3  - Q1)",understand interquartil rang,"['understand', 'interquartil', 'rang']",Understanding Interquartile Range,interquartil rang anoth measur spread like rang affect data extrem q1 q2 q3 quartil divid set data four quarter note q2 median q1 call lower quartil q3 upper quartil interquartil rang defin q3 q1
420,"Steps to Calculate IQR

> Arrange the data in asc or desc order

> Q2= median of the data

> Q1=The median of data values below the main median

> Q3=The median of data values above the main median

> IQR = Q3-Q1

Here,

Q1=percentile 25

Q2=percentile 50

Q3=percentile 75

Q4=percentile 100

Percentile = (Number of values below ""X""/Total Number of all the Values) * 100",step calcul interquartil rang,"['step', 'calcul', 'interquartil', 'rang']",Steps to Calculate IQR,arrang data asc desc order q2 median data q1the median data valu main median q3the median data valu main median interquartil rang q3q1hereq1percentil 25q2percentil 50q3percentil 75q4percentil 100percentil number valu xtotal number valu 100
421,"Outliers with respect to IQR

Data below (Q1-1.5*IQR) and data above (Q3+1.5*IQR) are generally called outliers",outlier respect interquartil rang,"['outlier', 'respect', 'interquartil', 'rang']",Outliers with respect to IQR,data q115iqr data q315iqr general call outlier
422,"3. Measures of Symmetry

Coefficient of Skewness

> Positively skewed or right-skewed (Mean > Median > Mode) 

> Negatively skewed or left-skewed (Mean < Median < Mode)

> Normal or Symmetrical (Mean = Median = Mode)

Peak value of the distributioin is always the mode (highest frequency) for all of the above three categories",3 measur symmetri,"['3', 'measur', 'symmetri']",3. Measures of Symmetry,coeffici skew posit skew rightskew mean median mode negat skew leftskew mean median mode normal symmetr mean median modepeak valu distributioin alway mode highest frequenc three categori
423,"Understanding skewness

When we plot graph between two conclusive parameters (e.g. house price range and number of houses), we can draw conclusion from the skewness.

> If the plot is positively skewed, there is highest no. of houses with lower price range-Its a village

> If the plot is negatively skewed, there is highest no. of houses with higher price range-Its a metro

> If the distribution is normal, there is highest no. of houses with medium price range-Its a town",understand skew,"['understand', 'skew']",Understanding skewness,plot graph two conclus paramet eg hous price rang number hous draw conclus skew plot posit skew highest hous lower price rangeit villag plot negat skew highest hous higher price rangeit metro distribut normal highest hous medium price rangeit town
424,"Libraries for Summarizing Data

from statistics import mode

mode(my_list)

from statistics import median

from statistics import mean

we can also calculate median and mean  using numpy library
 
Another way
from scipy import stats

stats.mode(a)",librari summar data,"['librari', 'summar', 'data']",Libraries for Summarizing Data,statist import modemodemylistfrom statist import medianfrom statist import meanw also calcul median mean use numpi librari anoth way scipi import statsstatsmodea
425,"Skewness and Kurtosis measurements

my_df['column_name'].skew()

my_df['column_name'].kurt()

",skew kurtosi measur,"['skew', 'kurtosi', 'measur']",Skewness and Kurtosis measurements,mydfcolumnnameskewmydfcolumnnamekurt
426,"Understanding random variable

A random variable (numerical feature or column name) is a numerical description of the outcome of a statistical experiment

Variable can store single data or multiple data as data structure. Thus random variable can store single random data or multiple random data as data structure.

Let X = number of heads when we toss a coin (X can take two values, 0 and 1)

Let Y = number that comes up when we roll a die (Y can take values 1, 2, 3, 4, 5 or 6)

It is conventional to denote the random variable by a capital letter and the possible values it can take by a small letter.

Let X = number of heads when we toss a coin, 
then x belongs to {0, 1}

Let Z = weight of a randomly selected student in a class, 
then z belongs to (0, ∞)

> We can use set data structure to store all possible outcomes of an experiment.

> A random variable is a rule for associating a number with each element in the set. Thus random variable is a function of the outcomes.

> Experiment: Tossing a coin

Sample space: S = {H, T}. 
X is the number of heads when we toss a coin.
Then, X(H) = 1 and X(T) = 0.

Random variable is introduced for graphical representation of the outcomes. 

Because without the numerical values of the outcome, we can not plot them on an axis.",understand random variabl,"['understand', 'random', 'variabl']",Understanding random variable,random variabl numer featur column name numer descript outcom statist experimentvari store singl data multipl data data structur thus random variabl store singl random data multipl random data data structurelet x number head toss coin x take two valu 0 1let number come roll die take valu 1 2 3 4 5 6it convent denot random variabl capit letter possibl valu take small letterlet x number head toss coin x belong 0 1let z weight random select student class z belong 0 ∞ use set data structur store possibl outcom experi random variabl rule associ number element set thus random variabl function outcom experi toss coinsampl space h x number head toss coin xh 1 xt 0random variabl introduc graphic represent outcom without numer valu outcom plot axi
427,"Python Code for Random Variables

import random
X=random.randint(1,6)
print(""You rolled"",X) 

returns any random number between 1 to 6

X=random.random()
print(""The random float is"",X)

> random library creates random data. Data structure can not be created using random library. Numpy is used in that case",python code random variabl,"['python', 'code', 'random', 'variabl']",Python Code for Random Variables,import random xrandomrandint16 printyou rolledx return random number 1 6xrandomrandom printth random float isx random librari creat random data data structur creat use random librari numpi use case
428,"Random seed

Repeat the same random numbers using seed (useful for presentation)

random.seed(11)
X=random.random()
print(""The random float is"",X)

NumPy random seed
np.random.seed(0) ; np.random.rand(4)

It is a pseudo-random number generator

np.random.rand() # generates a single random float between 0 and 1",random seed,"['random', 'seed']",Random seed,repeat random number use seed use presentationrandomseed11 xrandomrandom printth random float isxnumpi random seed nprandomseed0 nprandomrand4it pseudorandom number generatornprandomrand generat singl random float 0 1
429,"Types of Random Variables

1. Discrete Random Variables (countable)

e.g.
I. Flipping a coin
II. Attendance at a game
III. Drawing a card etc.

2. Continuous Random Variables (uncountable)

e.g.
I. Height of students 
II. Driving time 
III. Product price etc.
",type random variabl,"['type', 'random', 'variabl']",Types of Random Variables,1 discret random variabl countableeg flip coin ii attend game iii draw card etc2 continu random variabl uncountableeg height student ii drive time iii product price etc
430,"Continuous and Discrete Random variables using numpy array

X=np.random.uniform(1,3)
returns a float between 1 and 3

X=np.random.uniform(1,3,5)
returns a numpy array with 5 floats between 1 and 3
 
X=np.random.randint(1,3)
returns an int between 1 and 3

X=np.random.randint(1,3,5)
returns a numpy 1D array with 5 int between 1 and 3",continu discret random variabl use numpi array,"['continu', 'discret', 'random', 'variabl', 'use', 'numpi', 'array']",Continuous and Discrete Random variables using numpy array,xnprandomuniform13 return float 1 3xnprandomuniform135 return numpi array 5 float 1 3 xnprandomrandint13 return int 1 3xnprandomrandint135 return numpi 1d array 5 int 1 3
431,"Discrete Random Variables in Probability Distribution

1. Probability Distribution or Mass Function (pf or pdf or pmf)

The function fX(x) = P(X=x) for each x in the range of X is the probability function (pf) of X.

2. Cumulative Distribution Function(cdf or df)-summation

The cumulative distribution function (cdf) of X is FX(x) = P(X ≤ x). It gives the probability that X assumes a value that does not exceed x. CDFs are also known as “Distribution functions (df)""",discret random variabl probabl distribut,"['discret', 'random', 'variabl', 'probabl', 'distribut']",Discrete Random Variables in Probability Distribution,1 probabl distribut mass function pf pdf pmfthe function fxx pxx x rang x probabl function pf x2 cumul distribut functioncdf dfsummationth cumul distribut function cdf x fxx px ≤ x give probabl x assum valu exceed x cdfs also known “distribut function df
432,"Continuous Random Variables in Probability Distribution 

1. Probability Density Function

In case of continuous variables we always take intervals into account. 

The probability associated with an interval of values, (a, b), is represented as P(a < X < b) – and is the area under the curve of the probability density function (pdf) from a to b.

2. Cumulative Distribution Function-Integration

The cumulative distribution function (cdf) is defined to be the function: FX(x) = P(X ≤ x) For a continuous random variable, FX(x) is a continuous, non-decreasing function, defined for all real values of x.",continu random variabl probabl distribut,"['continu', 'random', 'variabl', 'probabl', 'distribut']",Continuous Random Variables in Probability Distribution ,1 probabl densiti functionin case continu variabl alway take interv account probabl associ interv valu b repres pa x b – area curv probabl densiti function pdf b2 cumul distribut functionintegrationth cumul distribut function cdf defin function fxx px ≤ x continu random variabl fxx continu nondecreas function defin real valu x
433,"Mean of Random Variables

E[X] is a measure of the average/centre/location/level of the distribution of X. It is called the expected value of X, or mean of X, and is usually denoted as μ.

>> Mean of Discrete Random Variables

E(X) = ∑xP(X=x)

>> Mean of Continuous Random Variables by integration

The expected value of a random variable is the mean value over an infinite number of observations of the variable.

> In statistics infinite number means large number",mean random variabl,"['mean', 'random', 'variabl']",Mean of Random Variables,ex measur averagecentrelocationlevel distribut x call expect valu x mean x usual denot μ mean discret random variablesex ∑xpxx mean continu random variabl integrationth expect valu random variabl mean valu infinit number observ variabl statist infinit number mean larg number
434,"Variance of Random Variables

The variance σ² is a measure of the spread/dispersion/variability of the distribution. Specifically, it is a measure of the spread of the distribution about its mean.

Variance of Discrete Random Variables

Var(X) 
= ∑(x-μ)^2*P(X=x)
= E(X^2)- E(X)^2

For variance of Continuous Random Variables integration is used in place of summation",varianc random variabl,"['varianc', 'random', 'variabl']",Variance of Random Variables,varianc σ² measur spreaddispersionvari distribut specif measur spread distribut meanvari discret random variablesvarx ∑xμ2pxx ex2 ex2for varianc continu random variabl integr use place summat
435,"Point probability, cumulative probability

When we have a probability distribution function of a random variable and the definition of random variable is matching with our requirement, then we can answer point probability or cumulative probability for any outcome or mean and variance for the set of outcomes",point probabl cumul probabl,"['point', 'probabl', 'cumul', 'probabl']","Point probability, cumulative probability",probabl distribut function random variabl definit random variabl match requir answer point probabl cumul probabl outcom mean varianc set outcom
436,"Formulas for expectation and variance

E(aX+b)= a*E(X)+b

e.g.
E(5X - 2Y) = 5*E(X) - 2*E(Y)

Var(aX+b)= a^2*Var(X)

e.g. 
Var(5X - 2Y) =  5^2*Var(X) + (-2)^2*Var(Y)

",formula expect varianc,"['formula', 'expect', 'varianc']",Formulas for expectation and variance,eaxb aexbeg e5x 2y 5ex 2eyvaraxb a2varxeg var5x 2y 52varx 22vari
437,"Basics of Probability distribution

Probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.

Simply, probability is the function of outcomes

Thus probability distribution is the graph between all possible numerical outcomes of an experiment vs. probability of occurance of the outcome.

The set of all possible numerical outcome is the random variable.

Thus probability distribution is the graph between random variable vs. probability",basic probabl distribut,"['basic', 'probabl', 'distribut']",Basics of Probability distribution,probabl distribut mathemat function give probabl occurr differ possibl outcom experimentsimpli probabl function outcomesthus probabl distribut graph possibl numer outcom experi vs probabl occur outcometh set possibl numer outcom random variablethus probabl distribut graph random variabl vs probabl
438,"Types of Discrete Probability Distribution

> Depending on definition of random variable,X, we can identify the pdf

1. Uniform Distribution

2. Bernoulli Distribution

3. Binomial Distribution

4. Geometric Distribution

5. Poisson Distribution",type discret probabl distribut,"['type', 'discret', 'probabl', 'distribut']",Types of Discrete Probability Distribution,depend definit random variablex identifi pdf1 uniform distribution2 bernoulli distribution3 binomi distribution4 geometr distribution5 poisson distribut
439,"1. Uniform Distribution-discrete

-all outcomes are equally likely

X is the no. of outcome from 1 to k. Thus,

x= 1,2,3,...k

P(X=x)=1/k
 
E(X)=(k+1)/2

Var(X)=(k^2-1)/12

",1 uniform distributiondiscret,"['1', 'uniform', 'distributiondiscret']",1. Uniform Distribution-discrete,outcom equal likelyx outcom 1 k thusx 123kpxx1k exk12varxk2112
440,"2. Bernoulli Distribution

Bernauli trial is an experiment which has only two possible outcomes ""success"" and ""failure""

Means the Bernoulli distribution represents the success or failure of a single Bernoulli trial.

For example we can consider tossing a coin for one time.

X is the success or failure in a trial. Thus,

x= 0,1

P(X=1)= θ

P(X=0)= 1- θ

P(X=x)= θ^x*(1- θ)^(1-x)

E(X) = θ

Var(X) = θ - θ^2

> Bernoulli trials are also called as yes or no questions.",2 bernoulli distribut,"['2', 'bernoulli', 'distribut']",2. Bernoulli Distribution,bernauli trial experi two possibl outcom success failuremean bernoulli distribut repres success failur singl bernoulli trialfor exampl consid toss coin one timex success failur trial thusx 01px1 θpx0 1 θpxx θx1 θ1xex θvarx θ θ2 bernoulli trial also call yes question
441,"3. Binomial Distribution

Binomial Distribution is the widely used probability distribution, derived from Bernoulli Process (Only two possible outcomes, i.e. success or failure).

Binomial distribution is one in which the probability of repeated number of trials (sequence of n bernoulli trials) are studied.

For example, we can consider tossing a coin multiple times.

If ‘p’, ‘q’ and ‘n’ are probability of success, failure and number of trials respectively,

Mean = np

Standard Deviation= 
√npq

>  For larger values of ‘n’, Binomial Distribution tends to Poisson Distribution

X is the number of success (or failure) in n trials. Thus 
x= 0,1,2...n

Also denoted as Bin(n,θ) or Bin(n,p)

P(X=x)= C(n,x)*θ^x*(1- θ)^(n-x)

E(X) = nθ

Var(X) = nθ(1-θ)",3 binomi distribut,"['3', 'binomi', 'distribut']",3. Binomial Distribution,binomi distribut wide use probabl distribut deriv bernoulli process two possibl outcom ie success failurebinomi distribut one probabl repeat number trial sequenc n bernoulli trial studiedfor exampl consid toss coin multipl timesif p q n probabl success failur number trial respectivelymean npstandard deviat √npq larger valu n binomi distribut tend poisson distributionx number success failur n trial thus x 012nalso denot binnθ binnppxx cnxθx1 θnxex nθvarx nθ1θ
442,"4. Geometric Distribution

-gives the probability of first success with number of trials.

X is the no. of trial on which the first success occurs. Thus,

x= 1,2,3,…

P(X=x)= θ*(1- θ)^(x-1)

E(X) = 1/θ

Var(X) = (1- θ)/θ^2

> The probability of success (successive trials are without replacement) changes from trial to trial in Hypergeometric Distribution 

> θ is also denoted by p",4 geometr distribut,"['4', 'geometr', 'distribut']",4. Geometric Distribution,give probabl first success number trialsx trial first success occur thusx 123…pxx θ1 θx1ex 1θvarx 1 θθ2 probabl success success trial without replac chang trial trial hypergeometr distribut θ also denot p
443,"5. Poisson Distribution-

-models/trials the no. of events that occurs within a specified interval of time.

Thus, rate of occurance, λ=E(X)=Var(X)

Unlimited number of possible outcomes (can  be used for two outcomes-success or failure).

X is the number of specified outcome (success, failure or anything else) in a specified interval of time. Thus,

x= 0,1,2,....

P(X=x)= λ^x*e^(-λ)/x!

from scipy.stats import poisson",5 poisson distribut,"['5', 'poisson', 'distribut']",5. Poisson Distribution-,modelstri event occur within specifi interv timethus rate occur λexvarxunlimit number possibl outcom use two outcomessuccess failurex number specifi outcom success failur anyth els specifi interv time thusx 012pxx λxeλxfrom scipystat import poisson
444,"Moments of probability distribution

1) The first moment is the mean, which indicates the central tendency of a distribution. 

2) The second moment is the variance, which indicates the width or spread 

3) The third moment is the skewness, which indicates any asymmetric 'leaning' to either left or right.

4) The fourth standardized moment is the kurtosis, which indicates the heaviness of the tail of the distribution.",moment probabl distribut,"['moment', 'probabl', 'distribut']",Moments of probability distribution,1 first moment mean indic central tendenc distribut 2 second moment varianc indic width spread 3 third moment skew indic asymmetr lean either left right4 fourth standard moment kurtosi indic heavi tail distribut
445,"Moments in Physics

> The total mass is the zeroth moment of mass.

> The first moment is the center of the mass, and

> The second moment is the rotational inertia.

From these moments we can understand how the physical quantity is arranged. Similarly, from the moments of pdf, we can understand its arrangement. ",moment physic,"['moment', 'physic']",Moments in Physics,total mass zeroth moment mass first moment center mass second moment rotat inertiafrom moment understand physic quantiti arrang similar moment pdf understand arrang
446,"Basics of continuous distribution

Since continuous probability functions are defined for an infinite number of points over a continuous interval, the probability at a single point is always zero (e.g. P(weight=50.555)=0).

Hence, we will use P(X<=x) and P(X>=x)

P(X<=x) = P(X<x) and
P(X>=x) = P(X>x)

Since the sum of probabilities over the entire range of x is 1, hence,P(X>x) = 1 - P(X<x)",basic continu distribut,"['basic', 'continu', 'distribut']",Basics of continuous distribution,sinc continu probabl function defin infinit number point continu interv probabl singl point alway zero eg pweight505550henc use pxx pxxpxx pxx pxx pxxsinc sum probabl entir rang x 1 hencepxx 1 pxx
447,"Types of continuous probability distribution

1. Uniform Distribution

2. Normal Distribution 

3. Standard Normal Distribution

4. Exponential Distribution

5. Gamma Distribution

6. Chi-square Distribution

7. t-Distribution

8. F-Distribution
",type continu probabl distribut,"['type', 'continu', 'probabl', 'distribut']",Types of continuous probability distribution,1 uniform distribution2 normal distribut 3 standard normal distribution4 exponenti distribution5 gamma distribution6 chisquar distribution7 tdistribution8 fdistribut
448,"1. Uniform Distribution-continuous

X is the outcome between α (also denoted as a) and β (also denoted as b)

P(X<x)= 1/(β- α)

E(X) = (β+ α)/2

Var(X) = (β- α)^2/12",1 uniform distributioncontinu,"['1', 'uniform', 'distributioncontinu']",1. Uniform Distribution-continuous,x outcom α also denot β also denot bpxx 1β αex β α2varx β α212
449,"2. Normal or Gaussian Distribution

It is a probability distribution that is symmetric (bell-shaped) about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean.

X is the outcome between -∞ and 
+ ∞ and expectation is μ and standard deviation is σ

Normal distribution is the function of x, μ and σ 

X ~ N(μ, σ^2)

> It is asymptotic, means each end approaches the horizontal axis but never reaches it",2 normal gaussian distribut,"['2', 'normal', 'gaussian', 'distribut']",2. Normal or Gaussian Distribution,probabl distribut symmetr bellshap mean show data near mean frequent occurr data far meanx outcom ∞ ∞ expect μ standard deviat σnormal distribut function x μ σ x nμ σ2 asymptot mean end approach horizont axi never reach
450,"3. Standard Normal Distribution- 

X is a normally distributed random variable with mean, μ=0 and standard deviation, σ=1. It will always be denoted by z_score",3 standard normal distribut,"['3', 'standard', 'normal', 'distribut']",3. Standard Normal Distribution- ,x normal distribut random variabl mean μ0 standard deviat σ1 alway denot zscore
451,"4. Exponential Distribution

X is the outcome between 0 and ∞ and has a rate of occurance λ

P(X<x)= λ*e^-λx

E(X) = 1/λ

Var(X) = 1/λ^2

X ~ Exp(λ) means random variable X has an exponential distribution with rate parameter λ

> Exponential distribution is a special case of gamma distribution

> The Exponential distribution also describes the time between events in a Poisson process

> It is used to model the lifetime of any equipment.

> It also gives the distribution of the waiting time, T

P(T>t)= e^-λt

P(T<t)= 1- e^-λt",4 exponenti distribut,"['4', 'exponenti', 'distribut']",4. Exponential Distribution,x outcom 0 ∞ rate occur λpxx λeλxex 1λvarx 1λ2x expλ mean random variabl x exponenti distribut rate paramet λ exponenti distribut special case gamma distribut exponenti distribut also describ time event poisson process use model lifetim equip also give distribut wait time tptt eλtptt 1 eλt
452,"Gamma Function

Gamma Function, Γ(α) = ∫t^(α −1)* e^(−t) dt #  interval [0, ∞ ]

The gamma function is one commonly used extension of the factorial function to complex numbers.

> A complex number is a number that can be expressed in the form a + bi, where a and b are real numbers, and i is a symbol called the imaginary unit, and satisfying the equation i² = −1",gamma function,"['gamma', 'function']",Gamma Function,gamma function γα ∫tα −1 e−t dt interv 0 ∞ gamma function one common use extens factori function complex number complex number number express form bi b real number symbol call imaginari unit satisfi equat i² −1
453,"5. Gamma Distribution

Widely used distribution and its importance is largely due to its relation to exponential and normal distributions.

X is the outcome between 0 and ∞, rate of occurance, λ (also denoted as  θ)  and has a shape parameter α (also denoted as k)

P(X<x) = λ^α*x^(α-1)*e^-λx/Γ(α)

E(X) = α/λ",5 gamma distribut,"['5', 'gamma', 'distribut']",5. Gamma Distribution,wide use distribut import larg due relat exponenti normal distributionsx outcom 0 ∞ rate occur λ also denot θ shape paramet α also denot kpxx λαxα1eλxγαex αλ
454,"6. Chi-square Distribution

The chi-squared distribution is a special case of the gamma distribution

X is the chi_square_score between 0 and ∞,  degrees of freedom= k 

P(X<x) = x^(k/2-1)*e^(-x/2)/(2^k/2*Γ(k/2))

chi_square_score= (n-1)*S^2/σ^2",6 chisquar distribut,"['6', 'chisquar', 'distribut']",6. Chi-square Distribution,chisquar distribut special case gamma distributionx chisquarescor 0 ∞ degre freedom k pxx xk21ex22k2γk2chisquarescor n1s2σ2
455,"7. t-Distribution 

It is similar to the normal distribution, just with fatter tails. Have higher kurtosis than normal distributions

X is the t_score between -∞ and + ∞, degrees of freedom= k or γ

",7 tdistribut,"['7', 'tdistribut']",7. t-Distribution ,similar normal distribut fatter tail higher kurtosi normal distributionsx tscore ∞ ∞ degre freedom k γ
456,"8. F-Distribution

X is f_score between 0 and ∞, when two independent random samples of size n1 and n2 are taken respectively ",8 fdistribut,"['8', 'fdistribut']",8. F-Distribution,x fscore 0 ∞ two independ random sampl size n1 n2 taken respect
457,"log-normal distribution 

In probability theory, a log-normal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. Thus, if the random variable X is log-normally distributed, then Y = ln(X) has a normal distribution.",lognorm distribut,"['lognorm', 'distribut']",log-normal distribution ,probabl theori lognorm distribut continu probabl distribut random variabl whose logarithm normal distribut thus random variabl x lognorm distribut lnx normal distribut
458,"Exponential expressions

Exponential expressions are just a way to write powers in short form. The exponent indicates the number of times the base is used as a factor. Exponentiation is the Arithmatic Operation for power.

f(x) = e^x is called the natural exponential function  or in short, exponential function",exponenti express,"['exponenti', 'express']",Exponential expressions,exponenti express way write power short form expon indic number time base use factor exponenti arithmat oper powerfx ex call natur exponenti function short exponenti function
459,"Euler's number

The number e, also known as Euler's number (like pi, is a transcendental number), is a mathematical constant approximately equal to 2.71828, and can be characterized in many ways. It is the base of the natural logarithm. 

> The reason Euler's number is such an important constant is that it has unique properties that simplify many equations and patterns.

> Apart from natural logarithm, other two logarithms are 

1. common logarithm-logarithm of base 10 

2. binary logarithm-logarithm of base 2

Logarithmic operation returns the power of the base. Thus, logarithms are a convenient way to express large numbers.",euler number,"['euler', 'number']",Euler's number,number e also known euler number like pi transcendent number mathemat constant approxim equal 271828 character mani way base natur logarithm reason euler number import constant uniqu properti simplifi mani equat pattern apart natur logarithm two logarithm 1 common logarithmlogarithm base 10 2 binari logarithmlogarithm base 2logarithm oper return power base thus logarithm conveni way express larg number
460,"Python Code for Continuous Distributions

from scipy.stats import uniform

from scipy.stats import norm

from scipy.stats import chi2

from scipy.stats import t

from scipy.stats import f

from scipy.stats import * (this imports all the modules)

lower_limit = 50

upper_limit = 150

range = upper_limit - lower_limit

# P(X < 74) is
uniform.cdf(74, lower_limit, range)

standard_deviation=math.sqrt(variance)

P(X<28) is norm.cdf(28, mean, standard_deviation)

P(X<z_score) is norm.cdf(z_score)

chi2.cdf(chi_square_score, degrees_of_freedom)

t.cdf(t_score, degrees_of_freedom)

f.cdf(f_score, degrees_of_freedom1, degrees_of_freedom2)",python code continu distribut,"['python', 'code', 'continu', 'distribut']",Python Code for Continuous Distributions,scipystat import uniformfrom scipystat import normfrom scipystat import chi2from scipystat import tfrom scipystat import ffrom scipystat import import moduleslowerlimit 50upperlimit 150rang upperlimit lowerlimit px 74 uniformcdf74 lowerlimit rangestandarddeviationmathsqrtvariancepx28 normcdf28 mean standarddeviationpxzscor normcdfzscorechi2cdfchisquarescor degreesoffreedomtcdftscor degreesoffreedomfcdffscor degreesoffreedom1 degreesoffreedom2
461,"Continuous distributions in ML

Different probability distributions serve different purposes and represent different data generation processes.

Use of continuous distribution are in the distribution of numerical input and output variables for models and in the distribution of errors made by models.",continu distribut machin learn,"['continu', 'distribut', 'machin', 'learn']",Continuous distributions in ML,differ probabl distribut serv differ purpos repres differ data generat processesus continu distribut distribut numer input output variabl model distribut error made model
462,"Most frequent types of distribution for data scientist:

1. Bernoulli 

2. Uniform 

3. Binomial 

4. Poisson 

5. Normal 

6. Exponential ",frequent type distribut data scientist,"['frequent', 'type', 'distribut', 'data', 'scientist']",Most frequent types of distribution for data scientist:,1 bernoulli 2 uniform 3 binomi 4 poisson 5 normal 6 exponenti
463,"Basics of Joint Distribution

For a given experiment, we are often interested not only in probability distribution functions of individual random variables but also in the relationships between two or more random variables.

The joint probability distribution can be expressed either in terms of a joint cumulative distribution function or in terms of a joint probability density function (in the case of continuous variables) or joint probability mass function (in the case of discrete variables).

>> Let A and B be the two events, joint probability is the probability of event B occurring at the same time that event A occurs. This can be written as P(A, B) or P(A ∩ B). Thus, the joint probability is also called the intersection of two or more events.",basic joint distribut,"['basic', 'joint', 'distribut']",Basics of Joint Distribution,given experi often interest probabl distribut function individu random variabl also relationship two random variablesth joint probabl distribut express either term joint cumul distribut function term joint probabl densiti function case continu variabl joint probabl mass function case discret variabl let b two event joint probabl probabl event b occur time event occur written pa b pa ∩ b thus joint probabl also call intersect two event
464,"Two dimensional random vector

The combination of two random variable is called two dimensional random vector (discrete or continuous). Better to call it random matrix.",two dimension random vector,"['two', 'dimension', 'random', 'vector']",Two dimensional random vector,combin two random variabl call two dimension random vector discret continu better call random matrix
465,"Marginal Distribution and Conditional Distribution

> Marginal Distribution (discrete and continuous)-the probabilities for any one of the variables with no reference to any specific ranges of values for the other variables

> Conditional Probability Distribution- the probabilities for any subset of the variables conditional on particular values of the remaining variables. Conditional Distribution and Conditional expectation can be discrete or continuous.

Say the joint density function is f(x,y,z)

Then the marginal density functions are

fX(x)=∫∫f(x,y,z)dydz

fY(y)=∫∫f(x,y,z)dxdz

fZ(z)=∫∫f(x,y,z)dxdy

For independent random variables,

fX,Y,Z(x,y,z)= fX(x)*fY(y)*fZ(z)",margin distribut condit distribut,"['margin', 'distribut', 'condit', 'distribut']",Marginal Distribution and Conditional Distribution,margin distribut discret continuousth probabl one variabl refer specif rang valu variabl condit probabl distribut probabl subset variabl condit particular valu remain variabl condit distribut condit expect discret continuoussay joint densiti function fxyzthen margin densiti function arefxx∫∫fxyzdydzfyy∫∫fxyzdxdzfzz∫∫fxyzdxdyfor independ random variablesfxyzxyz fxxfyyfzz
466,"Independent Random Variables 

Two random variables X and Y are independent if knowing the value of one of them does not change the probabilities for the other one

Independent Random Variables (discrete and continuous) means outcomes are equally likely",independ random variabl,"['independ', 'random', 'variabl']",Independent Random Variables ,two random variabl x independ know valu one chang probabl oneindepend random variabl discret continu mean outcom equal like
467,"Probability mass function

> Probability mass function (for discrete random variable) or joint probability mass function is always expressed as matrix",probabl mass function,"['probabl', 'mass', 'function']",Probability mass function,probabl mass function discret random variabl joint probabl mass function alway express matrix
468,"Degree of association

Covariance and correlation are two important measures which describe the degree of association between two random variables X1 & X2

> Covariance gives the direction of relation

> Correlation give the strength of relation

> Covariance can vary between -∞ and +∞ 

> Correlation ranges between -1 and +1",degre associ,"['degre', 'associ']",Degree of association,covari correl two import measur describ degre associ two random variabl x1 x2 covari give direct relat correl give strength relat covari vari ∞ ∞ correl rang 1 1
469,"Understanding of Covariance 

Covariance  signifies  the  direction  of  the  linear  relationship between the two variables. By direction, we mean, if the variables are directly proportional or inversely proportional to each other. 

In general, it can be shown that a positive value of Cov(X1, X2) is an indication that X2 tends to increase as X1 does, whereas a negative value indicates that X2 tends to decrease as X1 increases. 

Cov(X1,X2)= E(X1*X2)-E(X1)*E(X2)

Cov(X1,X1)= Var(X1)

Cov((X1+X2),X3)= Cov(X1,X3)+ Cov(X2,X3) 

Cov(aX, Y) = Cov(X,aY)",understand covari,"['understand', 'covari']",Understanding of Covariance ,covari signifi direct linear relationship two variabl direct mean variabl direct proport invers proport general shown posit valu covx1 x2 indic x2 tend increas x1 wherea negat valu indic x2 tend decreas x1 increas covx1x2 ex1x2ex1ex2covx1x1 varx1covx1x2x3 covx1x3 covx2x3 covax covxay
470,"Understanding of Correlation

The strength of the relationship between X1 and X2 is indicated by the correlation between X1 and X2, a dimensionless quantity

Corr(X1,X2)= Cov(X1,X2)/sqrt(Var(X1)*Var(X2))
 
Thus correlation refers to the scaled form of covariance.

> As correlation is dimensionless, it is not influenced by the change in scale.",understand correl,"['understand', 'correl']",Understanding of Correlation,strength relationship x1 x2 indic correl x1 x2 dimensionless quantitycorrx1x2 covx1x2sqrtvarx1varx2 thus correl refer scale form covari correl dimensionless influenc chang scale
471,"Basics of Sampling & Statistical Inference

> Population of Data or simply population indicates the full set of data or experience of any topic which is impossible to gain  for our practical experiments or problems.

> A  set  of  items  selected  from  a  parent  population  is  a random sample if the probability that any item in the population is included in the sample is proportional to its frequency in the parent population and the inclusion/ exclusion of any item in the sample operates independently 

> From the statistics of the sample, we can draw inference or conclusion about population.",basic sampl statist infer,"['basic', 'sampl', 'statist', 'infer']",Basics of Sampling & Statistical Inference,popul data simpli popul indic full set data experi topic imposs gain practic experi problem set item select parent popul random sampl probabl item popul includ sampl proport frequenc parent popul inclus exclus item sampl oper independ statist sampl draw infer conclus popul
472,"Random sample

Random sample is a set of possible outcomes of sampling operation. 

Random sample is denoted by X=(X1,X2,X3,…Xn)

Thus random sample is nothing but the values of a single feature.",random sampl,"['random', 'sampl']",Random sample,random sampl set possibl outcom sampl oper random sampl denot xx1x2x3…xnthus random sampl noth valu singl featur
473,"Understanding of Statistics

STATISTIC is the score or measurement of each individual data or experience. 

STATISTICS is therefore, the process of designing, comparing, interpreting and analysing data. 

Statistic is related to the sample and parameter is related to the population. A statistic is used to estimate a parameter.

A statistics are also  random variables that is a function of the random sample, but not a function of unknown parameters.

Therefore, Random variable is a function of random sample.

Thus, statistics means creating multiple  random variables (i.e. mean, mode, median, variance etc.) from the random sample data or experiences.

These random variables or statistics are the function of each feature of a dataset.

my_df.describe() # gives the statistics

my_df.describe(include='all') # to include all the categorical columns

> Descriptive statistics is responsible for examining trends or distributions",understand statist,"['understand', 'statist']",Understanding of Statistics,statist score measur individu data experi statist therefor process design compar interpret analys data statist relat sampl paramet relat popul statist use estim parametera statist also random variabl function random sampl function unknown parameterstherefor random variabl function random samplethus statist mean creat multipl random variabl ie mean mode median varianc etc random sampl data experiencesthes random variabl statist function featur datasetmydfdescrib give statisticsmydfdescribeincludeal includ categor column descript statist respons examin trend distribut
474,"Statistical Inference 

> Looking at the sample, concluding about the population

To reach the final conclusion about the population, we take the help of different probability distribution.

(we can find individual probability, marginal probability or conditional probability of the random variables) 

Thus, Statistical Inference is the theory, methods, and practice of forming judgements about the parameters of a population.

The reliability of statistical inference typically depends on the random sampling.",statist infer,"['statist', 'infer']",Statistical Inference ,look sampl conclud populationto reach final conclus popul take help differ probabl distributionw find individu probabl margin probabl condit probabl random variabl thus statist infer theori method practic form judgement paramet populationth reliabl statist infer typic depend random sampl
475,"Sample Mean and Sample Variance

Sample Mean, 
X̄ = (1/n) * Σ Xi

Sample Variance, 
S^2 = (1/(n-1)) * Σ (Xi - X̄)^2


variance = lambda x : sum([(i - np.mean(x))**2 for i in x])/(len(x)-1)

variance(my_sample_list)- returns the sample variance

Sample mean is denoted by x̄ and population mean is denoted by μ.

Sample standard deviation is denoted by S and population standard deviation is denoted by σ",sampl mean sampl varianc,"['sampl', 'mean', 'sampl', 'varianc']",Sample Mean and Sample Variance,sampl mean x̄ 1n σ xisampl varianc s2 1n1 σ xi x̄2 varianc lambda x sumi npmeanx2 xlenx1variancemysamplelist return sampl variancesampl mean denot x̄ popul mean denot μsampl standard deviat denot popul standard deviat denot σ
476,"Independent and indentically distributed (IID) random variables

A collection of random variables is independent and identically distributed if each random variable has the same probability distribution (same μ and σ^2) as the others and all are mutually independent.

This holds true when we take multiple samples randomly from the same population",independ indent distribut iid random variabl,"['independ', 'indent', 'distribut', 'iid', 'random', 'variabl']",Independent and indentically distributed (IID) random variables,collect random variabl independ ident distribut random variabl probabl distribut μ σ2 other mutual independentthi hold true take multipl sampl random popul
477,"Sampling with Replacement

Sampling ''with replacement'' means that when a unit selected at random from the population, it is returned to the population (replaced), and then a second element is selected at random.",sampl replac,"['sampl', 'replac']",Sampling with Replacement,sampl replac mean unit select random popul return popul replac second element select random
478,"Central Limit Theorem (CLT) or z-results or z-score

Consider a case that we need to learn the distribution of the heights of all 20-year-old people in a country. It is almost impossible and, of course not practical, to collect this data. So, we take samples of 20-year-old people across the country and calculate the mean height of the people in samples.

> The Central Limit Theorem states that the  distribution of the sample means approaches a normal distribution as we take more and more samples from the population— no matter what the shape of the sample distribution. This fact holds especially true for no. of samples, n>= 30 (more accuracy n>= 50)

Thus, the average of our sample means will be the population mean.

As per CLT,
Expectation of the sample mean, E(x̄) = μ

Variance of the sample mean, Var(x̄) = σ^2/n

z-result of the sample mean, z_score = (x̄ – μ) / (σ/√n)

> Sample size is the total no. of observations/ rows in the set of samples

> One sample means a set with all observations for one feature of the samples space

> z-score (also called a standard score) gives us an idea of how far from the mean a data point (x) is. 
 z_score = (x – μ) /√σ^2

> We can find the cdf after getting the score (score is a generated statistic from the basic statistics like expectation and variance) of any probability distribution 

> CLT is is the bridge between probability and statistics

> CLT simply tells us that for large sample, population mean, μ is the expected value of sample mean, E(x̄) having  highest probability of occurance and rest of the probability distribution will be symmetrically decreasing from the mean",central limit theorem clt zresult zscore,"['central', 'limit', 'theorem', 'clt', 'zresult', 'zscore']",Central Limit Theorem (CLT) or z-results or z-score,consid case need learn distribut height 20yearold peopl countri almost imposs cours practic collect data take sampl 20yearold peopl across countri calcul mean height peopl sampl central limit theorem state distribut sampl mean approach normal distribut take sampl population— matter shape sampl distribut fact hold especi true sampl n 30 accuraci n 50thus averag sampl mean popul meana per central limit theorem expect sampl mean ex̄ μvarianc sampl mean varx̄ σ2nzresult sampl mean zscore x̄ – μ σ√n sampl size total observ row set sampl one sampl mean set observ one featur sampl space zscore also call standard score give us idea far mean data point x zscore x – μ √σ2 find cdf get score score generat statist basic statist like expect varianc probabl distribut central limit theorem bridg probabl statist central limit theorem simpli tell us larg sampl popul mean μ expect valu sampl mean ex̄ highest probabl occur rest probabl distribut symmetr decreas mean
479,"t-Result or t-Score

In most cases, we can get population mean but can not get population standard deviation. Thus, z-result cannot be used. We use the t-result in such cases.

> t-result is valid for samples from normal distribution only.

> The t-distribution is symmetrical about zero.

t_score = (x̄ – μ) / (S/√n)

Then we can calculate the probability for this t-score",tresult tscore,"['tresult', 'tscore']",t-Result or t-Score,case get popul mean get popul standard deviat thus zresult cannot use use tresult case tresult valid sampl normal distribut tdistribut symmetr zerotscor x̄ – μ s√nthen calcul probabl tscore
480,"F-result or F-Score

> F-result is valid for samples from normal distribution only.

If two independent random samples of size n1 and n2 are taken respectively,

f_score= (sample_1_variance/sample_2_variance)*(population_2_variance/population_1_variance)",fresult fscore,"['fresult', 'fscore']",F-result or F-Score,fresult valid sampl normal distribut onlyif two independ random sampl size n1 n2 taken respectivelyfscor sample1variancesample2variancepopulation2variancepopulation1vari
481,"Point Estimation

Point estimation is used to estimate a single value of population parameter (mean or variance)

> Method of moments- The basic principle is to equate population moments to  corresponding  sample moments and solve for the parameter(s)

> Maximum likelihood estimator (MLE)- The method of maximum likelihood is widely regarded as the best general method of finding estimators",point estim,"['point', 'estim']",Point Estimation,point estim use estim singl valu popul paramet mean varianc method moment basic principl equat popul moment correspond sampl moment solv paramet maximum likelihood estim mle method maximum likelihood wide regard best general method find estim
482,"Mean and Expectation

> Mean of a random variable is the simple average of all the values, expectation of a random variable is the  probability-weighted average. 

> For normal distribution, expectation= mean and the expected value of a random variable is the value that has the highest probability of occurrence

> Normal distribution is the plot for feature (height, age, price, temp, speed etc.) vs. its probability density and mean of the feature is the middle point of the bell. The  distribution is spreaded in both  side of μ with μ ± σ, μ ± 2σ, μ ± 3σ",mean expect,"['mean', 'expect']",Mean and Expectation,mean random variabl simpl averag valu expect random variabl probabilityweight averag normal distribut expect mean expect valu random variabl valu highest probabl occurr normal distribut plot featur height age price temp speed etc vs probabl densiti mean featur middl point bell distribut spread side μ μ ± σ μ ± 2σ μ ± 3σ
483,"Basics of Confidence interval

Confidence interval helps to determine the range of population parameters from sample parameters

Thus, a confidence interval provides an “interval estimate"" of an unknown population parameter (as opposed to a “point estimate"")

95% confidence interval means, the process we used will capture the true parameter 95% of the time in the long run

e.g.
The statement, ""the 95% confidence interval for the population mean is (350, 400)"" means that 95% of the population values are between 350 and 400.",basic confid interv,"['basic', 'confid', 'interv']",Basics of Confidence interval,confid interv help determin rang popul paramet sampl parametersthus confid interv provid “interv estimate unknown popul paramet oppos “point estimate95 confid interv mean process use captur true paramet 95 time long runeg statement 95 confid interv popul mean 350 400 mean 95 popul valu 350 400
484,"Calculating population parameters for one sample

> When σ is known, then assumming normal distribution of the sample mean, we are 95 % confident that 

μ= X̄ ± 1.96*σ*1/√n

or 
CI= X̄ ± 1.96*σ*1/√n

> When σ is unknown, then assumming t-distribution of the sample mean, we are 95 % confident that 

μ= X̄ ± t0.025, (n-1)*S*1/√n

95% means 100*(1-0.05), Here α=0.05, α/2=0.025

> When σ is unknown, then assumming Chi-square Distribution, we are 95% confident that 

σ^2= (n-1)*S^2/χ2  0.025,(n-1) and (n-1)*S^2/χ2  0.975,(n-1)

> We can calculate population proportion (p or θ) from sample proportion (p̂) with some confidence interval

It is valid only when the binomial distribution (categorical data) can be approximated by normal distribution.

p= p̂ ± error_in_estimate 
= p̂ ± Confidence_interval*sqrt(p̂*(1-p̂)/n)",calcul popul paramet one sampl,"['calcul', 'popul', 'paramet', 'one', 'sampl']",Calculating population parameters for one sample,σ known assum normal distribut sampl mean 95 confid μ x̄ ± 196σ1√nor ci x̄ ± 196σ1√n σ unknown assum tdistribut sampl mean 95 confid μ x̄ ± t0025 n1s1√n95 mean 1001005 α005 α20025 σ unknown assum chisquar distribut 95 confid σ2 n1s2χ2 0025n1 n1s2χ2 0975n1 calcul popul proport p θ sampl proport p̂ confid intervalit valid binomi distribut categor data approxim normal distributionp p̂ ± errorinestim p̂ ± confidenceintervalsqrtp̂1p̂n
485,"Population proportion for binomial experiment

A population proportion is a fraction of the population that has a certain characteristic.

For example, we had 1,000 people in the population and 237 of those people have blue eyes. The fraction of people who have blue eyes is 237 out of 1,000, or 237/1000 or 23.7%

> Proportion hypothesis testing may only be performed with categorical data.",popul proport binomi experi,"['popul', 'proport', 'binomi', 'experi']",Population proportion for binomial experiment,popul proport fraction popul certain characteristicfor exampl 1000 peopl popul 237 peopl blue eye fraction peopl blue eye 237 1000 2371000 237 proport hypothesi test may perform categor data
486,"Calculating population parameters for two samples from difference population

> When σ1 and σ2 are known, then assumming normal distribution of the sample mean, we are 95 % confident that,

μ1- μ2= (X̄1-X̄2) ± 1.96*σp*√(1/n1+1/n2))

σp is the pooled population standard deviation

> When σ1 and σ2 are unknown, then assumming t-distribution of the sample mean, we are 95 % confident that 

μ1- μ2 = (X̄1-X̄2) ± t0.025, (n1+n2-2)*Sp*√(1/n1+1/n2)

Sp is the pooled sample standard deviation

> When σ1 and σ2 are unknown, then we can calculate 

σ1^2/σ2^2

> We can calculate the difference in population proportion from sample proportions  with some confidence interval

For 95% confidence interval,

p1-p2 = (p̂1- p̂2)  ± 1.96*sqrt(p̂1*(1-p̂1)/n1+p̂2*(1-p̂2)/n2)",calcul popul paramet two sampl differ popul,"['calcul', 'popul', 'paramet', 'two', 'sampl', 'differ', 'popul']",Calculating population parameters for two samples from difference population,σ1 σ2 known assum normal distribut sampl mean 95 confid thatμ1 μ2 x̄1x̄2 ± 196σp√1n11n2σp pool popul standard deviat σ1 σ2 unknown assum tdistribut sampl mean 95 confid μ1 μ2 x̄1x̄2 ± t0025 n1n22sp√1n11n2sp pool sampl standard deviat σ1 σ2 unknown calcul σ12σ22 calcul differ popul proport sampl proport confid intervalfor 95 confid intervalp1p2 p̂1 p̂2 ± 196sqrtp̂11p̂1n1p̂21p̂2n2
487,"Pooled variance

Pooled variance is a method for estimating variance of several different populations when the mean of each population may be different, but one may assume that the variance of each population is the same as Sp.

Sp^2 = [(n1-1)*S1^2+ (n2-1)*S2^2]/(n1+n2-2)",pool varianc,"['pool', 'varianc']",Pooled variance,pool varianc method estim varianc sever differ popul mean popul may differ one may assum varianc popul spsp2 n11s12 n21s22n1n22
488,"Confidence limits

Confidence limits are the values that mark the boundaries of the confidence interval.

95% confidence level means the probability that the true value of the population parameter falls between the bounds of an already computed confidence interval is roughly 95%.",confid limit,"['confid', 'limit']",Confidence limits,confid limit valu mark boundari confid interval95 confid level mean probabl true valu popul paramet fall bound alreadi comput confid interv rough 95
489,"Sampling schemes from best to worst

i. simple random, 

ii. stratified, 

iii. convenience (or close to hand sample)

> A simple random sample randomly selects individuals from the population without any other consideration. 

> A stratified random sample, on the other hand, first divides the population into smaller groups, or strata, based on shared characteristics. Then randomly selects groups from the population. 

For imbalanced data this can be done during train-test split.

sklearn.model_selection.train_test_split(stratify=y)",sampl scheme best worst,"['sampl', 'scheme', 'best', 'worst']",Sampling schemes from best to worst,simpl random ii stratifi iii conveni close hand sampl simpl random sampl random select individu popul without consider stratifi random sampl hand first divid popul smaller group strata base share characterist random select group popul imbalanc data done traintest splitsklearnmodelselectiontraintestsplitstratifyy
490,"Basics of Hypothesis Testing

> A hypothesis can be defined as a proposed explanation for a phenomenon. It is not the absolute truth but a provisional working assumption.

> In statistics, a hypothesis is considered to be a particular assumption about a set of parameters of a population distribution

> It is called a hypothesis because it is not known whether  it is true or not

A hypothesis test is a standard procedure for testing a claim about a property of a population.

A statement whose validity is tested on the basis of a sample is called Statistical Hypothesis",basic hypothesi test,"['basic', 'hypothesi', 'test']",Basics of Hypothesis Testing,hypothesi defin propos explan phenomenon absolut truth provision work assumpt statist hypothesi consid particular assumpt set paramet popul distribut call hypothesi known whether true nota hypothesi test standard procedur test claim properti populationa statement whose valid test basi sampl call statist hypothesi
491,"Rare Event Rule for Inferential Statistics

If, under a given assumption, the probability of a particular observed event is exceptionally small, we conclude that the assumption is probably not correct",rare event rule inferenti statist,"['rare', 'event', 'rule', 'inferenti', 'statist']",Rare Event Rule for Inferential Statistics,given assumpt probabl particular observ event except small conclud assumpt probabl correct
492,"Components of a formal hypothesis test

> Given  a  claim,  identify  the  null  hypothesis  and  the  alternative hypothesis, and express them both in symbolic form

> Given a claim and sample data, calculate the value of the test statistic.

> Given a significance level, identify the critical value(s)

> Given a value of the test statistic, identify the P-value

> State  the  conclusion  of  a  hypothesis  test  in  simple,  non-technical terms",compon formal hypothesi test,"['compon', 'formal', 'hypothesi', 'test']",Components of a formal hypothesis test,given claim identifi null hypothesi altern hypothesi express symbol form given claim sampl data calcul valu test statist given signific level identifi critic valu given valu test statist identifi pvalu state conclus hypothesi test simpl nontechn term
493,"Null Hypothesis : H0

The null hypothesis (denoted by H0) is  a statement that the value of a population  parameter  (such  as  proportion,  mean,  or  standard deviation) is equal to or <=  or  >= some claimed value.

The statement being tested in a test of statistical significance is called the null hypothesis.

In simple words, null hypothesis is the initial assumption for any hypothesis test.",null hypothesi h0,"['null', 'hypothesi', 'h0']",Null Hypothesis : H0,null hypothesi denot h0 statement valu popul paramet proport mean standard deviat equal claim valueth statement test test statist signific call null hypothesisin simpl word null hypothesi initi assumpt hypothesi test
494,"Alternative Hypothesis : HA

The alternative hypothesis (denoted by H1 or Ha or HA) is the statement that  the  statistic  has  a  value  that  somehow  differs  from  the  null hypothesis

e.g.
Statement is ""On the average, the dosage sold under this brand is 50 mg""
Then,
H0: population mean dosage, μ = 50 mg (On the average, the dosage sold under this brand is 50 mg) 
Ha: population mean dosage, μ ≠ 50 mg",altern hypothesi ha,"['altern', 'hypothesi', 'ha']",Alternative Hypothesis : HA,altern hypothesi denot h1 ha ha statement statist valu somehow differ null hypothesiseg statement averag dosag sold brand 50 mg h0 popul mean dosag μ 50 mg averag dosag sold brand 50 mg ha popul mean dosag μ ≠ 50 mg
495,"Identifying the null and alternative hypothesis

If  we  are  conducting  a  study  and  want  to  use  a hypothesis test to support our claim, the null hypothesis must be worded such that our claim becomes the alternative hypothesis

e.g.
Are boys taller than girls at age eight? The null hypothesis is ""they are the same average height.""
Does eating an apple a day reduce visits to the doctor? The null hypothesis is ""apples do not reduce doctor visits.""",identifi null altern hypothesi,"['identifi', 'null', 'altern', 'hypothesi']",Identifying the null and alternative hypothesis,conduct studi want use hypothesi test support claim null hypothesi must word claim becom altern hypothesiseg boy taller girl age eight null hypothesi averag height eat appl day reduc visit doctor null hypothesi appl reduc doctor visit
496,"Test Statistic

With the assumption that the null hypothesis is true, we find test statistic from the sample statistics and hypothesised statistics

i. Test statistic for proportions 

z_score= (p̂ -p)/sqrt(p*(1-p)/n)

ii. Test statistic for mean

z_score = (x̄ – μ) / (σ/√n)

or,
t_score = (x̄ – μ) / (S/√n)

for two samples,
t_score= (X̄1-X̄2) - (μ1- μ2)/Sp*√(1/n1+1/n2))

iii Test statistic for variance

chi_square_score= (n-1)*S^2/σ^2

for two samples, 
when, Ho=σ1^2=σ2^2,
f_score=S1^2/S2^2",test statist,"['test', 'statist']",Test Statistic,assumpt null hypothesi true find test statist sampl statist hypothesis statisticsi test statist proport zscore p̂ psqrtp1pnii test statist meanzscor x̄ – μ σ√nor tscore x̄ – μ s√nfor two sampl tscore x̄1x̄2 μ1 μ2sp√1n11n2iii test statist variancechisquarescor n1s2σ2for two sampl hoσ12σ22 fscores12s22
497,"Significance Level

The significance level (denoted by α) defines how much evidence we require to reject H0 in favor of HA

> significance level is the lowest probability of null hypothesis in the population

confidence level= 1-significance level

For 95% confidence level, significance level is 0.05.

0.95= 1-0.05",signific level,"['signific', 'level']",Significance Level,signific level denot α defin much evid requir reject h0 favor ha signific level lowest probabl null hypothesi populationconfid level 1signific levelfor 95 confid level signific level 005095 1005
498,"Critical Region

The critical region (or rejection region) is  a set of values for the test statistic for which the null hypothesis is rejected.",critic region,"['critic', 'region']",Critical Region,critic region reject region set valu test statist null hypothesi reject
499,"Critical Value

A critical value is any value that separates the critical region  from the values of the test statistic that do not lead to rejection of the null hypothesis.  

The critical values depend on the nature of the null hypothesis, the sampling distribution that applies, and the significance level

The two‐tailed critical value will be larger than one tailed test for same significance level.

Z= +1.96 and Z= -1.96 are the critical values for 0.05 significance level in normal distribution",critic valu,"['critic', 'valu']",Critical Value,critic valu valu separ critic region valu test statist lead reject null hypothesi critic valu depend natur null hypothesi sampl distribut appli signific levelth two‐tail critic valu larger one tail test signific levelz 196 z 196 critic valu 005 signific level normal distribut
500,"Two-tailed, Right-tailed, Left-tailed Tests

The tails in a distribution are the extreme regions bounded by critical values.

A two-tailed test is a method in which the critical area of a distribution is two-sided

Right-tailed Tests is a method in which the critical area of a distribution is on the right side

Left-tailed Tests is a method in which the critical area of a distribution is on the left side",twotail righttail lefttail test,"['twotail', 'righttail', 'lefttail', 'test']","Two-tailed, Right-tailed, Left-tailed Tests",tail distribut extrem region bound critic valuesa twotail test method critic area distribut twosidedrighttail test method critic area distribut right sidelefttail test method critic area distribut left side
501,"P-value or probability value

The P-value (or probability value) is the probability of observing results as extreme or more extreme than currently observed, given that the null hypothesis is true. 

The level of statistical significance is often expressed as a p-value between 0 and 1. The smaller the p-value, the stronger the evidence that we should reject the null hypothesis.

The null hypothesis is rejected if the P-value is very small, such as 0.05 or less.

If a P-value is small enough, then we say the results are statistically significant

1. Ha ≠ H0- It is Two-tailed test

if cdf(test_statistic)> 0.5:

  p_value=2*(1- cdf(test_statistic))

else:

  p_value=2*cdf(test_statistic)

2. Ha> H0- It is Right-tailed test

p_value= 1- cdf(test_statistic)

3. Ha< H0- It is Left-tailed test

p_value= cdf(test_statistic)",pvalu probabl valu,"['pvalu', 'probabl', 'valu']",P-value or probability value,pvalu probabl valu probabl observ result extrem extrem current observ given null hypothesi true level statist signific often express pvalu 0 1 smaller pvalu stronger evid reject null hypothesisth null hypothesi reject pvalu small 005 lessif pvalu small enough say result statist significant1 ha ≠ h0 twotail testif cdfteststatist 05 pvalue21 cdfteststatisticels pvalue2cdfteststatistic2 ha h0 righttail testpvalu 1 cdfteststatistic3 ha h0 lefttail testpvalu cdfteststatist
502,"Conclusions in Hypothesis Testing based on P-value

We always test the null hypothesis.  The initial conclusion will always be one of the following

i) Reject the null hypothesis - if the P-value ≤ α 

ii) Fail to reject the null hypothesis - if the P-value > α",conclus hypothesi test base pvalu,"['conclus', 'hypothesi', 'test', 'base', 'pvalu']",Conclusions in Hypothesis Testing based on P-value,alway test null hypothesi initi conclus alway one followingi reject null hypothesi pvalu ≤ α ii fail reject null hypothesi pvalu α
503,"Type-I error

A Type-I error is the mistake of rejecting the null hypothesis when it is true.

This is the False Positive case.

Null hypothesis true means actual 0

Null hypothesis false means actual 1

Fail to reject null hypothesis means prediction 0

Rejecting null hypothesis means prediction 1

Example:
Say, in a criminal trial the null hypothesis is the person is innocent. If an innocent person is sent to jail then it is type I error.",typei error,"['typei', 'error']",Type-I error,typei error mistak reject null hypothesi truethi fals posit casenul hypothesi true mean actual 0null hypothesi fals mean actual 1fail reject null hypothesi mean predict 0reject null hypothesi mean predict 1exampl say crimin trial null hypothesi person innoc innoc person sent jail type error
504,"Type-II error

A Type-II error is the mistake of failing to reject the null hypothesis when it is false.

This is the False Negative case.

β (Beta) is the probability of Type-II error in any hypothesis test

Example:
Say, in a criminal trial the null hypothesis is the person is innocent. If a guilty person is set free then it is type II error.",typeii error,"['typeii', 'error']",Type-II error,typeii error mistak fail reject null hypothesi falsethi fals negat caseβ beta probabl typeii error hypothesi testexampl say crimin trial null hypothesi person innoc guilti person set free type ii error
505,"Power of a hypothesis test

The power of a hypothesis test is the probability (1 - beta) of rejecting a false null hypothesis (correct decision)

That is, the power of the hypothesis test is the probability of supporting an alternative hypothesis that is true.

The power of a test can be increased in a number of ways, for example increasing the sample size, decreasing the standard error, increasing the difference between the sample statistic and the hypothesized parameter, or increasing the alpha level.",power hypothesi test,"['power', 'hypothesi', 'test']",Power of a hypothesis test,power hypothesi test probabl 1 beta reject fals null hypothesi correct decisionthat power hypothesi test probabl support altern hypothesi trueth power test increas number way exampl increas sampl size decreas standard error increas differ sampl statist hypothes paramet increas alpha level
506,"encoding and decoding

Encoding means the creation of a messages and decoding means listener or audience of encoded message. So, decoding means interpreting the meaning of the message",encod decod,"['encod', 'decod']",encoding and decoding,encod mean creation messag decod mean listen audienc encod messag decod mean interpret mean messag
507,"Quantum computing

> Quantum computing is a type of computation that harnesses the collective properties of quantum states, to perform calculations (massive computing power).",quantum comput,"['quantum', 'comput']",Quantum computing,quantum comput type comput har collect properti quantum state perform calcul massiv comput power
508,"Blockchain

> Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. ",blockchain,['blockchain'],Blockchain,blockchain system record inform way make difficult imposs chang hack cheat system
509,"Degrees of Freedom

Degrees of Freedom refers to the number of logically independent values, which are values that have the freedom to vary, in the data sample. 

degrees of freedom= sample_size-1

Say, I have 5 shirts for office for Monday to Friday and I can not wear same shirt in a week. Then on Monday I have the freedom to choose any shirt among 5 and on Thursday I have the freedom to choose any one from last two shirts. But on Friday, I do not have any freedom to choose. I need to wear the remaining one. Thus I have (5-1) or 4 degrees of freedom. That means 4 days I have the freedom to choose.

This is like while taking one sample from a set without replacement, then for (n-1) cases there will be randomness or freedom to vary.",degre freedom,"['degre', 'freedom']",Degrees of Freedom,degre freedom refer number logic independ valu valu freedom vari data sampl degre freedom samplesize1say 5 shirt offic monday friday wear shirt week monday freedom choos shirt among 5 thursday freedom choos one last two shirt friday freedom choos need wear remain one thus 51 4 degre freedom mean 4 day freedom choosethi like take one sampl set without replac n1 case random freedom vari
510,"Understanding computer science

Computer science means the knowledge of computer. With the knowledge of computer we can create different functions, models or environments in computer.

> Artificial intelligence is an important part of Computer science

> Machine learning is an important part of Artificial intelligence

> Deep learning is an important part of Machine learning",understand comput scienc,"['understand', 'comput', 'scienc']",Understanding computer science,comput scienc mean knowledg comput knowledg comput creat differ function model environ comput artifici intellig import part comput scienc machin learn import part artifici intellig deep learn import part machin learn
511,"Understanding convensional programming

Convensional programming is nothing but creating a function, model or environment based on different rules as per our understanding or according to the need.

Non-intelligent robot or collaborative robot is also an example of convensional programming.

",understand convension program,"['understand', 'convension', 'program']",Understanding convensional programming,convension program noth creat function model environ base differ rule per understand accord neednonintellig robot collabor robot also exampl convension program
512,"Types of Computer languages

1. Low level language (binary language)-Machine language (First Generation, 1GL) and Assembly language (Second Generation, 2GL)

2. High Level language (based on low level language, human like language)-3GL(C,C++,Java), 4GL(Perl, Javascript, PHP, Python, Ruby, and SQL), 5GL (contains visual tools to help develop a program)",type comput languag,"['type', 'comput', 'languag']",Types of Computer languages,1 low level languag binari languagemachin languag first generat 1gl assembl languag second generat 2gl2 high level languag base low level languag human like language3glccjava 4glperl javascript php python rubi sql 5gl contain visual tool help develop program
513,"Programming language (1GL,2GL,3GL) vs scripting language (4GL,5GL)

> Programming language is compiler based and scripting is interpreter based.

> Programming language is difficult to write but scripting language is easy to write and use

> Programming language does not require host, scripting requires host (for example, a Web browser is a scripting host that can execute instructions)

> C++ programming language needs a compiler (g++)

> Code editor (notepad, Turbo C, VS Code) is needed for writing  code and make .cpp file. Then complier converts .cpp file to .exe file",program languag 1gl2gl3gl vs script languag 4gl5gl,"['program', 'languag', '1gl2gl3gl', 'vs', 'script', 'languag', '4gl5gl']","Programming language (1GL,2GL,3GL) vs scripting language (4GL,5GL)",program languag compil base script interpret base program languag difficult write script languag easi write use program languag requir host script requir host exampl web browser script host execut instruct c program languag need compil g code editor notepad turbo c vs code need write code make cpp file complier convert cpp file exe file
514,"AI technique

AI technique is nothing but implementing small fragments of human like intelligence in the model or environment created by convensional programming. Then the model or environment becomes intelligent model or environment.",artifici intellig techniqu,"['artifici', 'intellig', 'techniqu']",AI technique,artifici intellig techniqu noth implement small fragment human like intellig model environ creat convension program model environ becom intellig model environ
515,"Basics of machine learning

The main feature of human intelligence is learning from the experiences. Thus an intelligent model must have the capability of learning from experiences and machine learning algorithm serves this purpose.

> Human or Machine learning model learns the rule or true function from a noisy realworld experience set.",basic machin learn,"['basic', 'machin', 'learn']",Basics of machine learning,main featur human intellig learn experi thus intellig model must capabl learn experi machin learn algorithm serv purpos human machin learn model learn rule true function noisi realworld experi set
516,"Basics of deep learning

If we need the environment to be intelligent enough to learn from the raw experiences (directly from the sensors) like image, audio, video etc., then we need to implement raw experience processor (a algorithm) to convert raw experiences into organised experience table and deep learning algorithm to learn the rules from the experience table.
",basic deep learn,"['basic', 'deep', 'learn']",Basics of deep learning,need environ intellig enough learn raw experi direct sensor like imag audio video etc need implement raw experi processor algorithm convert raw experi organis experi tabl deep learn algorithm learn rule experi tabl
517,"Basics of data science

> Data Science means the knowledge of data or reading the experiences

> Knowledge of data is required in the field of computer science, artificial intelligence, machine learning, deep learning and business management.",basic data scienc,"['basic', 'data', 'scienc']",Basics of data science,data scienc mean knowledg data read experi knowledg data requir field comput scienc artifici intellig machin learn deep learn busi manag
518,"Role of a data scientist

> A dataset is an experience set and a data scientist is able to understand the features (or Dharma in Sanskrit) of the dataset and their relation.

> Thus, a data scientist can decide the bestfit learning model (algorithm) for a particular problem statement.

> In simple words, a data scientist is like a teacher or guru (in Sanskrit) who is expert in understanding features (or Dharma) and thus expert in training and testing a model.",role data scientist,"['role', 'data', 'scientist']",Role of a data scientist,dataset experi set data scientist abl understand featur dharma sanskrit dataset relat thus data scientist decid bestfit learn model algorithm particular problem statement simpl word data scientist like teacher guru sanskrit expert understand featur dharma thus expert train test model
519,"Difference between Business Analyst and Data Scientist

While a business analyst typically focuses on finding trends in data and developing ways to leverage that information to improve an organization's operations, data scientists tend to look more at what drives those trends.",differ busi analyst data scientist,"['differ', 'busi', 'analyst', 'data', 'scientist']",Difference between Business Analyst and Data Scientist,busi analyst typic focus find trend data develop way leverag inform improv organ oper data scientist tend look drive trend
520,"Predictive ML model

Predictive ML model is a machine learning algorithm which can predict the answer from a new problem statement on completion of training and testing of the model.",predict machin learn model,"['predict', 'machin', 'learn', 'model']",Predictive ML model,predict machin learn model machin learn algorithm predict answer new problem statement complet train test model
521,"Meaning of heuristic technique

A heuristic, or a heuristic technique, is any approach to problem-solving that uses a practical method or various shortcuts in order to produce solutions that may not be optimal but are sufficient given a limited timeframe or deadline.",mean heurist techniqu,"['mean', 'heurist', 'techniqu']",Meaning of heuristic technique,heurist heurist techniqu approach problemsolv use practic method various shortcut order produc solut may optim suffici given limit timefram deadlin
522,"Comparison between Heuristic technique and ML technique

After understanding the dataset for a particular problem statement, a data scientist may decide a heuristic technique (rule based or conventional model) or ML technique (data based or ML model) for problem solving.

Rule based model creates an environment in which data based models operate.",comparison heurist techniqu machin learn techniqu,"['comparison', 'heurist', 'techniqu', 'machin', 'learn', 'techniqu']",Comparison between Heuristic technique and ML technique,understand dataset particular problem statement data scientist may decid heurist techniqu rule base convent model machin learn techniqu data base machin learn model problem solvingrul base model creat environ data base model oper
523,"Types of learning models

1. Supervised learning model (Regression and classification)

2. Unsupervised learning model (Clustering)

3. Reinforcement learning model (Q-learning, policy learning)

> We know that supervised learning algorithm can not work without a target column. But if the requirement is to apply supervised learning without having a target column, then, if the dataset is small enough, we may manually create a target column as per feature understanding and label all the observations.

Otherwise, for big data, we can go for clustering technique for creating a target column with cluster numbers as labels. Now, the dataset has both independent and dependent variables i.e, target column present and supervised learning algorithm can be used.",type learn model,"['type', 'learn', 'model']",Types of learning models,1 supervis learn model regress classification2 unsupervis learn model clustering3 reinforc learn model qlearn polici learn know supervis learn algorithm work without target column requir appli supervis learn without target column dataset small enough may manual creat target column per featur understand label observationsotherwis big data go cluster techniqu creat target column cluster number label dataset independ depend variabl ie target column present supervis learn algorithm use
524,"Application of regression model

> Real estate prediction

> Weather forecasting

> Financial porfolio prediction

> ETA (Estimated Time of Arrival) etc.",applic regress model,"['applic', 'regress', 'model']",Application of regression model,real estat predict weather forecast financi porfolio predict eta estim time arriv etc
525,"Application of classification model

> Credit card fraud detection

> Image classification

> Spam detection

> Insurance decisioning etc. ",applic classif model,"['applic', 'classif', 'model']",Application of classification model,credit card fraud detect imag classif spam detect insur decis etc
526,"Application of Clustering model

> Document theme extraction

> Customer segmentation

> Insurance fraud detection

> Delivery store optimization etc. 

",applic cluster model,"['applic', 'cluster', 'model']",Application of Clustering model,document theme extract custom segment insur fraud detect deliveri store optim etc
527,"Components of reinforcement learning

> Environment

> Agent

> Action

> State and Reward

- It gathers its own experience from the environment and take action. A reward mechanism works inside and thus a reinforcement learning model learns from mistakes.",compon reinforc learn,"['compon', 'reinforc', 'learn']",Components of reinforcement learning,environ agent action state reward gather experi environ take action reward mechan work insid thus reinforc learn model learn mistak
528,"Application of Reinforcement learning model

> Traffic light control

> Resource management

> Robotics

> Games

> Bidding and advertisement etc.",applic reinforc learn model,"['applic', 'reinforc', 'learn', 'model']",Application of Reinforcement learning model,traffic light control resourc manag robot game bid advertis etc
529,"Supervised, parametric, regression algorithm

> Linear regression is a supervised, parametric, regression algorithm. 

> If the organised experience set have answers or labels, then it is called a supervised experience.

> If the answers are continuous numerical values, then it is regression problem.

> The linear regression model consists of independent variables and a dependent variable related linearly to each other. 

> We try to find the relationship between independent variable(input) and a corresponding dependent variable (output)",supervis parametr regress algorithm,"['supervis', 'parametr', 'regress', 'algorithm']","Supervised, parametric, regression algorithm",linear regress supervis parametr regress algorithm organis experi set answer label call supervis experi answer continu numer valu regress problem linear regress model consist independ variabl depend variabl relat linear tri find relationship independ variableinput correspond depend variabl output
530,"Basics of linear regression

Linear regression or multiple linear regression is a statistical technique that can be used to analyze the relationship between a single dependent variable and several independent variables.

If the decesion or answer (dependent variable,Y) from an experience is linearly dependent on the dimensions (independent variables,X1,X2,X3,...Xn), then we can assume that the dependent variable is a function of all individual weights (W0, W1, W2,....Wn). 

Y=W0+W1*X1+W2*X2+W3*X3+...+Wn*Xn+ E

The goal of linear regression is to create a trend line or best fit line

Best fit line is the spinal cord of whole experiences about which the pipeline weights are placed. It is also called the regression line.",basic linear regress,"['basic', 'linear', 'regress']",Basics of linear regression,linear regress multipl linear regress statist techniqu use analyz relationship singl depend variabl sever independ variablesif deces answer depend variabley experi linear depend dimens independ variablesx1x2x3xn assum depend variabl function individu weight w0 w1 w2wn yw0w1x1w2x2w3x3wnxn eth goal linear regress creat trend line best fit linebest fit line spinal cord whole experi pipelin weight place also call regress line
531,"Error or residuals

The errors between the actual value and the predicted value is called the error or residuals.

> Reason of squaring the errors

Predicted value can have positive or negative error. If we don’t square the error, then the positive and negative points will cancel each other out during summation.  That is why we use SSE : Sum square error

SSE is also called Residual sum square (RSS)",error residu,"['error', 'residu']",Error or residuals,error actual valu predict valu call error residu reason squar errorspredict valu posit negat error don't squar error posit negat point cancel summat use sse sum squar errorss also call residu sum squar rss
532,"Loss function and cost function

SSE= Σ(y-W0+W1*X1+W2*X2+W3*X3+...+Wn*Xn)^2

SSE= Σ(y_test - y_pred)^2

This is called L2 loss. In this equation, error of prediction for one observation is squared and then it is summed up for all the observations. 

Absolute Error is called L1 loss which is expressed as  Σ|y_test - y_pred|

Loss function is used for single observation and cost fuction (as it provides the cost of  loss from interpreting the data using a linear regression) is used for the entire dataset

That means,

loss= (y_test - y_pred)^2

cost= Σ(y_test - y_pred)^2",loss function cost function,"['loss', 'function', 'cost', 'function']",Loss function and cost function,sse σyw0w1x1w2x2w3x3wnxn2ss σytest ypred2thi call l2 loss equat error predict one observ squar sum observ absolut error call l1 loss express σytest ypredloss function use singl observ cost fuction provid cost loss interpret data use linear regress use entir datasetthat meansloss ytest ypred2cost σytest ypred2
533,"Types of loss function

Mean Square Error (MSE) or Root Mean Square Error (RMSE) is used instead of SSE to save space and avoid memory explosion with large no. of observations

SSE = Σ(y_actual - y_pred)^2

MSE = 1/n*Σ(y_actual - y_pred)^2

RMSE = np.sqrt(MSE)

Other errors are Mean Absolute Error, Mean Absolute Percentage Error(MAPE), R2 Score (R – Squared) etc.

> Residual Standard Deviation or Residual Standard Error (RSE) is calculated based on SSE (or RSS) and degrees of freedom
RSE = np.sqrt(SSE/(n-2))

> MAPE = np.mean(np.abs((actual - predicted) / actual)) * 100)

> r2 = 1 -RSS/TSS,
where TSS=  total sum of squares of errors = actual - mean

r2_score 0.83 means 83% of variance in Y is explained by variance in all the independent variables (x1, x2,...)

From the RMSE, we can understand the amount of the error in the prediction by our model whereas from r2_score or adjusted_r2, we can understand the perfection of our model (higher r2_score is better)

> adjusted_r2 = 1 - (((n-1)/(n-k-1))*(1-r2_score(actual, predicted)))
n is the number of samples and k is the number of independent variable

Adjusted R-squared, a modified version of R-squared, adds precision and reliability by considering the impact of additional independent variables that tend to skew the results of R-squared measurements. 

The adjusted R-squared shows whether adding additional predictors improve a regression model or not. 

The most vital difference between adjusted R-squared and R-squared is simply that adjusted R-squared considers and tests different independent variables against the model and R-squared does not.",type loss function,"['type', 'loss', 'function']",Types of loss function,mean squar error mse root mean squar error rmse use instead sse save space avoid memori explos larg observationsss σyactual ypred2ms 1nσyactual ypred2rms npsqrtmseother error mean absolut error mean absolut percentag errormean absolut percentag error rsquar score r – squar etc residu standard deviat residu standard error rse calcul base sse rss degre freedom rse npsqrtssen2 mean absolut percentag error npmeannpabsactu predict actual 100 rsquar 1 rsstss tss total sum squar error actual meanrsquaredscor 083 mean 83 varianc explain varianc independ variabl x1 x2from rmse understand amount error predict model wherea rsquaredscor adjustedrsquar understand perfect model higher rsquaredscor better adjustedrsquar 1 n1nk11rsquaredscoreactu predict n number sampl k number independ variableadjust rsquar modifi version rsquar add precis reliabl consid impact addit independ variabl tend skew result rsquar measur adjust rsquar show whether ad addit predictor improv regress model vital differ adjust rsquar rsquar simpli adjust rsquar consid test differ independ variabl model rsquar
534,"OLS method for finding out the model parameters

The OLS method seeks to minimize (finds least value) the sum of the squared residuals.

It is also called the linear least squares. This is a method for determining the unknown parameters located in a linear regression model.

We differentiate loss function first with respect to β0(or W0) and then to β1(or W1) and βn(or Wn)

As per the rule of calculus, setting these partial derivatives equal to zero yields 'n' equation with 'n' unknowns to get the minimum values of β0, β1,....βn .

These equations are known as the normal equations.

Thus we can solve these 'n' equation with matrix method.",ordinari least squar method find model paramet,"['ordinari', 'least', 'squar', 'method', 'find', 'model', 'paramet']",OLS method for finding out the model parameters,ordinari least squar method seek minim find least valu sum squar residualsit also call linear least squar method determin unknown paramet locat linear regress modelw differenti loss function first respect β0or w0 β1or w1 βnor wnas per rule calculus set partial deriv equal zero yield n equat n unknown get minimum valu β0 β1βn equat known normal equationsthus solv n equat matrix method
535,"
Gradient Descent Fundamentals

> Gradient is the slope of cost function and Descent means moving downward

> Gradient Descent is an optimization algorithm which runs on repeated steps (iteration) starting with a random prediction. It helps machine learning models to find the values of the parameters for which the error in prediction will be minimum. It is applied when Parametric ML model is fitted on training data.

> OLS (Non-iterative process) provides exact solution but gradient decent provides approximate but good enough (decent) solution.

> Gradient descent requires additional inputs like alpha and intial random prediction

> Learning rate, α (hyper-parameter) is the step size of moving downward in gradient descent.

> GD is suitable for datasets with large no. of features and observations",gradient descent fundament,"['gradient', 'descent', 'fundament']","
Gradient Descent Fundamentals",gradient slope cost function descent mean move downward gradient descent optim algorithm run repeat step iter start random predict help machin learn model find valu paramet error predict minimum appli parametr machin learn model fit train data ordinari least squar nonit process provid exact solut gradient decent provid approxim good enough decent solut gradient descent requir addit input like alpha intial random predict learn rate α hyperparamet step size move downward gradient descent gradient descent suitabl dataset larg featur observ
536,"
Assumptions of regression

Assumptions of general linear model's (GLM) or Classical Linear Regression Models (CLRM) are:  

1.The relation between the dependent and independent variables is almost linear.

2.There is homoscedasticity or equal variance in a regression model. This assumption means that the variance around the regression line is same for all values of the predictor variable (X).

3.There is no multicollinearity among the independent variables. Multicollinearity generally occurs when there are high correlations between two or more independent variables.

4. There is no auto-correlation in the variables means there is no correlation of current value with previous value.

5.Mean of the residuals is zero or close to 0. It ensures that our regression line is actually the line of “best fit” (running through the middle of the data).

> from the heatmap we can find the high positive or negative correlations between the independent variables",assumpt regress,"['assumpt', 'regress']","
Assumptions of regression",assumpt general linear model glm classic linear regress model clrm 1the relat depend independ variabl almost linear2ther homoscedast equal varianc regress model assumpt mean varianc around regress line valu predictor variabl x3there multicollinear among independ variabl multicollinear general occur high correl two independ variables4 autocorrel variabl mean correl current valu previous value5mean residu zero close 0 ensur regress line actual line “best fit” run middl data heatmap find high posit negat correl independ variabl
537,"Multicollinearity issue

Multicollinearity is a statistical concept where several independent variables in a model are correlated. 

Multicollinearity among independent variables results in less reliable statistical inferences.

To solve multicollinearity issue:

i.Remove some of the highly correlated independent variables.

ii.Linearly combine the independent variables, such as adding them together.

iii.Perform an analysis designed for highly correlated variables, such as principal components analysis",multicollinear issu,"['multicollinear', 'issu']",Multicollinearity issue,multicollinear statist concept sever independ variabl model correl multicollinear among independ variabl result less reliabl statist inferencesto solv multicollinear issueiremov high correl independ variablesiilinear combin independ variabl ad togetheriiiperform analysi design high correl variabl princip compon analysi
538,"Heteroscedasticity issue

Heteroscedasticity refers to data for which the variance of the dependent variable is unequal across the range of independent variable.

To solve heteroscedasticity issue:

i.Log-transformation of features

ii.Outlier treatment

iii.Try polynomial fit",heteroscedast issu,"['heteroscedast', 'issu']",Heteroscedasticity issue,heteroscedast refer data varianc depend variabl unequ across rang independ variableto solv heteroscedast issueilogtransform featuresiioutli treatmentiiitri polynomi fit
539,"Properties of regression line

Regression line passes through the point (mean of independent variable, mean of dependent variable)

This means that for mean of x, the actual y and predicted y will be same (zero error).",properti regress line,"['properti', 'regress', 'line']",Properties of regression line,regress line pass point mean independ variabl mean depend variablethi mean mean x actual predict zero error
540,"Advantages of linear regression

1.Linear regression is simple to implement and easier to interpret the output coefficients

2.When we know the relationship between the independent and dependent variable is linear, linear regression is the best model, because it’s less complex as compared to other algorithms

3.It works well irrespective of data size",advantag linear regress,"['advantag', 'linear', 'regress']",Advantages of linear regression,1linear regress simpl implement easier interpret output coefficients2when know relationship independ depend variabl linear linear regress best model it less complex compar algorithms3it work well irrespect data size
541,"Limitations of linear regression

1.Outliers can have huge effect on the regression line.

2.Prone to underfitting (or Overgeneralizing) - Linear regression sometimes fails to capture the underneath pattern in data properly due to simplicity of the algorithm.
",limit linear regress,"['limit', 'linear', 'regress']",Limitations of linear regression,1outlier huge effect regress line2pron underfit overgener linear regress sometim fail captur underneath pattern data proper due simplic algorithm
542,"Data preparation for linear regression

1. Linear Assumption: We may need to log transform ""generally natural logarithm"" (on X's) for an exponential relationship-It makes our skewed original data more normal. It improves linearity between our dependent and independent variables. 

Linear regression will make more reliable predictions if our independent and dependent variables have a Gaussian distribution or normal distribution.

2. Remove Collinearity

After analyzing the dataset, we need to find the best possible independent variables (have highest possitive or highest negative correlation with y, means the most linear relations) to make any future predictions about our dependent variable

3. Remove Outlier

4. Rescale Inputs",data prepar linear regress,"['data', 'prepar', 'linear', 'regress']",Data preparation for linear regression,1 linear assumpt may need log transform general natur logarithm xs exponenti relationshipit make skew origin data normal improv linear depend independ variabl linear regress make reliabl predict independ depend variabl gaussian distribut normal distribution2 remov collinearityaft analyz dataset need find best possibl independ variabl highest possit highest negat correl mean linear relat make futur predict depend variable3 remov outlier4 rescal input
543,"PyTorch

> PyTorch is a machine learning library developed and released by Facebook’s AI group (FAIR)",pytorch,['pytorch'],PyTorch,pytorch machin learn librari develop releas facebook artifici intellig group fartifici intelligenc
544,"Omission of relevant variable from a regression equation

If a relevant variable is omitted from a regression equation, the consequences would be

1. The standard errors would be biased

2. If the excluded variable is uncorrelated with all of the included variables, all of the slope and intercept coefficients will be inconsistent

> Including relevant lagged values of the dependent variable on the right hand side of a regression equation could lead to 
Biased but consistent coefficient estimates",omiss relev variabl regress equat,"['omiss', 'relev', 'variabl', 'regress', 'equat']",Omission of relevant variable from a regression equation,relev variabl omit regress equat consequ would be1 standard error would biased2 exclud variabl uncorrel includ variabl slope intercept coeffici inconsist includ relev lag valu depend variabl right hand side regress equat could lead bias consist coeffici estim
545,"Visualizing Linear Regression

We can visualize an experience as a pipeline which consists of indivisual straight pipelines in different dimension. As different dimension has weight, we can assume that the diameter of pipeline is different for different dimension. Thus weight per unit length will be different for different dimension.

Weight per unit length is the feature or property of the dimension. Here it is known as coefficient of dimension (W1,W2,...Wn).

W1, W2,…Wn are also called model parameters.

Here we are considering material and thickness of the pipeline are same for all the dimensions. 

One experience row tells us about different values in deifferent dimensions. Here the values we are assuming as length. Therefore, to get the weight of one indivisual straight pipeline, we need to multiply the weight/length with the length. To get the total weight(y), we will sum up the individual weights in all dimension (W1*X1+W2*X2+ W3*X3+...+Wn*Xn).

Linear regression model learns the properties of all dimension from the experience set.

Intercept, W0 is nothing but a fixed weight of the pipeline which is independent of observations in different dimensions. This also called Bias term. Linear regression model also learns this parameter.

f(X)=W0+W1*X1+W2*X2+W3*X3+...+Wn*Xn is the equation of total predicted weight of the pipeline.

X1, X2,…Xn are the feature values

For building a supervised learning model, the actual total weight of any experience pipeline is known for the train or test data.

The model initially starts with the prediction of any random total weight of the experience pipeline in the the gradient descent. Then, it starts minimizing the error between actual and prediction. Finally the model learns the properties of all dimensions where the total error for total weight prediction is minimum.",visual linear regress,"['visual', 'linear', 'regress']",Visualizing Linear Regression,visual experi pipelin consist indivisu straight pipelin differ dimens differ dimens weight assum diamet pipelin differ differ dimens thus weight per unit length differ differ dimensionweight per unit length featur properti dimens known coeffici dimens w1w2wnw1 w2…wn also call model parametersher consid materi thick pipelin dimens one experi row tell us differ valu deiffer dimens valu assum length therefor get weight one indivisu straight pipelin need multipli weightlength length get total weighti sum individu weight dimens w1x1w2x2 w3x3wnxnlinear regress model learn properti dimens experi setintercept w0 noth fix weight pipelin independ observ differ dimens also call bias term linear regress model also learn parameterfxw0w1x1w2x2w3x3wnxn equat total predict weight pipelinex1 x2…xn featur valuesfor build supervis learn model actual total weight experi pipelin known train test datath model initi start predict random total weight experi pipelin gradient descent start minim error actual predict final model learn properti dimens total error total weight predict minimum
546,"Understanding of Feature scaling 

Normalization and Standardization both are feature scaling technique which are performed to bring all features in same range for ease of interpretation by our model.

Normalization technique:

After normalization, the data has min 0 and max 1

Mathematical operation:

(x-Min)/Range

MinMaxScaler() is the scikit-learn method

Standardization technique:

After standardization, the data has mean 0 and standard deviation 1

Mathematical operation:

(x-Mean)/Std. Dev

StandardScaler() is the scikit-learn method

> If required, we can perform MinMaxScaler operation after performing StandardScaler operation, when we want to see the data in same minimum (0) and maximum (1) range

> Unlike normalization, standardization does not have a bounding range.

> Scaling is performed after the train-test split because we do not perform scaling on the target variable.",understand featur scale,"['understand', 'featur', 'scale']",Understanding of Feature scaling ,normal standard featur scale techniqu perform bring featur rang eas interpret modelnorm techniqueaft normal data min 0 max 1mathemat operationxminrangeminmaxscal scikitlearn methodstandard techniqueaft standard data mean 0 standard deviat 1mathemat operationxmeanstd devstandardscal scikitlearn method requir perform minmaxscal oper perform standardscal oper want see data minimum 0 maximum 1 rang unlik normal standard bound rang scale perform traintest split perform scale target variabl
547,"Difference bwteen Matrix and metric

Matrix is matrix while metric is a measure for something; a means of deriving a quantitative measurement or approximation for otherwise qualitative phenomena (especially used in software engineering)",differ bwteen matrix metric,"['differ', 'bwteen', 'matrix', 'metric']",Difference bwteen Matrix and metric,matrix matrix metric measur someth mean deriv quantit measur approxim otherwis qualit phenomena especi use softwar engin
548,"Libraries for linear regression

from numpy import math

from sklearn.preprocessing import PolynomialFeatures

from sklearn.preprocessing import MinMaxScaler

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import r2_score

from sklearn.metrics import mean_squared_error",librari linear regress,"['librari', 'linear', 'regress']",Libraries for linear regression,numpi import mathfrom sklearnpreprocess import polynomialfeaturesfrom sklearnpreprocess import minmaxscalerfrom sklearnmodelselect import traintestsplitfrom sklearnlinearmodel import linearregressionfrom sklearnmetr import r2scorefrom sklearnmetr import meansquarederror
549,"Importance of csv file

> csv format is used for importing and exporting data in different softwares.

> A csv file can be viewed through notepad, ms word, ms excel, google spreadsheet, sql, python etc.

> Thus a file which is required to be viewed through multiple different softwares, are saved in csv format

> Formats like xls, xlsx, json (JavaScript Object Notation) etc. are also used for import-export purpose",import csv file,"['import', 'csv', 'file']",Importance of csv file,csv format use import export data differ softwar csv file view notepad ms word ms excel googl spreadsheet sql python etc thus file requir view multipl differ softwar save csv format format like xls xlsx json javascript object notat etc also use importexport purpos
550,"Implementation Steps of Linear Regression

1. Create dummy numerical variables for the catgeorical variable

> Dummy variables are useful because they enable us to use a single regression equation to represent multiple groups. The dummy variables act like 'switches' that turn various parameters on and off in an equation.

dataset['NewYork_State'] = np.where(dataset['State']=='New York', 1, 0)

2. Create two separate dataset(numpy array) for independent variables and dependent variable

dependent_variable = 'Profit'
independent_variables = list(set(dataset.columns.tolist()) - {dependent_variable})

X = dataset[independent_variables].values

y = dataset[dependent_variable].values

3. Splitting both the datasets into the Training set and Test set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

4. Scaling and transforming both the training and test data

5. Training the ML model
regressor = LinearRegression()

regressor.fit(X_train, y_train)

6. Testing the ML Model
y_pred = regressor.predict(X_test)

7. Evaluating the ML Model
mean_squared_error(y_test, y_pred)

r2_score(y_test, y_pred)
> compare the prediction error both for train and test set

8. Explaining the model
regressor.coef_
regressor.intercept_",implement step linear regress,"['implement', 'step', 'linear', 'regress']",Implementation Steps of Linear Regression,1 creat dummi numer variabl catgeor variabl dummi variabl use enabl us use singl regress equat repres multipl group dummi variabl act like switch turn various paramet equationdatasetnewyorkst npwheredatasetstatenew york 1 02 creat two separ datasetnumpi array independ variabl depend variabledependentvari profit independentvari listsetdatasetcolumnstolist dependentvariablex datasetindependentvariablesvaluesi datasetdependentvariablevalues3 split dataset train set test setxtrain xtest ytrain ytest traintestsplitx testsiz 02 randomst 04 scale transform train test data5 train machin learn model regressor linearregressionregressorfitxtrain ytrain6 test machin learn model ypred regressorpredictxtest7 evalu machin learn model meansquarederrorytest ypredr2scoreytest ypred compar predict error train test set8 explain model regressorcoef regressorintercept
551,"transform and fit_transform

>  Why we use fit_transform() on training data but transform() on the test data?

scaler= MinMaxScaler()

X_train_scaled=scaler.fit_transform(X_train)

X_test_scaled= scaler.transform(X_test)

.fit_transform() is the combined method for .fit() and .transform()

fit method calculates the minimum and maximum values for each of the features and transform method transforms all the features using the respective min and max.

Using the transform method we can use the same min and max (mean and variance for StandardScaler) as it is calculated from our training data to transform our test data.",transform fittransform,"['transform', 'fittransform']",transform and fit_transform,use fittransform train data transform test datascal minmaxscalerxtrainscaledscalerfittransformxtrainxtestsc scalertransformxtestfittransform combin method fit transformfit method calcul minimum maximum valu featur transform method transform featur use respect min maxus transform method use min max mean varianc standardscal calcul train data transform test data
552,"Optimal Model

The bias-variance tradeoff is a conceptual idea in applied machine learning to help understand the sources of error in models.

The learning model is called best fit model when the total error (Bias error+Variance error) in prediction on test data is optimal.

Bias and Variance are statistical quantities

Bias and variance are used to analyse the performance of the model on unseen data

> For the best performance of our ML models, there shall be great randomness in the selection of  experiences from the population. 

> The bias or influence in the selection of experiences shall be optimal. Then the sample data will be a true representative of the population data.",optim model,"['optim', 'model']",Optimal Model,biasvari tradeoff conceptu idea appli machin learn help understand sourc error modelsth learn model call best fit model total error bias errorvari error predict test data optimalbia varianc statist quantitiesbia varianc use analys perform model unseen data best perform machin learn model shall great random select experi popul bias influenc select experi shall optim sampl data true repres popul data
553,"Underfit Model

1. When the model is very simple it is called underfit model. Here the total error in prediction is high due to high bias and low variance.

2. Simple model is not able to catch proper signal from the experience. It learns very low weight per unit length for the features and learns a very high fixed weight or intercept value (bias)

> We can call them overgeneralizing model

3. As the weight/length in all the dimensions are small enough, there is a very little impact of the dimensions of the experience. Thus for different experience there will be very little variation in performance (little difference between train and test performance metric). Here, the spread of prediction is also low. This is called low variance.",underfit model,"['underfit', 'model']",Underfit Model,1 model simpl call underfit model total error predict high due high bias low variance2 simpl model abl catch proper signal experi learn low weight per unit length featur learn high fix weight intercept valu bias call overgener model3 weightlength dimens small enough littl impact dimens experi thus differ experi littl variat perform littl differ train test perform metric spread predict also low call low varianc
554,"Overfit Model

1. When the model is very complex it is called overfit model. Here the total error in prediction is high due to high variance and low bias.

2. Complex model catches the signal along with noise (outliers or exceptions) from the experience. It provides overimportance to each dimension of the experience and learns very high weight/length for the dimensions of the experience.

Thus for different experience there will be huge variation in the performance (there is huge difference between train performance metric and test performance metric). In this situation the spread of prediction (variance) will also be high.

3. For a complex model, as the weight/length is very high for the dimensions, the intercept value is very low. This is called low bias.",overfit model,"['overfit', 'model']",Overfit Model,1 model complex call overfit model total error predict high due high varianc low bias2 complex model catch signal along nois outlier except experi provid overimport dimens experi learn high weightlength dimens experiencethus differ experi huge variat perform huge differ train perform metric test perform metric situat spread predict varianc also high3 complex model weightlength high dimens intercept valu low call low bias
555,"Understanding Estimator

Estimator is the rule or equation learned by the ML model which is a close approximation of the true function hidden or underlying in the experience set.

In other words, an estimator models the relationship between independent and dependent variable

Thus an estimator is able to estimate or predict the dependent variable for a set of independent variables",understand estim,"['understand', 'estim']",Understanding Estimator,estim rule equat learn machin learn model close approxim true function hidden under experi setin word estim model relationship independ depend variablethus estim abl estim predict depend variabl set independ variabl
556,"Polynomial model 

In place of linear regression model we can choose polynomial model and train the model on training data

Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial.

e.g.

list_of_polynomial_degrees = [1, 3, 10]
theta = {}
fit = {}

for degree_of_polynomial in list_of_polynomial_degrees:

  theta[degree_of_polynomial] = np.polyfit(X_train, y_train, degree_of_polynomial)
 
To view the model parameters

for degree_of_polynomial in list_of_polynomial_degrees:

  fit[degree_of_polynomial] = np.polyval(theta[degree_of_polynomial], x_grid)

> x_grid is the instance at which need to evaluate the function",polynomi model,"['polynomi', 'model']",Polynomial model ,place linear regress model choos polynomi model train model train datapolynomi regress form regress analysi relationship independ variabl x depend variabl model nth degre polynomialeglistofpolynomialdegre 1 3 10 theta fit degreeofpolynomi listofpolynomialdegre thetadegreeofpolynomi nppolyfitxtrain ytrain degreeofpolynomi view model parametersfor degreeofpolynomi listofpolynomialdegre fitdegreeofpolynomi nppolyvalthetadegreeofpolynomi xgrid xgrid instanc need evalu function
557,"Conversion of Categorical column to numerical

In machine learning, we usually deal with datasets that contain multiple labels in one or more than one columns. These labels can be in the form of words or numbers. To make the data understandable or in human-readable form, the training data is often labelled in words. 

Label Encoding refers to converting the labels into a numeric form so as to convert them into the machine-readable form.

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

my_df['categorical_column']= label_encoder.fit_transform(my_df['categorical_column'])

For one hot encoding,

my_df = my_df.join(pd.get_dummies(my_df['column_name'],drop_first=True))",convers categor column numer,"['convers', 'categor', 'column', 'numer']",Conversion of Categorical column to numerical,machin learn usual deal dataset contain multipl label one one column label form word number make data understand humanread form train data often label word label encod refer convert label numer form convert machineread formfrom sklearnpreprocess import labelencoderlabelencod labelencodermydfcategoricalcolumn labelencoderfittransformmydfcategoricalcolumnfor one hot encodingmydf mydfjoinpdgetdummiesmydfcolumnnamedropfirsttru
558,"Multi Label columns to Binary

from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer(sparse_output=True)

df = df.join(
            pd.DataFrame.sparse.from_spmatrix(
                mlb.fit_transform(df.pop('genres_list')),
                index=df.index,
                columns=mlb.classes_))

> One topic_combination column to all individual topic columns",multi label column binari,"['multi', 'label', 'column', 'binari']",Multi Label columns to Binary,sklearnpreprocess import multilabelbinarizermlb multilabelbinarizersparseoutputtruedf dfjoin pddataframesparsefromspmatrix mlbfittransformdfpopgenreslist indexdfindex columnsmlbclass one topiccombin column individu topic column
559,"Number-String to numerical value

my_df[list_of columns]=my_df[list_of columns].astype('float64')

or

pd.to_numeric(my_df[list_of columns])",numberstr numer valu,"['numberstr', 'numer', 'valu']",Number-String to numerical value,mydflistof columnsmydflistof columnsastypefloat64orpdtonumericmydflistof column
560,"Noise, Underfittiing, Overfitting and Overgeneralizing 

> Underfitting means giving less importance to the features

> Overfitting means giving more importance to the features

> Overgeneralizing means giving more importance to the bias

> There is no noise-free or pure data (experience) in the world, because every data or experience is influenced by some surrounding factors. 

> So, larger the dataset, better will be the understanding of noise-free trend by the learning model. This is called optimal model 

> For smaller dataset, learning model can be simple (looking at small no. of features) or complex (looking at large no. of features). 

> With the increase in training data, Bias increases and Variance decreases for an overfitting model",nois underfitti overfit overgener,"['nois', 'underfitti', 'overfit', 'overgener']","Noise, Underfittiing, Overfitting and Overgeneralizing ",underfit mean give less import featur overfit mean give import featur overgener mean give import bias noisefre pure data experi world everi data experi influenc surround factor larger dataset better understand noisefre trend learn model call optim model smaller dataset learn model simpl look small featur complex look larg featur increas train data bias increas varianc decreas overfit model
561,"Basics of Regularized Linear Regression

Regularized linear regression is the first option for linear regression problem on small dataset (<1lakh experiences)

When we fit normal linear regression model in our small training data with large no. of features, then there is a chance that the intercept value (bias) is very low and other model coeffients (variances) are high enough, then our model will become a complex model.

Regularized Linear Regression reduces the chance of model complexity

When we fit regularized linear regression with perfect tuning parameter λ, it increases the intercept value and reduces other model parameters towards zero. 

λ is denoted by alpha(α) in sklearn",basic regular linear regress,"['basic', 'regular', 'linear', 'regress']",Basics of Regularized Linear Regression,regular linear regress first option linear regress problem small dataset 1lakh experienceswhen fit normal linear regress model small train data larg featur chanc intercept valu bias low model coeffient varianc high enough model becom complex modelregular linear regress reduc chanc model complexitywhen fit regular linear regress perfect tune paramet λ increas intercept valu reduc model paramet toward zero λ denot alphaα sklearn
562,"Types of Regularization

i. Ridge (L2 Regularization)

ii. Lasso (L1 Regularization)

Regularized Liear Regression minimizes the sum of RSS and a ""penalty term""

Thus the cost function for Regularized LR model,

f(model parameters)= RSS+penalty term

Penalty term has a tuning parameter lambda(λ) multiplied with model coefficients. 

It is best to try both regularization and see which one works better. Usually L2 regularization can be expected to give superior performance over L1.

There's also a ElasticNet regression, which is a combination of Lasso regression and Ridge regression.

Lasso regression is preferred if we want a sparse model, meaning that we believe many features are irrelevant to the output.",type regular,"['type', 'regular']",Types of Regularization,ridg l2 regularizationii lasso l1 regularizationregular liear regress minim sum rss penalti termthus cost function regular lr modelfmodel paramet rsspenalti termpenalti term tune paramet lambdaλ multipli model coeffici best tri regular see one work better usual l2 regular expect give superior perform l1there also elasticnet regress combin lasso regress ridg regressionlasso regress prefer want spars model mean believ mani featur irrelev output
563,"Ridge Regression (L2 Regularization)

Ridge means a long, narrow, elevated strip of land.

> Here the penalty term has sum of model coefficient Square

> It has closed form solution (finite no. of operations)

> Ridge regression shrinks coefficients toward zero, but they rarely reach zero
",ridg regress l2 regular,"['ridg', 'regress', 'l2', 'regular']",Ridge Regression (L2 Regularization),ridg mean long narrow elev strip land penalti term sum model coeffici squar close form solut finit oper ridg regress shrink coeffici toward zero rare reach zero
564,"Lasso Regression (L1 Regularization)

Lasso stands for least absolute shrinkage and selection operator. 

> Here the penalty term has sum of model coefficient Modulus

> Lasso regression shrinks coefficients all the way to zero, thus removing few features from the model.",lasso regress l1 regular,"['lasso', 'regress', 'l1', 'regular']",Lasso Regression (L1 Regularization),lasso stand least absolut shrinkag select oper penalti term sum model coeffici modulus lasso regress shrink coeffici way zero thus remov featur model
565,"Libraries for Regularized Linear Regression

from sklearn.datasets import load_boston

The Boston housing dataset is a famous dataset from 1970s. It contains 506 observations and 13 features on housing prices around Boston.

from sklearn.linear_model import Ridge, RidgeCV

from sklearn.linear_model import Lasso, LassoCV

from sklearn.preprocessing import StandardScaler
",librari regular linear regress,"['librari', 'regular', 'linear', 'regress']",Libraries for Regularized Linear Regression,sklearndataset import loadbostonth boston hous dataset famous dataset 1970s contain 506 observ 13 featur hous price around bostonfrom sklearnlinearmodel import ridg ridgecvfrom sklearnlinearmodel import lasso lassocvfrom sklearnpreprocess import standardscal
566,"Alpha value

my_model=Ridge(alpha=0.1, fit_intercept=True)

my_model=Lasso(alpha=0.1 , max_iter= 3000)

For a very small λ or alpha (near zero), Regularized LR model will act like a normal LR model

For a very large λ or alpha, large coefficients are not penalized 

> normal range of alpha is between 0 and 0.1

In Lasso and Ridge regression as alpha value increases, the slope of the regression line reduces and becomes horizontal (means only have bias, no slope or weight).",alpha valu,"['alpha', 'valu']",Alpha value,mymodelridgealpha01 fitintercepttruemymodellassoalpha01 maxit 3000for small λ alpha near zero regular lr model act like normal lr modelfor larg λ alpha larg coeffici penal normal rang alpha 0 01in lasso ridg regress alpha valu increas slope regress line reduc becom horizont mean bias slope weight
567,"Variance Inflation Factor (VIF) 

It quantifies the severity of multicollinearity

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):

    # Calculating VIF

    vif = pd.DataFrame()
    vif[""variables""] = X.columns
    vif[""VIF""] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

calc_vif(dataset[[i for i in dataset.describe().columns if i not in ['car_ID','price']]])

We need to bring the VIF for the final independent numerical feature <10, to get rid of multicolinearity.

By repeated checking of VIF and  heatmap, we remove the independent variables with multicolinearity",varianc inflat factor vif,"['varianc', 'inflat', 'factor', 'vif']",Variance Inflation Factor (VIF) ,quantifi sever multicollinearityfrom statsmodelsstatsoutliersinflu import varianceinflationfactordef calcvari inflat factorx calcul varianc inflat factor varianc inflat factor pddatafram varianc inflat factorvari xcolumn varianc inflat factorvari inflat factor varianceinflationfactorxvalu rangexshape1 returnvari inflat factorcalcvari inflat factordataseti datasetdescribecolumn caridpricew need bring varianc inflat factor final independ numer featur 10 get rid multicolinearitybi repeat check varianc inflat factor heatmap remov independ variabl multicolinear
568,"Cross validation Basics

Cross validation adds more randomness in the selection of training and testing experience set by subseting the main dataset, to make our model more accurate in prediction. 

Thus helps us to avoid overfitting.",cross valid basic,"['cross', 'valid', 'basic']",Cross validation Basics,cross valid add random select train test experi set subset main dataset make model accur predict thus help us avoid overfit
569,"Simple Validation vs Cross Validation

> In simple validation we divide the data into two pieces.  80% for training and 20% for testing

> In Cross validation, we divide the data into 'k' pieces or buckets (files) and randomly select any one piece as testing data and the combined rest pieces as training data",simpl valid vs cross valid,"['simpl', 'valid', 'vs', 'cross', 'valid']",Simple Validation vs Cross Validation,simpl valid divid data two piec 80 train 20 test cross valid divid data k piec bucket file random select one piec test data combin rest piec train data
570,"k-fold CV

Cross validation is also known as k-fold CV. In k-fold cross validation, we have multiple(k) train-test sets instead of 1. 

This basically means that in a k-fold CV we will be training our model k-times and also testing it k-times.

Then, we check the cross_validation_score

> When the cross_validation_score becomes very high, near 100% then there may be overfitting isssue.

> Other than k-fold there is stratified Cross Validation (Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label)

> If K=N, then it is called Leave one out cross validation, where N is the number of observations.",kfold cross valid,"['kfold', 'cross', 'valid']",k-fold CV,cross valid also known kfold cross valid kfold cross valid multiplek traintest set instead 1 basic mean kfold cross valid train model ktime also test ktimesthen check crossvalidationscor crossvalidationscor becom high near 100 may overfit isssu kfold stratifi cross valid stratifi ensur fold dataset proport observ given label kn call leav one cross valid n number observ
571,"Python coding for CV

from sklearn.model_selection import cross_val_score

results = cross_val_score(my_model,X_test,y_test, cv = 3)

print(f'[""accuracy"", ""precision"", ""recall""]: {results}')

> classification_report provides simple validation report whereas cross_val_score provides cross validation report

> For K-cross validation, smaller k implies less variance and larger k increases variance. cv=3 means 3 fold cross validation",python code cross valid,"['python', 'code', 'cross', 'valid']",Python coding for CV,sklearnmodelselect import crossvalscoreresult crossvalscoremymodelxtestytest cross valid 3printfaccuraci precis recal result classificationreport provid simpl valid report wherea crossvalscor provid cross valid report kcross valid smaller k impli less varianc larger k increas varianc cross validation3 mean 3 fold cross valid
572,"yellowbrick CVScores

The results from each evaluation are averaged together for a final score, then the final model is fit on the entire dataset for operationalization

!pip install yellowbrick

from yellowbrick.model_selection import CVScores

visualizer = CVScores(model, cv=3, scoring='f1_weighted')

visualizer.fit(X, y)        
visualizer.show()",yellowbrick cvscore,"['yellowbrick', 'cvscore']",yellowbrick CVScores,result evalu averag togeth final score final model fit entir dataset operationalizationpip instal yellowbrickfrom yellowbrickmodelselect import cvscoresvisu cvscoresmodel cv3 scoringf1weightedvisualizerfitx visualizershow
573,"Fundamentals of hyperparameters

The parameters apart from model parameters whose values are used to control the learning process are called hyperparameters.

Examples:
n_iter

test_size

max_depth

random_state

n_neighbors

alpha

C

gamma

n_components

karnel

metric

n_folds

penalty

cv

> random_state hyperparameter of sklearn does similar thing as seed function",fundament hyperparamet,"['fundament', 'hyperparamet']",Fundamentals of hyperparameters,paramet apart model paramet whose valu use control learn process call hyperparametersexampl nitertestsizemaxdepthrandomstatenneighborsalphacgammancomponentskarnelmetricnfoldspenaltycv randomst hyperparamet sklearn similar thing seed function
574,"Hyperparameters tuning

GridSearchCV is a method to tune the hyperparameters

It trains the model using all possible combinations of hyperparameters and gives the best combination based on the best k-fold cv score obtained 

GridSearch is very slow (alternate options may be RandomSearchCV or  Bayesian Hyperparameter Optimization)

Bayesian optimization is usually employed to optimize expensive-to-evaluate functions like XGBoost, Random Forest. It takes combinations of X's intelligently.",hyperparamet tune,"['hyperparamet', 'tune']",Hyperparameters tuning,gridsearchcross valid method tune hyperparametersit train model use possibl combin hyperparamet give best combin base best kfold cross valid score obtain gridsearch slow altern option may randomsearchcross valid bayesian hyperparamet optimizationbayesian optim usual employ optim expensivetoevalu function like xgboost random forest take combin xs intellig
575,"Coding for Hyperparameters tuning

from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import RandomizedSearchCV

parameters = {'model_param': list_of_values}

cross_validated_model = GridSearchCV(my_model, param_grid=parameters, scoring='r2', cv=3, n_jobs=-1)

or 
cross_validated_model = RandomizedSearchCV(my_model, param_distributions=parameters, scoring='r2', cv=3, n_jobs=-1)

cross_validated_model.fit(X_train,y_train)

cross_validated_model.best_params_

cross_validated_model.best_score_

y_preds = cross_validated_model.predict(X_test)

> Here cv=3 means 3 fold cross validation, n_jobs=-1 means using all processors for higher speed in cross validation.",code hyperparamet tune,"['code', 'hyperparamet', 'tune']",Coding for Hyperparameters tuning,sklearnmodelselect import gridsearchcvfrom sklearnmodelselect import randomizedsearchcvparamet modelparam listofvaluescrossvalidatedmodel gridsearchcvmymodel paramgridparamet scoringr2 cv3 njobs1or crossvalidatedmodel randomizedsearchcvmymodel paramdistributionsparamet scoringr2 cv3 njobs1crossvalidatedmodelfitxtrainytraincrossvalidatedmodelbestparamscrossvalidatedmodelbestscoreypr crossvalidatedmodelpredictxtest cv3 mean 3 fold cross valid njobs1 mean use processor higher speed cross valid
576,"Steps of ML modelling

Step-1: Detailed commenting on problem statement at the begining of the code

Step-2: Installing latest libraries with pip install, if required

Step-3: Importing necessary libraries for using readymade functions

Step-4: Data server connection with notebook

Step-5: Data importing or reading as pandas dataframe

Step-6: Data inspection with pandas methods and functions

Step-7: Initial Data Preparation/Wrangling/ cleaning and saving the file as different csv

Data cleaning activities include:

> Dealing with missing values

> Dealing with outliers

> Correcting typos

> Grouping sparse classes

> Dropping duplicates

Step-8: Initial data exploration with pandas profiling 

Step-9: Final Data Preparation/wrangling  with transformation

Data Transformation activities and techniques include:

> Categorical encoding

> Dealing with skewed data

> Scaling

> Bias mitigation

> Rank transformation

> Power functions

Step-10: Feature Engineering by creating new feature based upon knowledge about current features and required task

Step-11: Final data exploration through data visualization tools

Step-12: Correct ML model selection or solution of problem statement from the understanding of data
 
Step-13: Train-Test spliting of the data

Step-14: Training the ML model for learning the pattern of the data on fitting training experience set

Step-15: Improving the ML model through Cross validation and hyperparameter tuning

Step-16: Testing the ML model for evaluating its understanding of the trend on fitting testing experience set

Step-17: Retraining after error in prediction analysis and retesting the model for further evaluation

Step-18: Checking the performance of the model in realtime deployment",step machin learn model,"['step', 'machin', 'learn', 'model']",Steps of ML modelling,step1 detail comment problem statement begin codestep2 instal latest librari pip instal requiredstep3 import necessari librari use readymad functionsstep4 data server connect notebookstep5 data import read panda dataframestep6 data inspect panda method functionsstep7 initi data preparationwrangl clean save file differ csvdata clean activ includ deal miss valu deal outlier correct typo group spars class drop duplicatesstep8 initi data explor panda profil step9 final data preparationwrangl transformationdata transform activ techniqu includ categor encod deal skew data scale bias mitig rank transform power functionsstep10 featur engin creat new featur base upon knowledg current featur requir taskstep11 final data explor data visual toolsstep12 correct machin learn model select solut problem statement understand data step13 traintest splite datastep14 train machin learn model learn pattern data fit train experi setstep15 improv machin learn model cross valid hyperparamet tuningstep16 test machin learn model evalu understand trend fit test experi setstep17 retrain error predict analysi retest model evaluationstep18 check perform model realtim deploy
577,"Understanding Warnings

Warnings are provided to warn the developer of situations that aren’t necessarily exceptions. Usually, a warning occurs when there is some obsolete of certain programming elements, such as keyword, function or class, etc. A warning in a program is distinct from an error. Python program terminates immediately if an error occurs. Conversely, a warning is not critical.

import warnings

warnings.filterwarnings('ignore')",understand warn,"['understand', 'warn']",Understanding Warnings,warn provid warn develop situat aren't necessarili except usual warn occur obsolet certain program element keyword function class etc warn program distinct error python program termin immedi error occur convers warn criticalimport warningswarningsfilterwarningsignor
578,"Data preparation or data preprocessing

Data preparation (also referred to as “data preprocessing”) is the process of transforming raw data so that data scientists and analysts can run it through machine learning algorithms to uncover insights or make predictions.

Cross-validation gives a more accurate measure of model quality, which is especially important if we are making a lot of modeling decisions.

Before applying any transformation, we need to perform EDA to understand the shape of the distribution

Seaborn distplot or histogram plot may used to understand the probability distribution

To see all the shape of the data in a single graph, we can use pairplot and specify the kind of non-diagonal graphs, kind of diagonal graph and hue for target_feature",data prepar data preprocess,"['data', 'prepar', 'data', 'preprocess']",Data preparation or data preprocessing,data prepar also refer “data preprocessing” process transform raw data data scientist analyst run machin learn algorithm uncov insight make predictionscrossvalid give accur measur model qualiti especi import make lot model decisionsbefor appli transform need perform exploratori data analysi understand shape distributionseaborn distplot histogram plot may use understand probabl distributionto see shape data singl graph use pairplot specifi kind nondiagon graph kind diagon graph hue targetfeatur
579,"Linear Transformation or Scaling

MinMaxScaler or StandardScaler are used for linear transformations.

Linear transformation only does scaling, it has nothing to do with skewness of data

For MinMax scaling:

X_new= (X- min(X)) /(max(X)-min(X))

For Standard scaling:

X_new= (X- mean(X)) / Std(X)",linear transform scale,"['linear', 'transform', 'scale']",Linear Transformation or Scaling,minmaxscal standardscal use linear transformationslinear transform scale noth skew datafor minmax scalingxnew x minx maxxminxfor standard scalingxnew x meanx stdx
580,"Non-linear Transformations (for dealing with Skewed data)

> square-root for moderate skew: 

sqrt(x) for positively skewed data, 

sqrt(max(x+1) - x) for negatively skewed data

> log (or boxcox) for greater skew: 

log10(x) for positively skewed data, 

log10(max(x+1) - x) for negatively skewed data

np.log10(my_df['column_name'])

from scipy import stats

stats.boxcox(my_df['column_name']

> inverse for severe skew: 

1/x for positively skewed data 

1/(max(x+1) - x) for negatively skewed data

np.reciprocal(my_df['column_name']

> After the transformation, the distribution becomes more approximate to normal distribution.

> After taking log, zero values in some of columns are transformed with -Inf 

np.isinf(df[col_name]).values.sum()

Then after transformation, we need to replace inf values with zero

for col in X.columns:
  X[col].replace([np.inf, -np.inf], 0, inplace=True)

There is an alternate way. The np.log1p() method returns log(1+number), computed in a way that is accurate even when the value of number is close to zero. Here we do not get inf values. (But log10 may give better prediction, thus log10 also needs to be explored)

np.log1p(2.7183)

Linearity and heteroscedasticity:

First try log transformation in a situation where the dependent variable starts to increase more rapidly with increasing independent variable

If our data does the opposite – dependent variable decrease more rapidly with increasing independent variable – we can first consider a square-root transformation.",nonlinear transform deal skew data,"['nonlinear', 'transform', 'deal', 'skew', 'data']",Non-linear Transformations (for dealing with Skewed data),squareroot moder skew sqrtx posit skew data sqrtmaxx1 x negat skew data log boxcox greater skew log10x posit skew data log10maxx1 x negat skew datanplog10mydfcolumnnamefrom scipi import statsstatsboxcoxmydfcolumnnam invers sever skew 1x posit skew data 1maxx1 x negat skew datanpreciprocalmydfcolumnnam transform distribut becom approxim normal distribut take log zero valu column transform inf npisinfdfcolnamevaluessumthen transform need replac inf valu zerofor col xcolumn xcolreplacenpinf npinf 0 inplacetruether altern way nplog1p method return log1numb comput way accur even valu number close zero get inf valu log10 may give better predict thus log10 also need explorednplog1p27183linear heteroscedasticityfirst tri log transform situat depend variabl start increas rapid increas independ variableif data opposit – depend variabl decreas rapid increas independ variabl – first consid squareroot transform
581,"Basics of Logistic regression 

Logistic regression  is the first option for linear classification problem on small dataset (<1lakh experiences)

Logistic regression is a supervised, parametric, classification algorithm (non-linear regression technique). 

Why it is called regression?

In logistic regression, logit of event y, either continuously increases or decreases with the increase in independent variable like linear regression. Thus it is called logistic regression.

But in Logistic Regression, the continuous target values are only a limited number (generally forced to give two target values)

The logistic or sigmoid function returns a continuous value (fraction) from 0 to 1. Generally, we take a threshold such as 0.5. If the sigmoid function returns a value greater than or equal to 0.5, we take it as 1, and if the sigmoid function returns a value less than 0.5, we take it as 0.

>> Logistic regression does not need variables to be normally distributed for good performance",basic logist regress,"['basic', 'logist', 'regress']",Basics of Logistic regression ,logist regress first option linear classif problem small dataset 1lakh experienceslogist regress supervis parametr classif algorithm nonlinear regress techniqu call regressionin logist regress logit event either continu increas decreas increas independ variabl like linear regress thus call logist regressionbut logist regress continu target valu limit number general forc give two target valuesth logist sigmoid function return continu valu fraction 0 1 general take threshold 05 sigmoid function return valu greater equal 05 take 1 sigmoid function return valu less 05 take 0 logist regress need variabl normal distribut good perform
582,"Meaning of odds and logit function in probability

The odds are defined as the probability that the event will occur (P) divided by the probability that the event will not occur (1-P)

Here, we are considering the probability of dependent variable,Y as P

Odds of event = P/(1-P)

Logit of event, Y= Log-odds= Log(P/(1-P))

Y= Log(P/(1-P)) is called the Logit fuction",mean odd logit function probabl,"['mean', 'odd', 'logit', 'function', 'probabl']",Meaning of odds and logit function in probability,odd defin probabl event occur p divid probabl event occur 1phere consid probabl depend variabley podd event p1plogit event logodd logp1pi logp1p call logit fuction
583,"Logistic function or sigmoid function

In linear regression, the value of event, Y is a linear equation. 

Value of event, Y= w0+w1*x1+w2*x2+…wn*xn

In logistic regression, the logit of event, Y is a linear equation.

Logit of event, Y= w0+w1*x1+w2*x2+…wn*xn

or,
Log(P(Y)/(1-P(Y)))=w0+w1*x1+w2*x2+…wn*xn 

or,
Log(p/1-p)=W.T*x (By convention, we can assume that  x0=1)

or,
p = 1/(1+e^-W.T*x)

-This is called the logistic function or sigmoid function

The logistic or sigmoid function returns a continuous value (fraction) from 0 to 1. Generally, we take a threshold such as 0.5. If the sigmoid function returns a value greater than or equal to 0.5, we take it as 1, and if the sigmoid function returns a value less than 0.5, we take it as 0.",logist function sigmoid function,"['logist', 'function', 'sigmoid', 'function']",Logistic function or sigmoid function,linear regress valu event linear equat valu event w0w1x1w2x2…wnxnin logist regress logit event linear equationlogit event w0w1x1w2x2…wnxnor logpy1pyw0w1x1w2x2…wnxn logp1pwtx convent assum x01or p 11ewtxthi call logist function sigmoid functionth logist sigmoid function return continu valu fraction 0 1 general take threshold 05 sigmoid function return valu greater equal 05 take 1 sigmoid function return valu less 05 take 0
584,"Working of parametric model

Parametric algorithms involve two steps when ml_model_name.fit method is applied on the training data:

1. It selects a form for the function as called.

2. Learns the model parameters for the function by running a gradient descent algorithm to make the total error minimum.",work parametr model,"['work', 'parametr', 'model']",Working of parametric model,parametr algorithm involv two step mlmodelnamefit method appli train data1 select form function called2 learn model paramet function run gradient descent algorithm make total error minimum
585,"Benefits of Parametric ML models:

1. Simpler: These methods are easier to understand and interpret results.

2. Speed: Parametric models are very fast to learn from data.

3. Less Data: They do not require large training data ",benefit parametr machin learn model,"['benefit', 'parametr', 'machin', 'learn', 'model']",Benefits of Parametric ML models:,1 simpler method easier understand interpret results2 speed parametr model fast learn data3 less data requir larg train data
586,"Limitations of Parametric ML Models:

1. Constrained: By choosing a functional form these methods are highly constrained to the specified form.

2. Limited Complexity: The methods are more suited to simpler problems.

3. Poor Fit: A real data never perfectly follow a true function, but a data scientist call a ML function based on final EDA. Thus, the methods are unlikely to perfectly match the underlying  trend in the training data.

> Neural network is an expection",limit parametr machin learn model,"['limit', 'parametr', 'machin', 'learn', 'model']",Limitations of Parametric ML Models:,1 constrain choos function form method high constrain specifi form2 limit complex method suit simpler problems3 poor fit real data never perfect follow true function data scientist call machin learn function base final eda thus method unlik perfect match under trend train data neural network expect
587,"Working of Non-parametric model 

Non-parametric algorithm does the following step when ml_model_name.fit method is applied on the training data:

It selects a method as called to best fit the training data in constructing the mapping function.",work nonparametr model,"['work', 'nonparametr', 'model']",Working of Non-parametric model ,nonparametr algorithm follow step mlmodelnamefit method appli train datait select method call best fit train data construct map function
588,"Benefits of Nonparametric Machine Learning Algorithms:

1. Flexibility: Capable of fitting a large number of functional forms.

2. Power: No assumptions (or weak assumptions) about the underlying function.

3. Performance: Can result in higher performance models for prediction.",benefit nonparametr machin learn algorithm,"['benefit', 'nonparametr', 'machin', 'learn', 'algorithm']",Benefits of Nonparametric Machine Learning Algorithms:,1 flexibl capabl fit larg number function forms2 power assumpt weak assumpt under function3 perform result higher perform model predict
589,"Limitations of Nonparametric Machine Learning Algorithms:

1. More data: Require a lot more training data to estimate the mapping function.

2. Slower: A lot slower to train as they often have far more parameters to train.

3. Overfitting: More of a risk to overfit the training data",limit nonparametr machin learn algorithm,"['limit', 'nonparametr', 'machin', 'learn', 'algorithm']",Limitations of Nonparametric Machine Learning Algorithms:,1 data requir lot train data estim map function2 slower lot slower train often far paramet train3 overfit risk overfit train data
590,"Classification model and probability

A probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to.

It is possible for every probabilistic method to simply return the class with the highest probability and therefore seem deterministic.",classif model probabl,"['classif', 'model', 'probabl']",Classification model and probability,probabilist classifi classifi abl predict given observ input probabl distribut set class rather output like class observ belong toit possibl everi probabilist method simpli return class highest probabl therefor seem determinist
591,"Generative models and Discriminative models 

Generative models (learns joint probability, p(x,y) and outputs conditional probability, p(y|x))-able to predict for unseen data point:

1. Linear Regression

2. Naive Bayes

3. Hidden Markov Models

Deterministic or Discriminative models (learns conditional probability, p(y|x) and outputs conditional probability, p(y|x))-Estimate parameters directly from training data:

1. Logistic Regression

2. KNN

3. Decision Tree

4. Neural Network (can also become generative)

5. SVM (Non-Probabilistic-does not give the probability for prediction)

> Generative models are less accurate than discriminative models 

> Generative models can work with missing data, and discriminative models generally can’t.

> Compared with discriminative models, generative models need less data to train.",generat model discrimin model,"['generat', 'model', 'discrimin', 'model']",Generative models and Discriminative models ,generat model learn joint probabl pxi output condit probabl pyxabl predict unseen data point1 linear regression2 naiv bayes3 hidden markov modelsdeterminist discrimin model learn condit probabl pyx output condit probabl pyxestim paramet direct train data1 logist regression2 knn3 decis tree4 neural network also becom generative5 support vector machin nonprobabilisticdo give probabl predict generat model less accur discrimin model generat model work miss data discrimin model general can't compar discrimin model generat model need less data train
592,"Coding for Logistic regression 

from sklearn import metrics

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix

from sklearn.metrics import f1_score

from sklearn.metrics import log_loss

# log_loss is indicative of how close the prediction probability is to the corresponding actual/true value. A good model should have a smaller log_loss value (can never be negative).

# large log-likelihood statistic (parallel to F-test in OLS regression) indicates that the statistical model is a poor fit of the data.

# sklearn provides max 3 classes

# Training Logistic regression model

my_logistic_regress_mdl = LogisticRegression(fit_intercept=True, max_iter=10000)

my_logistic_regress_mdl.fit(X_train, y_train)

my_logistic_regress_mdl.coef_

my_logistic_regress_mdl.intercept_

# Evaluating the performance of Logistic regression model

train_accuracy = accuracy_score(y_pred_train,y_train)

test_accuracy = accuracy_score(y_pred_test,y_test)

my_cm = confusion_matrix(y_train, y_pred_train)

sns.heatmap(my_cm, vmin=0, cmap='Greens', annot=True)

f1_score(y_test, y_pred_test, average='binary')

> In f1_score, average can be {‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None, default=’binary’

'macro':
Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.",code logist regress,"['code', 'logist', 'regress']",Coding for Logistic regression ,sklearn import metricsfrom sklearnlinearmodel import logisticregressionfrom sklearnmetr import accuracyscor confusionmatrixfrom sklearnmetr import f1scorefrom sklearnmetr import logloss logloss indic close predict probabl correspond actualtru valu good model smaller logloss valu never negat larg loglikelihood statist parallel ftest ordinari least squar regress indic statist model poor fit data sklearn provid max 3 class train logist regress modelmylogisticregressmdl logisticregressionfitintercepttru maxiter10000mylogisticregressmdlfitxtrain ytrainmylogisticregressmdlcoefmylogisticregressmdlintercept evalu perform logist regress modeltrainaccuraci accuracyscoreypredtrainytraintestaccuraci accuracyscoreypredtestytestmycm confusionmatrixytrain ypredtrainsnsheatmapmycm vmin0 cmapgreen annottruef1scoreytest ypredtest averagebinari f1score averag micro macro sampl weight binari none default'binary'macro calcul metric label find unweight mean take label imbal account
593,"Confusion Matrix

Also known as error matrix

It is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one

In unsupervised learning it is usually called a matching matrix

> Evaluation through confusion matrix

Confusion matrix finds, in how many cases our classification model is  correct or incorrect in predicting the target class (TP,TN,FP,FN). 

Thus we can interpret the performance of the model more easily.

Many times, accuracy of the model is not enough to judge the performace, then we also need to check precision and recall

condition positive (P): the number of real positive cases in the data

condition negative (N): the number of real negative cases in the data

true positive (TP): eqv. with hit

true negative (TN): eqv. with correct rejection

false positive (FP): eqv. with false alarm, type I error or underestimation

false negative (FN): eqv. with miss, type II error or overestimation

> For regression model evaluation, there is RMSE, MSE, R2 etc.",confus matrix,"['confus', 'matrix']",Confusion Matrix,also known error matrixit specif tabl layout allow visual perform algorithm typic supervis learn onein unsupervis learn usual call match matrix evalu confus matrixconfus matrix find mani case classif model correct incorrect predict target class tptnfpfn thus interpret perform model easilymani time accuraci model enough judg performac also need check precis recallcondit posit p number real posit case datacondit negat n number real negat case datatru posit tp eqv hittru negat tn eqv correct rejectionfals posit fp eqv fals alarm type error underestimationfals negat fn eqv miss type ii error overestim regress model evalu rmse mse rsquar etc
594,"Accuracy, Precision, Recall and F1-Score

Accuracy= (TP+TN)/(TP+TN+FP+FN)

> Accuracy is the measure of total correctness of any output

Precision= TP/(TP+FP)

> Precision is a measure of correct positive with respect to total predicted positive. When FP is more vital, means need to be as low as possible, then precision is more important than recall.

For example, if a non-spam (0) mail is detected as spam (1), then it is harmful.

Recall (sensitivity or hit rate)= TP/(TP+FN)

> Recall is a measure of correct positive with respect total actual positive. When FN is more vital, means need to be as low as possible, then recall is more important than precision.

For example, during resume shortlisting, if a true candidate (1) is rejected (0), then it is harmful for the organization.

F1-Score 

This is defined as the harmonic mean of precision and recall.

F1=2*Precision*Recall/(Precision+Recall)

Example:
Say, I have a dataset with 20 cases with cancer and 80 cases without cancer. My model predicts 12 actual cancer as cancer and 4 without cancer as cancer. Then my precision is 75%. Again out of the 20 cases with cancer, my model predict 12 as cancer and 8 as non-cancer. Then my recall is 60%.",accuraci precis recal f1score,"['accuraci', 'precis', 'recal', 'f1score']","Accuracy, Precision, Recall and F1-Score",accuraci tptntptnfpfn accuraci measur total correct outputprecis tptpfp precis measur correct posit respect total predict posit fp vital mean need low possibl precis import recallfor exampl nonspam 0 mail detect spam 1 harmfulrecal sensit hit rate tptpfn recal measur correct posit respect total actual posit fn vital mean need low possibl recal import precisionfor exampl resum shortlist true candid 1 reject 0 harm organizationf1scor defin harmon mean precis recallf12precisionrecallprecisionrecallexampl say dataset 20 case cancer 80 case without cancer model predict 12 actual cancer cancer 4 without cancer cancer precis 75 20 case cancer model predict 12 cancer 8 noncanc recal 60
595,"Importance of F1 Score

> F1 score penalizes the extreme values.

> Accuracy is used when True Positives and True negatives both are important while F1-score is used when the False Negatives and False Positives are crucial

> Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.

> In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric for evaluation.

> F1 score can also be used for multiclass classification, on the other hand roc auc score typically used for a binary classification. ",import f1 score,"['import', 'f1', 'score']",Importance of F1 Score,f1 score penal extrem valu accuraci use true posit true negat import f1score use fals negat fals posit crucial accuraci use class distribut similar f1score better metric imbalanc class case reallif classif problem imbalanc class distribut exist thus f1score better metric evalu f1 score also use multiclass classif hand receiv oper characterist area curv score typic use binari classif
596,"Checking Cross-validation scores

from sklearn.linear_model import LogisticRegressionCV

from sklearn.model_selection import cross_validate

The cross_validate function differs from cross_val_score in two ways: cross_validate function allows specifying multiple metrics for evaluation. It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.",check crossvalid score,"['check', 'crossvalid', 'score']",Checking Cross-validation scores,sklearnlinearmodel import logisticregressioncvfrom sklearnmodelselect import crossvalidateth crossvalid function differ crossvalscor two way crossvalid function allow specifi multipl metric evalu return dict contain fittim scoretim option train score well fit estim addit test score
597,"List of ML models

1. For regression problems (supervised)

1A. Linear Regressors

i. Ridge,Lasso or ElasticNet (parametric)

ii. SVM (parametric)

iii. Neural Network-SGD (parametric)

1B. Non-linear Regressors

i. Ensembles of Decision Trees (non-parametric)

2. For Classification problems (supervised)

2A. Linear Classifers

i. SVM (parametric)

ii. Neural Network-SGD (parametric)

2B. Non-linear Classifiers

i. Logistic regression (parametric)-rarely used

ii. KNN (non-parametric)

iii. Naive Bayes (parametric)

iv. Ensembles of Decision trees (non-parametric)

v. Neural Network-SGD+Kernel approximation (parametric)

3. For Clustering problems (unsupervised)

i. K-Means Clustering (non-parametric)

ii. DBSCAN (non-parametric)

iii. Gaussian Mixture Model (GMM) (parametric)

iv. Hierarchical Clustering

Non-linear classifiers are the most important one among all groups of ML models. 

In real life, classifying a large experience set with large number of features is the most important one. 

And the underlying rule of this classification never follow a true function. 

Thus a non-linear classifier is employed.",list machin learn model,"['list', 'machin', 'learn', 'model']",List of ML models,1 regress problem supervised1a linear regressorsi ridgelasso elasticnet parametricii support vector machin parametriciii neural networksgd parametric1b nonlinear regressorsi ensembl decis tree nonparametric2 classif problem supervised2a linear classifersi support vector machin parametricii neural networksgd parametric2b nonlinear classifiersi logist regress parametricrar usedii knearest neighbor nonparametriciii naiv bay parametriciv ensembl decis tree nonparametricv neural networksgdkernel approxim parametric3 cluster problem unsupervisedi kmean cluster nonparametricii dbscan nonparametriciii gaussian mixtur model gmm parametriciv hierarch clusteringnonlinear classifi import one among group machin learn model real life classifi larg experi set larg number featur import one under rule classif never follow true function thus nonlinear classifi employ
598,"Important Terminology in Decision Tree ML Model

> Root node: It represents entire population or sample and this further gets divided into two sets.

> Decision node: When a sub-node splits further into sub-nodes, then it is called decision node. Decision node illustrates Test specification in a decision tree

> Leaf/Terminal node: Nodes do not split are called leaf or terminal nodes.

> Splitting: It is a process of dividing a node into two sub-nodes.

> Branch or Sub-tree: A sub section of entire tree is called branch or sub-tree.

> Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes whereas sub-nodes are the child of parent node.

> Pruning: When we remove sub-nodes of a decision node, this process is called pruning. we can say it the opposite process of splitting.",import terminolog decis tree machin learn model,"['import', 'terminolog', 'decis', 'tree', 'machin', 'learn', 'model']",Important Terminology in Decision Tree ML Model,root node repres entir popul sampl get divid two set decis node subnod split subnod call decis node decis node illustr test specif decis tree leaftermin node node split call leaf termin node split process divid node two subnod branch subtre sub section entir tree call branch subtre parent child node node divid subnod call parent node subnod wherea subnod child parent node prune remov subnod decis node process call prune say opposit process split
599,"Steps of Decision tree algorithm

1. It takes the entire dataset as root node and finds the most important feature for the decision of classification of experiences and splits the whole sample dataset into two or more datasets (sub-node or child node)

2. Then each subset of data is further splitted in two or more sebsets, on the basis of second important feature.

3. The process of splitting goes on till the algorithm reach leaf or terminal node of most similar class of experiences. 
 
4. Thus the algorithm learns a treelike sequence and criteria of decisions from the training data.

5. Then the model is fitted with the testing experience set to check the performance scores.

6. When we give one real time experience to the decision tree model, it checks the decision criterion as learned and shows the class for the experience.

Decision tree is also referred to as Recursive (recurrence or repetition) partitioning",step decis tree algorithm,"['step', 'decis', 'tree', 'algorithm']",Steps of Decision tree algorithm,1 take entir dataset root node find import featur decis classif experi split whole sampl dataset two dataset subnod child node2 subset data split two sebset basi second import feature3 process split goe till algorithm reach leaf termin node similar class experi 4 thus algorithm learn treelik sequenc criteria decis train data5 model fit test experi set check perform scores6 give one real time experi decis tree model check decis criterion learn show class experiencedecis tree also refer recurs recurr repetit partit
600,"Methods to measure the similarity of child nodes

How a decision tree classifier finds the most important feature for the decision of spliting parent node into child nodes? 

Decision tree splits the parent node on all available features and then selects the split which results in most homogeneous, pure or similar child nodes.

Gini Index, Information Gain or Entropy etc. refer to ""Purity"" of our data points

Following methods are called splitting criterion:

1. Gini Index (criterion='gini' in sklearn, it is the default criterion)

 Also called Gini coefficient or Gini ratio.

Gini-Index = 1 signifies that all the elements are randomly distributed across various classes, and Gini-Index = 0.5 denotes the elements are uniformly distributed into some classes.

> Gini index is faster than Information Gain because log is used in the calculation of Information gain.

2. Information Gain (criterion='entropy' in sklearn)

The entropy (means the degree of randomness or disorder in a system in physics) of a node decreases as we go down a decision tree. 

Information Gain= 1- Entropy

Higher information gain means more homogeneous or pure node.

For Binary classification,

Entropy of any node= -P(1)*log2(P(1))- P(0)*log2(P(0))

Decision tree selects the feature at every node which results in highest information gain in both the child nodes.

3. Gain ratio

Gain ratio= Information gain/ Intrinsic information

4. Chi Square (required to write the code)

5. Reduction in Variance (required to write the code)-used for continuous target variables that are used for regression problems",method measur similar child node,"['method', 'measur', 'similar', 'child', 'node']",Methods to measure the similarity of child nodes,decis tree classifi find import featur decis splite parent node child node decis tree split parent node avail featur select split result homogen pure similar child nodesgini index inform gain entropi etc refer puriti data pointsfollow method call split criterion1 gini index criteriongini sklearn default criterion also call gini coeffici gini ratioginiindex 1 signifi element random distribut across various class giniindex 05 denot element uniform distribut class gini index faster inform gain log use calcul inform gain2 inform gain criterionentropi sklearnth entropi mean degre random disord system physic node decreas go decis tree inform gain 1 entropyhigh inform gain mean homogen pure nodefor binari classificationentropi node p1log2p1 p0log2p0decis tree select featur everi node result highest inform gain child nodes3 gain ratiogain ratio inform gain intrins information4 chi squar requir write code5 reduct varianc requir write codeus continu target variabl use regress problem
601,"Fitting Decision tree classifier

from sklearn.tree import DecisionTreeClassifier

X = dataset[independent_variables]

y = dataset[dependent_variable]

my_classifier = DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=10, random_state=0)

my_classifier.fit(X_train, y_train)

min_sample_split is the minimum no. of sample required for a split. 

For instance, if min_sample_split = 5 and there are 7 samples in the node and say the split happens one with 1 sample and other with 6 samples

min_sample_leaf is the minimum no. of sample required to be a leaf node. 

In the above example, if min_sample_leaf = 2, then the split will not happen because here one node has 1 sample

> min_sample_leaf is positively correlated with overfitting. This is a similar effect like max_depth",fit decis tree classifi,"['fit', 'decis', 'tree', 'classifi']",Fitting Decision tree classifier,sklearntre import decisiontreeclassifierx datasetindependentvariablesi datasetdependentvariablemyclassifi decisiontreeclassifiercriterionentropi maxleafnodes10 randomstate0myclassifierfitxtrain ytrainminsamplesplit minimum sampl requir split instanc minsamplesplit 5 7 sampl node say split happen one 1 sampl 6 samplesminsampleleaf minimum sampl requir leaf node exampl minsampleleaf 2 split happen one node 1 sampl minsampleleaf posit correl overfit similar effect like maxdepth
602,"Fitting Decision tree regressor

from sklearn.tree import DecisionTreeRegressor

my_regressor = DecisionTreeRegressor(criterion='mse', max_leaf_nodes=10, random_state=0)

my_regressor.fit(X_train, y_train)

> Full form of CART  is Classification And Regression Trees",fit decis tree regressor,"['fit', 'decis', 'tree', 'regressor']",Fitting Decision tree regressor,sklearntre import decisiontreeregressormyregressor decisiontreeregressorcriterionms maxleafnodes10 randomstate0myregressorfitxtrain ytrain full form cart classif regress tree
603,"Coding for Visualizing Decision Tree

from sklearn.tree import DecisionTreeClassifier, export_graphviz

from sklearn import tree

from IPython.display import SVG

from graphviz import Source

from IPython.display import display

graph = Source(tree.export_graphviz(humidity_classifier, out_file=None, feature_names=X_train.columns, class_names=['0', '1'], filled = True))

display(SVG(graph.pipe(format='svg')))

>> Here, decision nodes are represented by square shape",code visual decis tree,"['code', 'visual', 'decis', 'tree']",Coding for Visualizing Decision Tree,sklearntre import decisiontreeclassifi exportgraphvizfrom sklearn import treefrom ipythondisplay import svgfrom graphviz import sourcefrom ipythondisplay import displaygraph sourcetreeexportgraphvizhumidityclassifi outfilenon featurenamesxtraincolumn classnames0 1 fill truedisplaysvggraphpipeformatsvg decis node repres squar shape
604,"Advantages of Decision tree

1. Decision tree output is very easy to understand

2. Many times it is used as EDA or Data mining tool, as Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables.

3. Less data cleaning required as it is not influenced by outliers and missing values to a fair degree.

4. Possible Scenarios can be added",advantag decis tree,"['advantag', 'decis', 'tree']",Advantages of Decision tree,1 decis tree output easi understand2 mani time use exploratori data analysi data mine tool decis tree one fastest way identifi signific variabl relat two variables3 less data clean requir influenc outlier miss valu fair degree4 possibl scenario ad
605,"Disadvantages of Decision tree

High chance of overfitting as the model is a true mapping of training experiences and it does not use any probability of events.

This single disadvantage can be overcome by making a decision through a group of decision trees.",disadvantag decis tree,"['disadvantag', 'decis', 'tree']",Disadvantages of Decision tree,high chanc overfit model true map train experi use probabl eventsthi singl disadvantag overcom make decis group decis tree
606,"Basics of Ensembles of decision trees 

Ensembles of decision trees is the last option for non-linear regression/ classification problems on small dataset (<1lakh experiences) when KNN or Naive Bayes are not working well",basic ensembl decis tree,"['basic', 'ensembl', 'decis', 'tree']",Basics of Ensembles of decision trees ,ensembl decis tree last option nonlinear regress classif problem small dataset 1lakh experi knearest neighbor naiv bay work well
607,"Ensemble techniques

Meaning of ensemble is group. The concept is, a group of weak learners (which are trained on less data) come together to form a strong learner.

There are three ensemble techniques:

1. Bagging

2. Boosting

3. Stacking

> Decrease the fraction of samples to build base learners will result in decrease in variance",ensembl techniqu,"['ensembl', 'techniqu']",Ensemble techniques,mean ensembl group concept group weak learner train less data come togeth form strong learnerther three ensembl techniques1 bagging2 boosting3 stack decreas fraction sampl build base learner result decreas varianc
608,"Bagging technique

It reduces the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same dataset.

This is similar to asking same question  to different persons and averaging the answers for the final answer.

Random Forest is an ensemble decision tree algorithm which uses bagging (bootstrap aggregating) technique to reduce the chance of overfitting. 

Bootstrapping is a statistical resampling technique that involves random sampling of a dataset with replacement. Individual tree is built on these random subset of the features and subset of observations Thus the word random comes.

The process of splitting of parent node to child nodes till the leaf nodes for individual tree is same as decision tree.

Then, for each observation in the dataset, count the number of trees that it is classified in one category over the number of trees. 

Finally assign each observation to a final category by a majority vote over the set of trees. 

In sklearn, there are also simple BaggingRegressor() or BaggingClassifier()

> It perform similar operations as dropout in a neural network",bag techniqu,"['bag', 'techniqu']",Bagging technique,reduc varianc predict combin result multipl classifi model differ subsampl datasetthi similar ask question differ person averag answer final answerrandom forest ensembl decis tree algorithm use bag bootstrap aggreg techniqu reduc chanc overfit bootstrap statist resampl techniqu involv random sampl dataset replac individu tree built random subset featur subset observ thus word random comesth process split parent node child node till leaf node individu tree decis treethen observ dataset count number tree classifi one categori number tree final assign observ final categori major vote set tree sklearn also simpl baggingregressor baggingclassifi perform similar oper dropout neural network
609,"Boosting technique

In Random Forest, trees are side by side means parallel, whereas in Boosting, multiple trees are in sequence.

Boosting fit a sequence of weak learners (with sub-samples). More weight is given to examples that were misclassified by earlier rounds.

There are many boosting algorithms which impart additional boost to model’s accuracy:

1. AdaBoost (foundation of boosting algorithm)

2. Gradient Boosting (uses gradient descent method to minimize loss)

3.  XGBoost (imporve on overfitting, optimization of running speed through parallel processing)

4. LightGBM (further improve on running speed using leaf wise tree growth. It chooses the leaf with max delta loss to grow.)

5. CatBoost (handles categorical features automatically)

from xgboost import XGBRegressor

my_model=XGBRegressor(objective= , booster= , learning_rate= , max_depth, min_child_weight=,
                               , colsample_bytree=,n_estimators=, reg_alpha=, reg_lambda=, gamma=)

from lightgbm import LGBMRegressor

my_model=LGBMRegressor()

from catboost import CatBoostRegressor

my_model = CatBoostRegressor(iterations=2000, learning_rate=0.05, depth=5) 

# For all the models, hyperparameters shall be tuned with cross validation ",boost techniqu,"['boost', 'techniqu']",Boosting technique,random forest tree side side mean parallel wherea boost multipl tree sequenceboost fit sequenc weak learner subsampl weight given exampl misclassifi earlier roundsther mani boost algorithm impart addit boost model accuracy1 adaboost foundat boost algorithm2 gradient boost use gradient descent method minim loss3 extrem gradient boost imporv overfit optim run speed parallel processing4 lightgbm improv run speed use leaf wise tree growth choos leaf max delta loss grow5 catboost handl categor featur automaticallyfrom extrem gradient boost import xgbregressormymodelxgbregressorobject booster learningr maxdepth minchildweight colsamplebytreenestim regalpha reglambda gammafrom lightgbm import lgbmregressormymodellgbmregressorfrom catboost import catboostregressormymodel catboostregressoriterations2000 learningrate005 depth5 model hyperparamet shall tune cross valid
610,"Extreme Gradient Boosting 

Extreme Gradient Boosting (XGBoost) is just an extension of gradient boosting. Advantages are:

1. Regularization,

2. Parallel Processing

3. High Flexibility

4. Handling Missing Values

5. Tree Pruning

6. Built-in Cross-Validation

7. Continue on Existing Model",extrem gradient boost,"['extrem', 'gradient', 'boost']",Extreme Gradient Boosting ,extrem gradient boost xgboost extens gradient boost advantag are1 regularization2 parallel processing3 high flexibility4 handl miss values5 tree pruning6 builtin crossvalidation7 continu exist model
611,"Stacking technique

In stacking, trees are being arranged in two levels: 

Level 0 (Base models): These trees are sequential like boosting, but all trees are trained on same numbers of experiences like original dataset.

Level 1 (Meta model): This tree learns how to best combine the predictions of the base models. This tree is absent in boosting technique.

> Stacking is best in case of limited training data, because each classifier is trained on all of the available data",stack techniqu,"['stack', 'techniqu']",Stacking technique,stack tree arrang two level level 0 base model tree sequenti like boost tree train number experi like origin datasetlevel 1 meta model tree learn best combin predict base model tree absent boost techniqu stack best case limit train data classifi train avail data
612,"Python coding for Ensembles of decision trees

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100)

from sklearn.ensemble import GradientBoostingClassifier

from xgboost import XGBRegressor

model = XGBRegressor(objective='reg:squarederror')

>> Extra Trees (Extremely Randomized Trees) and Random Forest function does not use learning_rate hyperparameter

>> Increasing the value of max_depth may overfit the data

from sklearn.ensemble import StackingRegressor

estimators = [('lr', RidgeCV()), ('svr', LinearSVR(random_state=42))

reg = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor(n_estimators=10, random_state=42))",python code ensembl decis tree,"['python', 'code', 'ensembl', 'decis', 'tree']",Python coding for Ensembles of decision trees,sklearnensembl import randomforestclassifierrf randomforestclassifiernestimators100from sklearnensembl import gradientboostingclassifierfrom extrem gradient boost import xgbregressormodel xgbregressorobjectiveregsquarederror extra tree extrem random tree random forest function use learningr hyperparamet increas valu maxdepth may overfit datafrom sklearnensembl import stackingregressorestim lr ridgecv svr linearsvrrandomstate42reg stackingregressorestimatorsestim finalestimatorrandomforestregressornestimators10 randomstate42
613,"Finding feature importance

from sklearn.ensemble import ExtraTreesRegressor

my_model=ExtraTreesRegressor()

my_model.fit(X_train,y_train)

my_model.feature_importances_

> It returns feature importance scores",find featur import,"['find', 'featur', 'import']",Finding feature importance,sklearnensembl import extratreesregressormymodelextratreesregressormymodelfitxtrainytrainmymodelfeatureimport return featur import score
614,"Classification report

It is very useful for multiclass classification problem

from sklearn.metrics import classification_report

clf_rpt = classification_report(Y_validation,y_pred)

print(""classification report :"", clf_rpt)

> It gives precision, recall, f1-score etc.
",classif report,"['classif', 'report']",Classification report,use multiclass classif problemfrom sklearnmetr import classificationreportclfrpt classificationreportyvalidationypredprintclassif report clfrpt give precis recal f1score etc
615,"Ways to improve random forest accuracy

Algorithm Tuning

Add more data

Feature Selection",way improv random forest accuraci,"['way', 'improv', 'random', 'forest', 'accuraci']",Ways to improve random forest accuracy,algorithm tuningadd datafeatur select
616,"Basics of Model Explainability

Explainability in machine learning means that we can explain what happens in our model from input to output. It makes models transparent and solves the black box problem. Explainable AI (XAI) is the more formal way to describe this and applies to all artificial intelligence.",basic model explain,"['basic', 'model', 'explain']",Basics of Model Explainability,explartifici intelligencen machin learn mean explartifici intelligencen happen model input output make model transpar solv black box problem explartifici intelligencen artifici intellig xartifici intellig formal way describ appli artifici intellig
617,"Black Box Model vs. White Box Model

Black-box models, such as deep-learning (deep neural network), boosting, and random forest models, are highly non-linear by nature and are harder to explain in general. With black-box models, users can only observe the input-output relationship.

White-box models are the type of models in which one can clearly explain how they behave, how they produce predictions, and what the influencing variables are.",black box model vs white box model,"['black', 'box', 'model', 'vs', 'white', 'box', 'model']",Black Box Model vs. White Box Model,blackbox model deeplearn deep neural network boost random forest model high nonlinear natur harder explain general blackbox model user observ inputoutput relationshipwhitebox model type model one clear explain behav produc predict influenc variabl
618,"Explainable AI

Explainable AI is about understanding ML models better. How they make decisions, and why. The three most important aspects of model explainability are:

1. Transparency

2. Ability to question

3. Ease of understanding",explartifici intelligencen artifici intellig,"['explartifici', 'intelligencen', 'artifici', 'intellig']",Explainable AI,explartifici intelligencen artifici intellig understand machin learn model better make decis three import aspect model explartifici intelligencen are1 transparency2 abil question3 eas understand
619,"Importance of explainability

Explainability connects the data science team and non-technical team, improving knowledge exchange, and giving all stakeholders a better understanding of product requirements and limitations.

Reasons:

1. Accountability

2. Trust

3. Compliance

4. Performance

5. Enhanced control",import explain,"['import', 'explain']",Importance of explainability,explain connect data scienc team nontechn team improv knowledg exchang give stakehold better understand product requir limitationsreasons1 accountability2 trust3 compliance4 performance5 enhanc control
620,"A. Scope of explainability

1. Global : This is the overall explanation of model behavior. It shows us a big picture view of the model, and how features in the data collectively affect the result.

2. Local : This tells us about each instance and feature in the data individually (kind of like explaining observations seen at certain points in the model), and how features individually affect the result.",scope explain,"['scope', 'explain']",A. Scope of explainability,1 global overal explan model behavior show us big pictur view model featur data collect affect result2 local tell us instanc featur data individu kind like explain observ seen certain point model featur individu affect result
621,"B. Approach of explainability

i) Model-Agnostic Approach: It works across all types of models

ii) Model-Specific Approach:  It is tailor-made for a particular class of algorithms

Explainable models (white box models) are:

1. Linear models

2. Decision Tree Algorithms

3. Generalized Additive Models (GAM)-it is like linear model, but X is another polynomial function. Example is Splines
",b approach explain,"['b', 'approach', 'explain']",B. Approach of explainability,modelagnost approach work across type modelsii modelspecif approach tailormad particular class algorithmsexplain model white box model are1 linear models2 decis tree algorithms3 general addit model gamit like linear model x anoth polynomi function exampl spline
622,"Techniques or Libraries for Explainability in ML (mainly for black box models)

> LIME

> SHAP

> ELI5",techniqu librari explain machin learn main black box model,"['techniqu', 'librari', 'explain', 'machin', 'learn', 'main', 'black', 'box', 'model']",Techniques or Libraries for Explainability in ML (mainly for black box models),lime shap explain like 5
623,"Local Interpretable Model-Agnostic Explanations (LIME)

It can give us the top most feature weights which are influencing (positively and negatively) the output

!pip install lime

import lime

import lime.lime_tabular

from __future__ import print_function

> This technique lie under model-agnostic approach and scope is local",local interpret modelagnost explan lime,"['local', 'interpret', 'modelagnost', 'explan', 'lime']",Local Interpretable Model-Agnostic Explanations (LIME),give us top featur weight influenc posit negat outputpip instal limeimport limeimport limelimetabularfrom futur import printfunct techniqu lie modelagnost approach scope local
624,"Shapley Additive Explanations (SHAP)

The concept is a mathematical solution for a game theory problem – how to share a reward among team members in a cooperative game?

1. TreeExplainer - for the analysis of decision trees

2. DeepExplainer - for the deep learning algorithms (less advance)

3. KernelExplainer - for most of the algorithms

> Shapely means having an attractive or well-proportioned shape (especially of a woman or part of her body).",shapley addit explan shap,"['shapley', 'addit', 'explan', 'shap']",Shapley Additive Explanations (SHAP),concept mathemat solut game theori problem – share reward among team member cooper game1 treeexplain analysi decis trees2 deepexplain deep learn algorithm less advance3 kernelexplain algorithm shape mean attract wellproport shape especi woman part bodi
625,"Implementing SHAP

!pip install shap

import shap

shap.initjs()

shap_values = shap.TreeExplainer(model_rf_final).shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=list(X.columns), plot_type=""bar"")

shap_values = shap.KernelExplainer(model_svr_final.predict, X_train[100:150], link='identity').shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=list(X.columns), plot_type=""bar"")

There are currently four types of Summary Plots in SHAP: dot, bar, violin, and compact dot

plot_type=""dot""

plot_type=""bar""

plot_type=""violin""

plot_type=""compact dot""

Other plots in SHAP

shap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[0])

for elem in top_features:
  shap.dependence_plot(elem, shap_values, X_train)",implement shapley addit explan,"['implement', 'shapley', 'addit', 'explan']",Implementing SHAP,pip instal shapimport shapshapinitjsshapvalu shaptreeexplainermodelrffinalshapvaluesxtest shapsummaryplotshapvalu xtest featurenameslistxcolumn plottypebarshapvalu shapkernelexplainermodelsvrfinalpredict xtrain100150 linkidentityshapvaluesxtest shapsummaryplotshapvalu xtest featurenameslistxcolumn plottypebarther current four type summari plot shap dot bar violin compact dotplottypedotplottypebarplottypeviolinplottypecompact dototh plot shapshapforceplotexplainerexpectedvalue0 shapvalues0 xtestiloc0for elem topfeatur shapdependenceplotelem shapvalu xtrain
626,"Explain Like I'm 5 (ELI5)

It is an unified API and very straight forward in giving the explanation.

!pip install eli5

import eli5 as eli

eli.explain_weights(model_rf_final,feature_names=list(X.columns))",explain like im 5 eli5,"['explain', 'like', 'im', '5', 'eli5']",Explain Like I'm 5 (ELI5),unifi applic program interfac straight forward give explanationpip instal explain like 5import explain like 5 elieliexplainweightsmodelrffinalfeaturenameslistxcolumn
627,"Other techniques for Explainability in ML

1. Partial Dependence Plots (PDP)

2. Individual Condition Expectations plots (ICE)

3. Leave One Column Out (LOCO)

4. Accumulated Local Effects (ALE)

5. Yellowbrick

6. Lucid

7. Anchors

8. Deep Learning Important Features (DeepLIFT)

9. Layer-wise relevance propagation (LRP)

10. Contrastive Explanations Method (CEM)

11. ProfWeight etc.",techniqu explain machin learn,"['techniqu', 'explain', 'machin', 'learn']",Other techniques for Explainability in ML,1 partial depend plot pdp2 individu condit expect plot ice3 leav one column loco4 accumul local effect ale5 yellowbrick6 lucid7 anchors8 deep learn import featur deeplift9 layerwis relev propag lrp10 contrast explan method cem11 profweight etc
628,"Basics of KNN

kNN is the first option for non linear classification problems on small non-text dataset (<1lakh experiences)

May be applied for regression problems. Then it takes the mean value of k closest points.

> k-Nearest Neighbors is one of the simplest supervised learning algorithms and it is robust to the noisy training data

> It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification,  it classifies that data into a category that is much similar to the new data",basic knearest neighbor,"['basic', 'knearest', 'neighbor']",Basics of KNN,knearest neighbor first option non linear classif problem small nontext dataset 1lakh experiencesmay appli regress problem take mean valu k closest point knearest neighbor one simplest supervis learn algorithm robust noisi train data also call lazi learner algorithm learn train set immedi instead store dataset time classif classifi data categori much similar new data
629,"Euclidean Distance

The Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance.

Euclidean distance between two 'n' dimensional experiences or data points=sqrt((X11-X21)^2+(X12-X22)^2+...(X1n-X2n)^2)

Euclidean distance between two n-dimensional experiences treats each feature or dimension as equally important

> Manhattan distance can be used for continuous variables

> Other distance metrics for kNN

Tanimoto

Jaccard",euclidean distanc,"['euclidean', 'distanc']",Euclidean Distance,euclidean distanc two point euclidean space length line segment two point calcul cartesian coordin point use pythagorean theorem therefor occasion call pythagorean distanceeuclidean distanc two n dimension experi data pointssqrtx11x212x12x222x1nx2n2euclidean distanc two ndimension experi treat featur dimens equal import manhattan distanc use continu variabl distanc metric knntanimotojaccard
630,"Working of kNN

Step-1: Select the number K of the neighbors

Step-2: Calculate the Euclidean distances among all data points

Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.

Step-4: Among these k neighbors, count the number of the data points in each category.

Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.",work knearest neighbor,"['work', 'knearest', 'neighbor']",Working of kNN,step1 select number k neighborsstep2 calcul euclidean distanc among data pointsstep3 take k nearest neighbor per calcul euclidean distancestep4 among k neighbor count number data point categorystep5 assign new data point categori number neighbor maximum
631,"Ways to select the value of k in the kNN Algorithm

K represents the number of nearest neighbours we want to select to predict the class of a data point.

There is no particular way to determine the best value for ""k"", so we need to try some values to find the best out of them. 

The odd value of “K” is preferred over even values in order to ensure that there are no ties in the voting. 

The most preferred value for k is 5. 

Best value of k may be found out for best accuracy  score with the help of cross validation

> The boundary becomes smoother and the bias increases (simplification of the model) with increasing value of K

> The smaller value of K causes noise to have a higher influence on the result which will also lead to a large variation in the predictions.

> If K is too large, then our model is under-fitted. As a result, the error will go up again. The computational expense of the algorithm also increases if we choose the k very large.

> So, there is a tradeoff between overfitting and underfitting and we have to maintain a balance while choosing the value of K in KNN. Therefore, K should not be too small or too large.",way select valu k knearest neighbor algorithm,"['way', 'select', 'valu', 'k', 'knearest', 'neighbor', 'algorithm']",Ways to select the value of k in the kNN Algorithm,k repres number nearest neighbour want select predict class data pointther particular way determin best valu k need tri valu find best odd valu “k” prefer even valu order ensur tie vote prefer valu k 5 best valu k may found best accuraci score help cross valid boundari becom smoother bias increas simplif model increas valu k smaller valu k caus nois higher influenc result also lead larg variat predict k larg model underfit result error go comput expens algorithm also increas choos k larg tradeoff overfit underfit maintain balanc choos valu k knn therefor k small larg
632,"Disadvantages of kNN Algorithm:

> Always needs to determine the value of K which may be complex some time.

> Computationally expensive both in terms of storage & time because of storing the training data and calculating the distance between the data points for all the training samples during prediction.

>  kNN is very likely to overfit due to the curse of dimensionality (dimensionality reduction and feature selection shall be used beforing fitting kNN model)

> Accuracy largely depends on the quality of the data.

> kNN algorithm is found to be adversely affected by the presence of outliers in the datasets.",disadvantag knearest neighbor algorithm,"['disadvantag', 'knearest', 'neighbor', 'algorithm']",Disadvantages of kNN Algorithm:,alway need determin valu k may complex time comput expens term storag time store train data calcul distanc data point train sampl predict knearest neighbor like overfit due curs dimension dimension reduct featur select shall use befor fit knearest neighbor model accuraci larg depend qualiti data knearest neighbor algorithm found advers affect presenc outlier dataset
633,"Python coding for KNN

from sklearn.neighbors import KNeighborsClassifier

my_knn = KNeighborsClassifier(n_neighbors=5)

my_knn.fit(X_train,y_train)

my_knn.score(X_test,y_test)

from sklearn.metrics import confusion_matrix,roc_curve

from sklearn.metrics import roc_auc_score

roc_auc_score(y_test,test_preds)

> leaf_size is another important hyperparameter for KNN model",python code knearest neighbor,"['python', 'code', 'knearest', 'neighbor']",Python coding for KNN,sklearnneighbor import kneighborsclassifiermyknearest neighbor kneighborsclassifiernneighbors5myknearest neighborsfitxtrainytrainmyknearest neighborsscorextestytestfrom sklearnmetr import confusionmatrixroccurvefrom sklearnmetr import rocaucscorerocaucscoreytesttestpr leafsiz anoth import hyperparamet knearest neighbor model
634,"Receiver operating characteristic and AUC

The ROC is also known as a relative operating characteristic, because it is a comparison of two operating characteristics (TPR or sensitivity and FPR or specificity).

> Sensitivity: the ability of a test to correctly identify patients with a disease.

> Specificity: the ability of a test to correctly identify people without the disease.

> Recall and True Positive Rate (TPR) are exactly the same.

TPR=TP/(TP+FN)

FPR=FP/(FP+TN)

> Precision is more focused in the positive class than in the negative class, it actually measures the probability of correct detection of positive values, while FPR and TPR (ROC metrics) measure the ability to distinguish between the classes.

AUC means area under the curve. So to get ROC-AUC score we need to define ROC curve first.

AUC is also known as AOC (Area under operating characteristic curve)

The AUC represents a model’s ability to discriminate between positive and negative classes. When AUC = 1, then the classifier is able to perfectly distinguish between all the positive and the negative classes correctly. 

AUC =1 means FPR =0 and TPR =1. Again, FPR =0 means FP =0 and TPR =1 means FN =0. Thus our model is clearly identifying TP and TN classes.

AUC =0, means FRP=1, TPR=0, the classifier would be predicting all Negatives as Positives, and all Positives as Negatives.

An area of 0.5 represents a model as good as random. (Here FPR =1 and TPR =1)

> Thus, a good model should have roc_auc_score > 0.5

Example:
Say, I have a dataset with 20 cases with cancer and 80 cases without cancer. Out of 80 cases without cancer, my model predicts 4 as cancer and 76 as non-cancer. Then my FPR is 5%. Again out of the 20 cases with cancer, my model predict 12 as cancer and 8 as non-cancer. Then my TPR is 60%. ",receiv oper characterist area curv,"['receiv', 'oper', 'characterist', 'area', 'curv']",Receiver operating characteristic and AUC,receiv oper characterist also known relat oper characterist comparison two oper characterist tpr sensit fpr specif sensit abil test correct identifi patient diseas specif abil test correct identifi peopl without diseas recal true posit rate tpr exact sametprtptpfnfprfpfptn precis focus posit class negat class actual measur probabl correct detect posit valu fpr tpr receiv oper characterist metric measur abil distinguish classesarea curv mean area curv get receiv oper characteristicarea curv score need defin receiv oper characterist curv firstarea curv also known aoc area oper characterist curveth area curv repres model abil discrimin posit negat class area curv 1 classifi abl perfect distinguish posit negat class correct area curv 1 mean fpr 0 tpr 1 fpr 0 mean fp 0 tpr 1 mean fn 0 thus model clear identifi tp tn classesarea curv 0 mean frp1 tpr0 classifi would predict negat posit posit negativesan area 05 repres model good random fpr 1 tpr 1 thus good model receiv oper characteristicarea curvescor 05exampl say dataset 20 case cancer 80 case without cancer 80 case without cancer model predict 4 cancer 76 noncanc fpr 5 20 case cancer model predict 12 cancer 8 noncanc tpr 60
635,"Knn for recommender system

from scipy.sparse import csr_matrix

my_df_matrix = csr_matrix(X.values)

from sklearn.neighbors import NearestNeighbors

model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')

model_knn.fit(my_df_matrix)

query_index = np.random.choice(X.shape[0])

distances, indices = model_knn.kneighbors(np.array(X.iloc[query_index,:].values).reshape(1, -1), n_neighbors = 6)",knearest neighbor recommend system,"['knearest', 'neighbor', 'recommend', 'system']",Knn for recommender system,scipyspars import csrmatrixmydfmatrix csrmatrixxvaluesfrom sklearnneighbor import nearestneighborsmodelknn nearestneighborsmetr cosin algorithm brutemodelknnfitmydfmatrixqueryindex nprandomchoicexshape0dist indic modelknnkneighborsnparrayxilocqueryindexvaluesreshape1 1 nneighbor 6
636,"Sparse matrix and Dense matrix

> We convert  normal 2D Array with mostly zeros to csr_matrix (sparse matrix)

> CSR stands for compressed sparse row.

> Sparse matrices are memory efficient data structures that enable us to store large matrices with very few non-zero elements. It does not store the zero values.

> We can not view a sparse matrix type.

> Dense matrix are normal matrix which stores all the zero and non-zero values

> For converting sparse matrix  to dense matrix 

sparse_matrix.todense()

> NearestNeighbors() model can take X in the form of dense matrix but using sparse matrix  makes it more efficient in computation",spars matrix dens matrix,"['spars', 'matrix', 'dens', 'matrix']",Sparse matrix and Dense matrix,convert normal 2d array most zero csrmatrix spars matrix csr stand compress spars row spars matric memori effici data structur enabl us store larg matric nonzero element store zero valu view spars matrix type dens matrix normal matrix store zero nonzero valu convert spars matrix dens matrix sparsematrixtodens nearestneighbor model take x form dens matrix use spars matrix make effici comput
637,"Basics of Naive Bayes

Naive Bayes is the first option for non linear classification problems on small text dataset (<1lakh experiences)

> Naive means showing a lack of experience, wisdom, or judgement. 

This algorithm is called Naive because, here the most important assumption is attributes are statistically independent of one another. This is a strong assumption and unrealistic for real data; however, the technique is very effective on a large range of complex problems.

> The Naive Bayesian classifier is based on Bayes' theorem

Perhaps the easiest Naive Bayes classifier to understand is Gaussian Naive Bayes. 

In this classifier, the assumption is that data for each label is drawn from a simple Gaussian distribution.",basic naiv bay,"['basic', 'naiv', 'bay']",Basics of Naive Bayes,naiv bay first option non linear classif problem small text dataset 1lakh experi naiv mean show lack experi wisdom judgement algorithm call naiv import assumpt attribut statist independ one anoth strong assumpt unrealist real data howev techniqu effect larg rang complex problem naiv bayesian classifi base bay theoremperhap easiest naiv bay classifi understand gaussian naiv bay classifi assumpt data label drawn simpl gaussian distribut
638,"Bayes theorem

Bayes' theorem provides a way to revise existing predictions or theories (update probabilities) given new or additional evidence.

That means, we calculate posterior probability with the help of prior probability and additional evidence event A.

P(C / X)= P(X / C) *P(C) / P(X)

Here, 
P(C / X) or P(yes/sunny)= posterior probability,

P(X / C) or P(sunny/yes) = likelihood

P(C) or P(yes)=class prior probability

P(X) or P(sunny)=predictor prior probability

> Baye's theorem or rule can be used for answering probabilistic query",bay theorem,"['bay', 'theorem']",Bayes theorem,bay theorem provid way revis exist predict theori updat probabl given new addit evidencethat mean calcul posterior probabl help prior probabl addit evid event apc x px c pc pxhere pc x pyessunni posterior probabilitypx c psunnyy likelihoodpc pyesclass prior probabilitypx psunnypredictor prior probabl bay theorem rule use answer probabilist queri
639,"Understanding of Naive Bayes

Naive Bayes is a generative model and is very fast and have very few tunable parameters

Typical applications of Naive Bayes are classification of documents, filtering spam, prediction and so on.

For Naive Bayes model

> Attributes are equally important.

> Attributes are statistically independent of one another

> Attributes can be nominal or numeric. No need to convert categorical column to numerical column. 

> When an attribute value in the testing record has no example in the training set, then the entire posterior probability will be zero.

> Steps in Naive Bayes Classifier:

1. Converting the given dataset into frequency table.

2. Generating likelihood table by finding the probabilities of given features.

3. Finally, using Bayes theorem to calculate the posterior probability.",understand naiv bay,"['understand', 'naiv', 'bay']",Understanding of Naive Bayes,naiv bay generat model fast tunabl parameterstyp applic naiv bay classif document filter spam predict onfor naiv bay model attribut equal import attribut statist independ one anoth attribut nomin numer need convert categor column numer column attribut valu test record exampl train set entir posterior probabl zero step naiv bay classifier1 convert given dataset frequenc table2 generat likelihood tabl find probabl given features3 final use bay theorem calcul posterior probabl
640,"Text Pre-processing

> The classification algorithms need some sort of numerical feature vector.

> For the conversion of text to numerical value, the simplest method is the bag-of-words approach (CountVectorizer is also called bag-of-words), where each unique word in a text will be represented by one number.

Steps of Text Pre-processing (used in  Topic modeling)

1. Importing necessary libraries

import nltk

nltk.download('stopwords')

import string

from nltk.corpus import stopwords

2. Writing functions which removes punctuation and stopwords from our data

def text_process(text):
    nopunc =[char for char in text if char not in string.punctuation]
    nopunc=''.join(nopunc)
    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])

3. Tokenization: It is just the term used to describe the process of converting the normal text strings in to a list of tokens. Tokens are also known as lemmas

message['tokenized_message'] = message['message'].apply(text_process)

4. Vectorization in three steps

4.1. Count how many times does a word occur in each message (Known as term frequency,TF)

from sklearn.feature_extraction.text import CountVectorizer

del vectorizer
(del keyword in python is primarily used to delete objects in Python)

vectorizer = CountVectorizer(max_df = 0.9,min_df = 10)
vectorizer_matrix = vectorizer.fit_transform(message['tokenized_message'])

4.2. Weigh the counts, so that frequent tokens get lower weight

4.3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)",text preprocess,"['text', 'preprocess']",Text Pre-processing,classif algorithm need sort numer featur vector convers text numer valu simplest method bagofword approach countvector also call bagofword uniqu word text repres one numberstep text preprocess use topic modeling1 import necessari librariesimport nltknltkdownloadstopwordsimport stringfrom nltkcorpus import stopwords2 write function remov punctuat stopword datadef textprocesstext nopunc char char text char stringpunctu nopuncjoinnopunc return joinword word nopuncsplit wordlow stopwordswordsenglish3 token term use describ process convert normal text string list token token also known lemmasmessagetokenizedmessag messagemessageapplytextprocess4 vector three steps41 count mani time word occur messag known term frequencytffrom sklearnfeatureextractiontext import countvectorizerdel vector del keyword python primarili use delet object pythonvector countvectorizermaxdf 09mindf 10 vectorizermatrix vectorizerfittransformmessagetokenizedmessage42 weigh count frequent token get lower weight43 normal vector unit length abstract origin text length l2 norm
641,"One hot encoding

One hot encoding is word representation without any context. It only tells about its presence in the vocabulary. It has nothing to do with the word meaning

Thus, by one hot encoding we are representing a word as point (or a line joining the origin and that point-means vector) in the vector space of vocabulary

One hot encoding is a process by which categorical variables (tokenized message) are converted into a form (matrix with zero/one value) that could be provided to ML algorithms to do a better job in prediction.

In one hot encoding there will be huge number of features (unigram words). So, it has a curse of dimensionality. It also can not count term frequency.

> Solution is sklearn Bag of Words models like CountVectorizer or TfidfVectorizer",one hot encod,"['one', 'hot', 'encod']",One hot encoding,one hot encod word represent without context tell presenc vocabulari noth word meaningthus one hot encod repres word point line join origin pointmean vector vector space vocabularyon hot encod process categor variabl token messag convert form matrix zeroon valu could provid machin learn algorithm better job predictionin one hot encod huge number featur unigram word curs dimension also count term frequenc solut sklearn bag word model like countvector tfidfvector
642,"Python coding for Naive Bayes

from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import classification_report,confusion_matrix

X=vectorizer_matrix.todense()

y=my_df['label']

> Then split for X_train, X_test, y_train, y_test

classification_model = GaussianNB().fit(X_train,y_train)

Evaluating performance

test_preds = spam_detect_model.predict(X_test)

test_preds[:2]

y_test[:2]

print(confusion_matrix(y_test,test_preds))

print(classification_report(y_test,test_preds))

vectorizer_matrix or tfidf_matrix are sparse matrix. But Naive Bayes model needs dense matrix",python code naiv bay,"['python', 'code', 'naiv', 'bay']",Python coding for Naive Bayes,sklearnnaivebay import gaussiannbfrom sklearnmetr import classificationreportconfusionmatrixxvectorizermatrixtodenseymydflabel split xtrain xtest ytrain ytestclassificationmodel gaussiannbfitxtrainytrainevalu performancetestpr spamdetectmodelpredictxtesttestpreds2ytest2printconfusionmatrixytesttestpredsprintclassificationreportytesttestpredsvectorizermatrix tfidfmatrix spars matrix naiv bay model need dens matrix
643,"Generalization Error

Generalization Error is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.",general error,"['general', 'error']",Generalization Error,general error measur accur algorithm abl predict outcom valu previous unseen data
644,"Basics of SVM

Support vector is the last option for linear regression/ classification problems on small dataset (<1lakh experiences) when regularized linear regressions is not working well

The intuition of SVM is rather than simply drawing a zero-width line between the classes, we can draw around each line a margin of some width, up to the nearest data points which are called support vectors.

> SVMs can also efficiently perform as non-linear classification model using the kernel trick. 

> SVM is also used for Outlier Detection and image classification Purposes on small dataset

SVM will be effective when:

1. The data is linearly separable

2. The data is clean and ready to use

3. Proper Kernel is selected

4. Proper Kernel Parameters are selected",basic support vector machin,"['basic', 'support', 'vector', 'machin']",Basics of SVM,support vector last option linear regress classif problem small dataset 1lakh experi regular linear regress work wellth intuit support vector machin rather simpli draw zerowidth line class draw around line margin width nearest data point call support vector support vector machin also effici perform nonlinear classif model use kernel trick support vector machin also use outlier detect imag classif purpos small datasetsupport vector machin effect when1 data linear separable2 data clean readi use3 proper kernel selected4 proper kernel paramet select
645,"Kernel function 

Apart from SVM, other kernelizable algorithms are linear regression, PCA, K-means, neural network etc.

The meaning of Kernel is the edible part of a nut, seed, or fruit.

> Thus kernel function of a model softens the decision boundary to fit for non-linear classification

> Kernel function map low dimensional data to high dimensional space. This means when a hyperplane is not able to separate two non-linearly separate classes in two dimensional space, it may happen that if the data points are plotted in a three dimensional space, they can be separated by a hyperplane.

> Kernel is a similarity function",kernel function,"['kernel', 'function']",Kernel function ,apart svm kerneliz algorithm linear regress pca kmean neural network etcth mean kernel edibl part nut seed fruit thus kernel function model soften decis boundari fit nonlinear classif kernel function map low dimension data high dimension space mean hyperplan abl separ two nonlinear separ class two dimension space may happen data point plot three dimension space separ hyperplan kernel similar function
646,"Hinge loss function, hypersurface and hyperplane

> SVM find a hyperplane to linearly separate the data into two sets, one for each class, by using the hinge loss function

> Margin lines are the decision boundaries called positive and negative hyperplane which passes through the support vectors.

> A general hypersurface in a small dimensional space is turned into a hyperplane in a space with much larger dimensions.",hing loss function hypersurfac hyperplan,"['hing', 'loss', 'function', 'hypersurfac', 'hyperplan']","Hinge loss function, hypersurface and hyperplane",support vector machin find hyperplan linear separ data two set one class use hing loss function margin line decis boundari call posit negat hyperplan pass support vector general hypersurfac small dimension space turn hyperplan space much larger dimens
647,"Python coding for SVM

Without Karnel (for linear or binary classification)

from sklearn.svm import SVC 

my_model = SVC(gamma=0.23, C=3.20)

my_model.fit(X_train, y_train)

To generate blobs data points with a Gaussian distribution,

from sklearn.datasets.samples_generator import make_blobs

X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)

To generate circular data points,

from sklearn.datasets.samples_generator import make_circles

X, y = make_circles(100, factor=.1, noise=.1)

Kernel SVM

from sklearn.svm import SVC 

my_kernel_model = SVC(kernel='rbf', gamma=0.23, C=3.20)

my_kernel_model.fit(X, y)

> Different kernel functions are linear, nonlinear, polynomial (poly), radial basis function (rbf-it uses Gaussian transformation), and sigmoid.

> 'gamma' is the kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Gamma decides that how much curvature we want in a decision boundary. For high gamma value the model becomes complex and would consider only the points close to the hyperplane for modeling",python code support vector machin,"['python', 'code', 'support', 'vector', 'machin']",Python coding for SVM,without karnel linear binari classificationfrom sklearnsvm import svc mymodel svcgamma023 c320mymodelfitxtrain ytrainto generat blob data point gaussian distributionfrom sklearndatasetssamplesgener import makeblobsx makeblobsnsamples50 centers2 randomstate0 clusterstd060to generat circular data pointsfrom sklearndatasetssamplesgener import makecirclesx makecircles100 factor1 noise1kernel svmfrom sklearnsvm import svc mykernelmodel svckernelrbf gamma023 c320mykernelmodelfitx differ kernel function linear nonlinear polynomi poli radial basi function rbfit use gaussian transform sigmoid gamma kernel coeffici rbf poli sigmoid gamma decid much curvatur want decis boundari high gamma valu model becom complex would consid point close hyperplan model
648,"Tuning the SVM

The hardness of the margin is controlled by a tuning parameter, most often known as  C . For very large  C , the margin is hard, and points cannot lie on it. For smaller  C , the margin is softer, and can grow to encompass some points.

my_model = SVC(kernel='linear', C=0.1).fit(X, y)

The optimal value of the  C  parameter will depend on our dataset, and should be tuned using cv.

Higher the value of C higher will be the overfitting and lower will be the margins (support vectors are close to the hyperplane).",tune support vector machin,"['tune', 'support', 'vector', 'machin']",Tuning the SVM,hard margin control tune paramet often known c larg c margin hard point cannot lie smaller c margin softer grow encompass pointsmymodel svckernellinear c01fitx ythe optim valu c paramet depend dataset tune use cvhigher valu c higher overfit lower margin support vector close hyperplan
649,"Advantages of SVM

Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.

Reason of high speed of SVMs 

1. Quadratic optimization (convex optimization)

2. They work in the dual, with relatively few points

3. The kernel trick",advantag support vector machin,"['advantag', 'support', 'vector', 'machin']",Advantages of SVM,affect point near margin work well highdimension data—even data dimens sampl challeng regim algorithmsreason high speed svms 1 quadrat optim convex optimization2 work dual relat points3 kernel trick
650,"Disadvantages of SVM

The results are strongly dependent on a suitable choice for the softening or regularization parameter C.  The strength of the regularization is inversely proportional to C. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size (We should always be careful about iteration for large dataset, otherwise our model will be computationally expensive).

If we have the CPU cycles to commit  training and cross-validating an SVM on our data, the method can lead to excellent results.

Apart from the problem of large datasets, SVM is not a good approach for the following cases:

The data is not linearly separable

The data is has more noise i.e. target classes are overlapping.

In cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform.",disadvantag support vector machin,"['disadvantag', 'support', 'vector', 'machin']",Disadvantages of SVM,result strong depend suitabl choic soften regular paramet c strength regular invers proport c must care chosen via crossvalid expens dataset grow size alway care iter larg dataset otherwis model comput expensiveif central process unit cycl commit train crossvalid support vector machin data method lead excel resultsapart problem larg dataset support vector machin good approach follow casesth data linear separableth data nois ie target class overlappingin case number featur data point exceed number train data sampl support vector machin underperform
651,"Basics of Artificial Neural Network

Artificial Neural Network (ANN) with SGD is the only option for linear Regression/ Classification problems on large dataset (> 1 lakh experiences)

ANN with SGD and kernel approximations is the only option for non-linear Classification problems on large dataset (>1 lakh experiences)

ANN's are representative of the human brain. Human nervous system consists of billions of neurons. These neurons collectively process input received from sensory organs and decides what to do in reaction to the input.

For simplicity, ANN is called Neural network",basic artifici neural network,"['basic', 'artifici', 'neural', 'network']",Basics of Artificial Neural Network,artifici neural network artifici neural network stochast gradient descent option linear regress classif problem larg dataset 1 lakh experiencesartifici neural network stochast gradient descent kernel approxim option nonlinear classif problem larg dataset 1 lakh experiencesartifici neural network repres human brain human nervous system consist billion neuron neuron collect process input receiv sensori organ decid reaction inputfor simplic artifici neural network call neural network
652,"Perceptron, neuron, node, unit

A neural network of single layer with single neuron which gives a single output from multiple inputs (i.e. multiple input features) with the help of activation function is called a perceptron.

Thus,
perceptron = neuron=node=unit 

Neural network can have multiple hidden layers and every hidden layer can have multiple neurons

The perceptron without any activation is a linear regression algorithm and perceptron with non linear activation function is a logistic regression node. This means that each perceptron learns a decision boundary that separates two classes using a line (called a hyperplane) in the feature space.",perceptron neuron node unit,"['perceptron', 'neuron', 'node', 'unit']","Perceptron, neuron, node, unit",neural network singl layer singl neuron give singl output multipl input ie multipl input featur help activ function call perceptronthus perceptron neuronnodeunit neural network multipl hidden layer everi hidden layer multipl neuronsth perceptron without activ linear regress algorithm perceptron non linear activ function logist regress node mean perceptron learn decis boundari separ two class use line call hyperplan featur space
653,"Model coefficients

The coefficients of the model are referred to as input weights and are trained using the stochastic gradient descent (SGD) optimization algorithm.",model coeffici,"['model', 'coeffici']",Model coefficients,coeffici model refer input weight train use stochast gradient descent sgd optim algorithm
654,"The components of neural network: 

1. Input layer (It includes all the feature values of the data for forward propagation)-This is nothing but inputs not a layer(defined as input_dim in the first hidden layer), 

2. Hidden layers (can have none, one or multiple) (defined with my_model.add method), 

3. Output layer (have 1 node for binary output) (defined with my_model.add method),

4. Activation function (defined with Dense function).",compon neural network,"['compon', 'neural', 'network']",The components of neural network: ,1 input layer includ featur valu data forward propagationthi noth input layerdefin inputdim first hidden layer 2 hidden layer none one multipl defin mymodeladd method 3 output layer 1 node binari output defin mymodeladd method4 activ function defin dens function
655,"Backward propagation

We must adjust the weights to make the network fit the training data. The process of making these adjustments is known as backward propagation.",backward propag,"['backward', 'propag']",Backward propagation,must adjust weight make network fit train data process make adjust known backward propag
656,"Understanding epochs

An epochs is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. 

One epoch means one forward and one backward propagation for the entire training data.

Epoch is defined during model training.",understand epoch,"['understand', 'epoch']",Understanding epochs,epoch term use machin learn indic number pass entir train dataset machin learn algorithm complet one epoch mean one forward one backward propag entir train dataepoch defin model train
657,"Batch size and iteration

Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration.

If the batch_size is the whole training dataset then the number of epochs is the number of iterations. 

One iteration means one forward and one backward propagation for the batch. 

batch_size is also defined during model training.",batch size iter,"['batch', 'size', 'iter']",Batch size and iteration,batch size term use machin learn refer number train exampl util one iterationif batchsiz whole train dataset number epoch number iter one iter mean one forward one backward propag batch batchsiz also defin model train
658,"Optimal accuracy in Neural Network

When we find the optimal number of neurons in multiple layers, we get optimal accuracy.

As a first attempt, we can try any one of the following:

> The number of hidden neurons should be between the size of the input layer (no. of features) and the size of the output layer (1 for binary classification).

> The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.

> The number of hidden neurons should be less than twice the size of the input layer. ",optim accuraci neural network,"['optim', 'accuraci', 'neural', 'network']",Optimal accuracy in Neural Network,find optim number neuron multipl layer get optim accuracya first attempt tri one follow number hidden neuron size input layer featur size output layer 1 binari classif number hidden neuron 23 size input layer plus size output layer number hidden neuron less twice size input layer
659,"Applications of Neural Networks:

a. Classification of data

b. Anomaly detection

c. Speech recognition

d. Audio generation

e. Time series analysis

f. Spell checking

g. Character recognition

h. Machine translation (Converts one human language to another)

i. Image processing

> Neural networks have higher computational rates than conventional computers because a lot of the operation is done in parallel",applic neural network,"['applic', 'neural', 'network']",Applications of Neural Networks:,classif datab anomali detectionc speech recognitiond audio generation time seri analysisf spell checkingg charact recognitionh machin translat convert one human languag anotheri imag process neural network higher comput rate convent comput lot oper done parallel
660,"Steps involved in a Neural Network

1. Take inputs

2. Add bias (if required)

3. Assign random weights to input features

4. Run the code for training (applying the learning function) in forward propagation.

5. Find the error in prediction.

6. Update the weight by SGD in back propagation.

7. Repeat the training phase with updated weights.

8. Make predictions.",step involv neural network,"['step', 'involv', 'neural', 'network']",Steps involved in a Neural Network,1 take inputs2 add bias required3 assign random weight input features4 run code train appli learn function forward propagation5 find error prediction6 updat weight stochast gradient descent back propagation7 repeat train phase updat weights8 make predict
661,"Libraries for ANN model

from keras.models import Sequential

from keras.layers import Dense

from tensorflow.keras.optimizers import Adam

from keras.layers import Dropout

from keras import regularizers
",librari artifici neural network model,"['librari', 'artifici', 'neural', 'network', 'model']",Libraries for ANN model,kerasmodel import sequentialfrom keraslay import densefrom tensorflowkerasoptim import adamfrom keraslay import dropoutfrom kera import regular
662,"Define, Create and Compile ANN model

# define a function to build the keras model

def create_model(a,b):
    
  # create model

  model = Sequential()

  model.add(Dense(a, input_dim=13, activation='relu'))
    
  model.add(Dense(b, activation='relu'))
      
  model.add(Dense(1, activation='sigmoid'))
    
  # compile model

  adam = Adam(learning_rate=0.001)

  model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
  
  return model

my_model = create_model(11,10)

print(my_model.summary())

> 'a' is the no. of neurons in the first hidden layer and 'b' is the no. of neurons in the second hidden layer ",defin creat compil artifici neural network model,"['defin', 'creat', 'compil', 'artifici', 'neural', 'network', 'model']","Define, Create and Compile ANN model",defin function build kera modeldef createmodelab creat model model sequenti modeladddensea inputdim13 activationrelu modeladddenseb activationrelu modeladddense1 activationsigmoid compil model adam adamlearningrate0001 modelcompilelossbinarycrossentropi optimizerrmsprop metricsaccuraci return modelmymodel createmodel1110printmymodelsummari neuron first hidden layer b neuron second hidden layer
663,"# fit the ANN model on the dataset

my_model.fit(X_train, y_train, epochs=150, batch_size=10)",fit artifici neural network model dataset,"['fit', 'artifici', 'neural', 'network', 'model', 'dataset']",# fit the ANN model on the dataset,mymodelfitxtrain ytrain epochs150 batchsize10
664,"# evaluate the ANN model

result = my_model.evaluate(X_test,y_test,verbose=1)

print('Test loss: ', result[0])

print('Test accuracy: ', result[1])

train_pred=my_model.predict(X_train)>0.5

test_pred=my_model.predict(X_test)>0.5

print(confusion_matrix(y_train,train_pred))

print(confusion_matrix(y_test,test_pred))",evalu artifici neural network model,"['evalu', 'artifici', 'neural', 'network', 'model']",# evaluate the ANN model,result mymodelevaluatextestytestverbose1printtest loss result0printtest accuraci result1trainpredmymodelpredictxtrain05testpredmymodelpredictxtest05printconfusionmatrixytraintrainpredprintconfusionmatrixytesttestpr
665,"Plotting roc_curve

fpr_train, tpr_train, thresholds_train = roc_curve(y_train, train_pred)
auc_train = auc(fpr_train, tpr_train)

plt.figure

plt.plot([0, 1], [0, 1], 'k--')

plt.plot(fpr_train, tpr_train, label='Keras (area = {:.3f})'.format(auc_train))

plt.xlabel('False positive rate')

plt.ylabel('True positive rate')

plt.title('ROC curve')

plt.legend(loc='best')

plt.show()

Plotting roc_curve (easy way)

from sklearn import metrics

metrics.plot_roc_curve(my_model, X_train, y_train)  

Plotting precision_recall_curve (easy way)

metrics.plot_precision_recall_curve(my_model, X_train, y_train)",plot roccurv,"['plot', 'roccurv']",Plotting roc_curve,fprtrain tprtrain thresholdstrain roccurveytrain trainpr auctrain aucfprtrain tprtrainpltfigurepltplot0 1 0 1 kpltplotfprtrain tprtrain labelkera area 3fformatauctrainpltxlabelfals posit ratepltylabeltru posit rateplttitleroc curvepltlegendlocbestpltshowplot roccurv easi wayfrom sklearn import metricsmetricsplotroccurvemymodel xtrain ytrain plot precisionrecallcurv easi waymetricsplotprecisionrecallcurvemymodel xtrain ytrain
666,"Understanding Feature engineering

Feature engineering is a process of transforming the given data into a form which is easier to interpret.

We define feature engineering as creating new features from our existing ones to improve model performance.",understand featur engin,"['understand', 'featur', 'engin']",Understanding Feature engineering,featur engin process transform given data form easier interpretw defin featur engin creat new featur exist one improv model perform
667,"Importance of Feature Engineering

The intention of feature engineering is to achieve two primary goals:

1. Preparing an input dataset that is compatible with and best fits the machine learning algorithm.

2. Improving the performance of machine learning models",import featur engin,"['import', 'featur', 'engin']",Importance of Feature Engineering,intent featur engin achiev two primari goals1 prepar input dataset compat best fit machin learn algorithm2 improv perform machin learn model
668,"Basic EDA

1. Univariate Analysis

Ex:- CDF, PDF, Box plot, Violin plot.

2.  Bivariate analysis

Ex:- Scatter Plot, Box plot, Violin plot, Joint plot.

3. Multivariate Analysis

Ex:- Pair Plot, 3D Scatter Plot.",basic exploratori data analysi,"['basic', 'exploratori', 'data', 'analysi']",Basic EDA,1 univari analysisex cdf pdf box plot violin plot2 bivari analysisex scatter plot box plot violin plot joint plot3 multivari analysisex pair plot 3d scatter plot
669,"Understanding Outliers

> Outlier are observations that are distant from the mean location of a distribution

> Outliers don’t necessarily represent abnormal behavior or behavior generated by a different process

> Identifying outliers in our dataset is probably one of the most difficult part of data cleanup, and it takes time to get right. Even if we have a deep understanding of statistics and how outliers might affect our data, it’s always a topic to explore cautiously.

> In univariate or bivariate analysis of box plot, an observation may seems to be outlier. However, there may be some other feature which is driving this excessive high or low value.

> When the feature is skewed, it can not show the true outlier. It has to be normal distribution to show the true outliers

The most common reasons for the outliers are:

> Data errors

> Noisy data points

> Hidden patterns in the datasets",understand outlier,"['understand', 'outlier']",Understanding Outliers,outlier observ distant mean locat distribut outlier don't necessarili repres abnorm behavior behavior generat differ process identifi outlier dataset probabl one difficult part data cleanup take time get right even deep understand statist outlier might affect data it alway topic explor cautious univari bivari analysi box plot observ may seem outlier howev may featur drive excess high low valu featur skew show true outlier normal distribut show true outliersth common reason outlier data error noisi data point hidden pattern dataset
670,"Understanding Clustering Algorithm

KMeans is the only option for unsupervised machine learning with small data (< 10K experiences) with known no. of categories or clusters (need to define n_clusters=k)

MiniBatch Kmeans is the only option for unsupervised machine learning with large data (> 10K experiences) with known no. of categories (need to define n_clusters=k)

In practice, the k-means algorithm is very fast

Variational Bayesian Gaussian mixture model (VB-GMM) is the best option for unsupervised machine learning with small data (<10K experiences) with unknown no. of categories",understand cluster algorithm,"['understand', 'cluster', 'algorithm']",Understanding Clustering Algorithm,kmean option unsupervis machin learn small data 10k experi known categori cluster need defin nclusterskminibatch kmean option unsupervis machin learn larg data 10k experi known categori need defin nclusterskin practic kmean algorithm fastvari bayesian gaussian mixtur model vbgmm best option unsupervis machin learn small data 10k experi unknown categori
671,"Python coding for Kmeans

from sklearn.cluster import KMeans

my_model = KMeans(n_clusters=4)

my_model.fit(X)

y_preds = my_model.predict(X)

my_model.cluster_centers_

from sklearn.cluster import MiniBatchKMeans

my_model = MiniBatchKMeans(n_clusters=2, random_state=0, batch_size=6, max_iter=10)

my_model.fit_predict(X_final)

GMM

from sklearn.mixture import GaussianMixture

gaussian_model = GaussianMixture(n_components=2)",python code kmean,"['python', 'code', 'kmean']",Python coding for Kmeans,sklearnclust import kmeansmymodel kmeansnclusters4mymodelfitxypr mymodelpredictxmymodelclustercentersfrom sklearnclust import minibatchkmeansmymodel minibatchkmeansnclusters2 randomstate0 batchsize6 maxiter10mymodelfitpredictxfinalgmmfrom sklearnmixtur import gaussianmixturegaussianmodel gaussianmixturencomponents2
672,"Few issues of Kmeans

1. Although the expectation– maximization algorithm is guaranteed to improve the result in each step, there is no assurance that it will lead to the global best solution. 

2. Another common challenge with k-means is that we must tell it how many clusters we expect: it cannot learn the number of clusters from the data.

Ideally, in the beginning of the algorithm we would not know how many clusters should we have.

3. The algorithm will often be ineffective if the clusters have complicated geometries.

It always makes spherical clusters.",issu kmean,"['issu', 'kmean']",Few issues of Kmeans,1 although expectation– maxim algorithm guarante improv result step assur lead global best solut 2 anoth common challeng kmean must tell mani cluster expect cannot learn number cluster dataid begin algorithm would know mani cluster have3 algorithm often ineffect cluster complic geometriesit alway make spheric cluster
673,"Expectation-Maximization approach 

For supervised learning model, we use error minimization algorithm like OLS, gradient descent as we have the actual value. 

For unsupervised learning model, we use expectation– maximization algorithm

In this algorithm, we assign the data points (data point means one observation in the feature space or n dimensional space) to the nearest cluster center in each expectation step",expectationmaxim approach,"['expectationmaxim', 'approach']",Expectation-Maximization approach ,supervis learn model use error minim algorithm like ol gradient descent actual valu unsupervis learn model use expectation– maxim algorithmin algorithm assign data point data point mean one observ featur space n dimension space nearest cluster center expect step
674,"Silhouette score for finding best no. of clusters 

1. Silhouette score tells how well samples are clustered with other samples that are similar to each other. 

from sklearn.metrics import silhouette_samples, silhouette_score

list_of_silhouette_score=[]

for k in range(2,15):

  my_kmeans_model = KMeans(n_clusters=k)

  my_kmeans_model.fit(X)

  y_preds = my_kmeans_model.predict(X)

  list_of_silhouette_score.append(silhouette_score(X, y_preds,  metric='euclidean'))

plt.plot(range(2,15), list_of_silhouette_score,'bo--',linewidth=2, markersize=8)

> Take the no. of cluster for the highest silhouette_score

> Same graph we can create by KElbowVisualizer with metric='silhouette'

from yellowbrick.cluster import SilhouetteVisualizer

my_model=KMeans(n_clusters=3)

visualizer = SilhouetteVisualizer(my_model)

visualizer.fit(X)

visualizer.poof() # Draw/show/poof the data",silhouett score find best cluster,"['silhouett', 'score', 'find', 'best', 'cluster']",Silhouette score for finding best no. of clusters ,1 silhouett score tell well sampl cluster sampl similar sklearnmetr import silhouettesampl silhouettescorelistofsilhouettescorefor k range215 mykmeansmodel kmeansnclustersk mykmeansmodelfitx ypred mykmeansmodelpredictx listofsilhouettescoreappendsilhouettescorex ypred metriceuclideanpltplotrange215 listofsilhouettescorebolinewidth2 markersize8 take cluster highest silhouettescor graph creat kelbowvisu metricsilhouettefrom yellowbrickclust import silhouettevisualizermymodelkmeansnclusters3visu silhouettevisualizermymodelvisualizerfitxvisualizerpoof drawshowpoof data
675,"Elbow method for finding best no. of clusters 

2. The elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use.

sum_of_sq_dist = {}

for k in range(1,15):

  km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)
      
  km = km.fit(X)

  sum_of_sq_dist[k] = km.inertia_
    
sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))

plt.xlabel('Number of Clusters(k)')
plt.ylabel('Sum of Square Distances')
plt.title('Elbow Method For Optimal k')

plt.show()

> We can also do elbow visualization in yellowbricks

> In the x-axis of elbow curve there is no. of clusters and in the y-axis there can be distortion or inertia.

Distortion: It is calculated as the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used.

Inertia: It is the sum of squared distances of samples to their closest cluster center.

> Before using elbow method or Silhouette score, we need to look at the problem statement and dataset carefully to get an inference about the best no. of clusters. 

Say, we have a customer segmentation problem and we can see some indication of customer groups like, buy costly product less quantity, buy costly product high quantity, buy less costly product high quantity and buy less costly product less quantity. Thus we can easily decide K=4",elbow method find best cluster,"['elbow', 'method', 'find', 'best', 'cluster']",Elbow method for finding best no. of clusters ,2 elbow method heurist use determin number cluster data set method consist plot explain variat function number cluster pick elbow curv number cluster usesumofsqdist k range115 km kmeansnclust k init kmean maxit 1000 km kmfitx sumofsqdistk kminertia snspointplotx listsumofsqdistkey listsumofsqdistvaluespltxlabelnumb clustersk pltylabelsum squar distanc plttitleelbow method optim kpltshow also elbow visual yellowbrick xaxi elbow curv cluster yaxi distort inertiadistort calcul averag squar distanc cluster center respect cluster typic euclidean distanc metric usedinertia sum squar distanc sampl closest cluster center use elbow method silhouett score need look problem statement dataset care get infer best cluster say custom segment problem see indic custom group like buy cost product less quantiti buy cost product high quantiti buy less cost product high quantiti buy less cost product less quantiti thus easili decid k4
676,"Plotting Clusters

y_preds = my_model.predict(X)

centers = my_model.cluster_centers_

plt.scatter(X[:, 6], X[:, 7], c=y_preds, s=50, cmap='viridis')

plt.scatter(centers[:, 6], centers[:, 7], c='blue', s=200, alpha=0.5)

> cluster centers have same dimension as the data

> Cluster ploting is not useful for visualizing multidimensional clusters",plot cluster,"['plot', 'cluster']",Plotting Clusters,ypred mymodelpredictxcent mymodelclustercenterspltscatterx 6 x 7 cypr s50 cmapviridispltscattercent 6 center 7 cblue s200 alpha05 cluster center dimens data cluster plote use visual multidimension cluster
677,"Features of KMeans model

> Kmeans is an example of partition clustering method

> Kmeans is most sensitive to outliers 

> Kmeans is sensitive to missing values (Imputation with Expectation Maximization algorithm is the valid iterative strategy for treating missing values) 

> Kmeans is sensitive to multicollinearity of features 

> K-means is extremely sensitive to cluster center initializations (cluster seeds)

Bad initialization can lead to poor convergence speed and bad overall clustering",featur kmean model,"['featur', 'kmean', 'model']",Features of KMeans model,kmean exampl partit cluster method kmean sensit outlier kmean sensit miss valu imput expect maxim algorithm valid iter strategi treat miss valu kmean sensit multicollinear featur kmean extrem sensit cluster center initi cluster seedsbad initi lead poor converg speed bad overal cluster
678,"Density-based clustering

Density-based clustering is a non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together. Example: Density-based spatial clustering of applications with noise (DBSCAN) 

from sklearn.cluster import DBSCAN

dbscan_model = DBSCAN(eps=3, min_samples=2).fit(X)

dbscan_model.labels_

dbscan_model.predict(training_data)",densitybas cluster,"['densitybas', 'cluster']",Density-based clustering,densitybas cluster nonparametr algorithm given set point space group togeth point close pack togeth exampl densitybas spatial cluster applic nois dbscan sklearnclust import dbscandbscanmodel dbscaneps3 minsamples2fitxdbscanmodellabelsdbscanmodelpredicttrainingdata
679,"recency, frequency and monetary (RFM)

RFM customer segmentation is very commonly used in marketing. It is a heuristic technique not a ml model

Recency: How recently a customer has made a purchase. 

Frequency: How often a customer makes a purchase. 

Monetary Value: How much money a customer spends on purchases.",recenc frequenc monetari rfm,"['recenc', 'frequenc', 'monetari', 'rfm']","recency, frequency and monetary (RFM)",rfm custom segment common use market heurist techniqu machin learn modelrec recent custom made purchas frequenc often custom make purchas monetari valu much money custom spend purchas
680,"Basics of Hierarchical clustering

This is also applicable on small dataset (< 10K experiences) like Kmeans

Hierarchical clustering takes away the problem of pre-defining the number of clusters like Kmeans.

The optimal number of clusters can be obtained by the model itself and gives practical visualization with the dendrogram.

Like K-Means clustering and Expectation-Maximization clustering, hierarchical clustering algorithms also suffers from the problem of convergence at local optima

Hierarchical clustering should be primarily used for exploration.",basic hierarch cluster,"['basic', 'hierarch', 'cluster']",Basics of Hierarchical clustering,also applic small dataset 10k experi like kmeanshierarch cluster take away problem predefin number cluster like kmeansth optim number cluster obtain model give practic visual dendrogramlik kmean cluster expectationmaxim cluster hierarch cluster algorithm also suffer problem converg local optimahierarch cluster primarili use explor
681,"Types of hierarchical clustering:

1. Agglomerative hierarchical clustering (Bottom Up Approach)

We assign each data point (sample, observation or experience) to an individual cluster at the begining. 'n' clusters for 'n' observations

Then, at each iteration, we merge the closest pair of clusters and repeat this step until only a single cluster is left.

It is also known as additive hierarchical clustering.

2. Divisive Hierarchical clustering (Top Down Approach)

Also called DIANA (DIvisive ANAlysis) is the inverse of agglomerative clustering

We start with a single cluster and assign all the data points to that cluster.

Now, at each iteration, we split the farthest point in the cluster and repeat this process until each cluster only contains a single point.",type hierarch cluster,"['type', 'hierarch', 'cluster']",Types of hierarchical clustering:,1 agglom hierarch cluster bottom approachw assign data point sampl observ experi individu cluster begin n cluster n observationsthen iter merg closest pair cluster repeat step singl cluster leftit also known addit hierarch clustering2 divis hierarch cluster top approachalso call diana divis analysi invers agglom clusteringw start singl cluster assign data point clusternow iter split farthest point cluster repeat process cluster contain singl point
682,"Proximity matrix

This is a mXm (m is the number of observations) square matrix which stores the euclidean distances between all the data points of the experience set.

In each iteration, proximity matrix is updated by considering a cluster as single observation.",proxim matrix,"['proxim', 'matrix']",Proximity matrix,mxm number observ squar matrix store euclidean distanc data point experi setin iter proxim matrix updat consid cluster singl observ
683,"Understanding Linkage

During both the types of hierarchical clustering, the distance between two sub-clusters needs to be computed. The different types of linkages describe the different approaches to measure the euclidean distance between two sub-clusters of data points. 

1. Single linkage: 

Here two closest data points between the clusters is considered.

This approach can separate non-spherical clusters as long as the gap between two clusters is not small.

This approach cannot separate clusters properly if there is noise between clusters.

2. Complete linkage: 

Here two furthest data points between the clusters is considered.

This approach does well in separating clusters if there is noise between clusters.

3. Average linkage: 

Average of the distances among all the data points between the clusters is considered.

This approach also does well in separating clusters if there is noise between clusters

4. Ward’s Method:

Similar like average linkage except that Ward’s method calculates the sum of the square of the distances.

Ward’s method approach does best in separating clusters if there is noise between clusters.",understand linkag,"['understand', 'linkag']",Understanding Linkage,type hierarch cluster distanc two subclust need comput differ type linkag describ differ approach measur euclidean distanc two subclust data point 1 singl linkag two closest data point cluster consideredthi approach separ nonspher cluster long gap two cluster smallthi approach cannot separ cluster proper nois clusters2 complet linkag two furthest data point cluster consideredthi approach well separ cluster nois clusters3 averag linkag averag distanc among data point cluster consideredthi approach also well separ cluster nois clusters4 ward methodsimilar like averag linkag except ward method calcul sum squar distancesward method approach best separ cluster nois cluster
684,"Finding out no. of clusters from visualization

import scipy.cluster.hierarchy as sch

dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))

Find largest vertical distance we can, without crossing any other horizontal line",find cluster visual,"['find', 'cluster', 'visual']",Finding out no. of clusters from visualization,import scipyclusterhierarchi schdendrogram schdendrogramschlinkagex method wardfind largest vertic distanc without cross horizont line
685,"Python coding for Hierarchical clustering

from sklearn.cluster import AgglomerativeClustering

hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')

y_hc = hc.fit_predict(X)",python code hierarch cluster,"['python', 'code', 'hierarch', 'cluster']",Python coding for Hierarchical clustering,sklearnclust import agglomerativeclusteringhc agglomerativeclusteringnclust 5 affin euclidean linkag wardyhc hcfitpredictx
686,"Basics of PCA

PCA is fundamentally a dimensionality reduction algorithm (to get rid of curse of dimensionality), but it can also be useful as a tool for visualization (in 2D or 3D plot), for noise filtering (does not eleminate the noise, but reduces), for feature extraction and engineering, data compression and much more.

PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set.

First principal component is the derived variable (or auto engineered feature without any feature understanding) formed as a linear combination of the original variables that explains the most variance. Here the feature engineering is done by the PCA algorithm based on the distribution of data points in the feature space.

Given any high-dimensional dataset, we can start with PCA in order to visualize the relationship between data points, to understand the main variance in the data, and to understand the intrinsic dimensionality.

> PCA is mostly used for unsupervised learning

> Assumption: There is linear relationship between all variables.

> PCA's main weakness is that it tends to be highly affected by outliers in the data.

> If we use features of different scales, we get misleading directions.

Thus outlier treatment and feature scaling must be performed prior to PCA",basic princip compon analysi,"['basic', 'princip', 'compon', 'analysi']",Basics of PCA,princip compon analysi fundament dimension reduct algorithm get rid curs dimension also use tool visual 2d 3d plot nois filter elemin nois reduc featur extract engin data compress much moreprincip compon analysi common use mani variabl high correl desir reduc number independ setfirst princip compon deriv variabl auto engin featur without featur understand form linear combin origin variabl explain varianc featur engin done princip compon analysi algorithm base distribut data point featur spacegiven highdimension dataset start princip compon analysi order visual relationship data point understand main varianc data understand intrins dimension princip compon analysi most use unsupervis learn assumpt linear relationship variabl princip compon analysiss main weak tend high affect outlier data use featur differ scale get mislead directionsthus outlier treatment featur scale must perform prior princip compon analysi
687,"Drawback of PCA

1. Some information is lost 

2. It can be computationally intensive

3. Transformed features are often hard to interpret",drawback princip compon analysi,"['drawback', 'princip', 'compon', 'analysi']",Drawback of PCA,1 inform lost 2 comput intensive3 transform featur often hard interpret
688,"Python Coding of PCA

from sklearn.decomposition import PCA

my_model = PCA(n_components=3)

my_model.fit(X)

X_new = my_model.transform(X)

transform() function reduces dimensions according to the learnings from fit() function

> Now, X_new can be fitted in any ml model

PCA inside Pipeline function

from sklearn.pipeline import Pipeline

my_model = Pipeline([('norm', MinMaxScaler()), ('pca', PCA()), ('m', LogisticRegression())])

my_model.fit(X_train,y_train) or,

my_model.fit_transform(X_train)

y_pred = my_model.predict(X_test)",python code princip compon analysi,"['python', 'code', 'princip', 'compon', 'analysi']",Python Coding of PCA,sklearndecomposit import pcamymodel pcancomponents3mymodelfitxxnew mymodeltransformxtransform function reduc dimens accord learn fit function xnew fit machin learn modelpca insid pipelin functionfrom sklearnpipelin import pipelinemymodel pipelinenorm minmaxscal pca pca logisticregressionmymodelfitxtrainytrain ormymodelfittransformxtrainypr mymodelpredictxtest
689,"Understanding the important features in PCA

# get the number of components

n_pcs= my_model.components_.shape[0]

# get the index of the most important feature on each component

most_important = [np.abs(my_model.components_[i]).argmax() for i in range(n_pcs)]

feature_names = X.columns.tolist()

# get the names

most_important_names = [feature_names[most_important[i]] for i in range(n_pcs)]

list_of_list = [['PC{}'.format(i+1),most_important_names[i],'{}'.format(my_model.explained_variance_[i])] for i in range(n_pcs)]

# build the dataframe

df = pd.DataFrame(list_of_list,columns=['Principle-Components','Feature-Name','Explained-Variance'])",understand import featur princip compon analysi,"['understand', 'import', 'featur', 'princip', 'compon', 'analysi']",Understanding the important features in PCA,get number componentsnpc mymodelcomponentsshape0 get index import featur componentmostimport npabsmymodelcomponentsiargmax rangenpcsfeaturenam xcolumnstolist get namesmostimportantnam featurenamesmostimportanti rangenpcslistoflist pcformati1mostimportantnamesiformatmymodelexplainedvariancei rangenpc build dataframedf pddataframelistoflistcolumnsprinciplecomponentsfeaturenameexplainedvari
690,"Deeper Understanding of PCA

PCA does not change the distribution of datapoints. It only shifts the origin approximately to the center of gravity of the whole data points and selects the spinal cord of the datapoints as the first PC dimension and recalculate the coordinate of each datapoint. When we know the coordinate of any data point, that means we know the projection of the datapoint in all PC dimension. 

PC dimension-1 is the most significant extracted feature.

This is a techniques from the field of linear algebra. This is often called “feature projection” and the algorithms used are referred to as “projection methods.”

PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

This transformation from data axes to principal axes is an affine transformation, which basically means it is composed of three basic transformation: translation, rotation and uniform scaling

> An affine transformation is any transformation that preserves collinearity (i.e., all points lying on a line initially still lie on a line after transformation) and ratios of distances (e.g., the midpoint of a line segment remains the midpoint after transformation).

> Translation is a geometric transformation that moves every point of a figure, shape or space by the same distance in a given direction. A translation can also be interpreted as the addition of a constant vector to every point, or as shifting the origin of the coordinate system.

> If we don’t rotate (orthogonally) the components, the effect of PCA will diminish and we’ll have to select more number of components to explain variance in the training set.",deeper understand princip compon analysi,"['deeper', 'understand', 'princip', 'compon', 'analysi']",Deeper Understanding of PCA,princip compon analysi chang distribut datapoint shift origin approxim center graviti whole data point select spinal cord datapoint first pc dimens recalcul coordin datapoint know coordin data point mean know project datapoint pc dimens pc dimension1 signific extract featurethi techniqu field linear algebra often call “featur projection” algorithm use refer “project methods”princip compon analysi defin orthogon linear transform transform data new coordin system greatest varianc scalar project data come lie first coordin call first princip compon second greatest varianc second coordin onthi transform data axe princip axe affin transform basic mean compos three basic transform translat rotat uniform scale affin transform transform preserv collinear ie point lie line initi still lie line transform ratio distanc eg midpoint line segment remain midpoint transform translat geometr transform move everi point figur shape space distanc given direct translat also interpret addit constant vector everi point shift origin coordin system don't rotat orthogon compon effect princip compon analysi diminish we'll select number compon explain varianc train set
691,"Math behind PCA

Step 1 : Take the whole dataset with dimension  d

Step 2 : Compute the mean of every dimension of the whole dataset.

Step 3 : Compute the covariance matrix (between the features) of the whole dataset

Step 4 : Compute Eigenvectors and corresponding Eigenvalues for the covariance matrix

Step 5 : Sort eigenvalues in descending order and choose  k  eigenvectors with the largest eigenvalues to form a  d×k  dimensional projection matrix, W.

Step 6 : Transform the original dataset X via W to obtain a k-dimensional feature subspace Y

> The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. The highest eigenvalue corresponds to the 1st Principal Component and so on. 

> Thus, if the eigenvalues are roughly equal PCA will perform badly",math behind princip compon analysi,"['math', 'behind', 'princip', 'compon', 'analysi']",Math behind PCA,step 1 take whole dataset dimens dstep 2 comput mean everi dimens whole datasetstep 3 comput covari matrix featur whole datasetstep 4 comput eigenvector correspond eigenvalu covari matrixstep 5 sort eigenvalu descend order choos k eigenvector largest eigenvalu form d×k dimension project matrix wstep 6 transform origin dataset x via w obtain kdimension featur subspac eigenvector princip compon determin direct new featur space eigenvalu determin magnitud highest eigenvalu correspond 1st princip compon thus eigenvalu rough equal princip compon analysi perform bad
692,"PCA and Truncated SVD 

PCA is Truncated SVD on centered data (made by per-feature mean substraction). If the data is already centered, PCA and Truncated SVD will do the same.

In practice Truncated SVD is useful on large sparse datasets which cannot be centered without making the memory usage explode.

> Truncated meaning shortened in duration or extent",princip compon analysi truncat singular valu decomposit,"['princip', 'compon', 'analysi', 'truncat', 'singular', 'valu', 'decomposit']",PCA and Truncated SVD ,princip compon analysi truncat singular valu decomposit center data made perfeatur mean substract data alreadi center princip compon analysi truncat singular valu decomposit samein practic truncat singular valu decomposit use larg spars dataset cannot center without make memori usag explod truncat mean shorten durat extent
693,"Scree Plot

A scree plot is a line plot of the principal components vs. eigenvalues. 

The scree plot is used to determine the number of factors to retain in an exploratory factor analysis or number of principal components to keep in a principal component analysis",scree plot,"['scree', 'plot']",Scree Plot,scree plot line plot princip compon vs eigenvalu scree plot use determin number factor retain exploratori factor analysi number princip compon keep princip compon analysi
694,"Trace of matrix

The Trace of a Matrix is defined only for a Square Matrix. It is sum of its diagonal elements from the upper left to lower right

Trace of a covariance matrix of shape (n X n) = n, because all the diagonal elements are one",trace matrix,"['trace', 'matrix']",Trace of matrix,trace matrix defin squar matrix sum diagon element upper left lower righttrac covari matrix shape n x n n diagon element one
695,"Basics of Anomaly detection 

Anomaly detection is the process of identifying unexpected items or events in data sets, which differ from the norm. 

In contrast to standard classification tasks, anomaly detection (unsupervised anomaly detection) is often applied on unlabeled data, taking only the internal structure of the dataset into account.

Neural network (RNN, Deep Autoencoders) anomaly detection technique is used for large dataset (> 1lakh experiences)

> Data scaling must be done before performing anomaly detection",basic anomali detect,"['basic', 'anomali', 'detect']",Basics of Anomaly detection ,anomali detect process identifi unexpect item event data set differ norm contrast standard classif task anomali detect unsupervis anomali detect often appli unlabel data take intern structur dataset accountneur network rnn deep autoencod anomali detect techniqu use larg dataset 1lakh experi data scale must done perform anomali detect
696,"Assumptions in Anomaly detection 

Anomaly detection has two basic assumptions:

1. Anomalies are the instances which occur very rarely.

2. Their features differ from the normal instances significantly.",assumpt anomali detect,"['assumpt', 'anomali', 'detect']",Assumptions in Anomaly detection ,anomali detect two basic assumptions1 anomali instanc occur rarely2 featur differ normal instanc signific
697,"Finding global outliers

Isolation Forest is similar in principle to Random Forest and is built on the basis of decision trees. Isolation Forest, however, identifies anomalies or outliers (for small dataset) rather than profiling normal data points. Here, majority class is 1 and minority class is -1. That means an anomaly score of -1 is assigned to anomalies and 1 to normal points.

> It Identifies anomalies as the observations with short average path lengths

> It splits the data points by randomly selecting a value between the maximum and the minimum of the selected features.

Random Forest is a supervised learning  algorithm and Isolation Forest is an unsupervised learning algorithm

A global outlier is a measured sample point that has a very high or a very low value relative to all the values in a dataset.",find global outlier,"['find', 'global', 'outlier']",Finding global outliers,isol forest similar principl random forest built basi decis tree isol forest howev identifi anomali outlier small dataset rather profil normal data point major class 1 minor class 1 mean anomali score 1 assign anomali 1 normal point identifi anomali observ short averag path length split data point random select valu maximum minimum select featuresrandom forest supervis learn algorithm isol forest unsupervis learn algorithma global outlier measur sampl point high low valu relat valu dataset
698,"1. Univariate Anomaly Detection

Here, anomaly detection is carried out on single column of the data structure.

Visualization of anomalies or outliers through scatter plot

plt.scatter(range(df.shape[0]), np.sort(df['column_name'].values))",1 univari anomali detect,"['1', 'univari', 'anomali', 'detect']",1. Univariate Anomaly Detection,anomali detect carri singl column data structurevisu anomali outlier scatter plotpltscatterrangedfshape0 npsortdfcolumnnamevalu
699,"Python coding for Isolation forest

from sklearn.ensemble import IsolationForest

X=df1['Sales'].values.reshape(-1, 1) # for reshaping in 2D array

clf = IsolationForest(n_estimators=100, contamination=0.01, random_state=0)

clf.fit(X)

df['multivariate_anomaly_score'] = clf.decision_function(X)

df['multivariate_outlier'] = clf.predict(X)

> other hyperparameters of IsolationForest are max_samples and max_features",python code isol forest,"['python', 'code', 'isol', 'forest']",Python coding for Isolation forest,sklearnensembl import isolationforestxdf1salesvaluesreshape1 1 reshap 2d arrayclf isolationforestnestimators100 contamination001 randomstate0clffitxdfmultivariateanomalyscor clfdecisionfunctionxdfmultivariateoutli clfpredictx hyperparamet isolationforest maxsampl maxfeatur
700,"2. Multivariate Anomaly Detection

Most of the analysis that we end up doing are multivariate due to complexity of the world we are living in. 

In multivariate anomaly detection, outlier is a combined unusual score on at least two variables.

X=df[list_of_two_columns]
Other codes are same as univariate

df[(df['outlier_univariate_sales'] == 1) & (df['outlier_univariate_profit'] == 1) & (df['multivariate_outlier'] == -1)] # gives the df of anomalies",2 multivari anomali detect,"['2', 'multivari', 'anomali', 'detect']",2. Multivariate Anomaly Detection,analysi end multivari due complex world live multivari anomali detect outlier combin unusu score least two variablesxdflistoftwocolumn code univariatedfdfoutlierunivariatesal 1 dfoutlierunivariateprofit 1 dfmultivariateoutli 1 give df anomali
701,"Finding local outliers-LOF

from sklearn.neighbors import LocalOutlierFactor

my_lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)

other codes are same as isolation forest

LOF performs well when the density of the data point isn't constant throughout the dataset.

It gives better results than the global approach to seek out outliers. But, as there's no threshold value of LOF, the choice of a data point as an outlier is user-dependent. 

> Therefore we can run first isolation forest and then LOF to check both

A local outlier is a measured sample point that has a value within the normal range for the entire dataset. It gives Local high or low values",find local outlierslof,"['find', 'local', 'outlierslof']",Finding local outliers-LOF,sklearnneighbor import localoutlierfactormylof localoutlierfactornneighbors20 contamination01 noveltytrueoth code isol forestlof perform well densiti data point isnt constant throughout datasetit give better result global approach seek outlier there threshold valu lof choic data point outlier userdepend therefor run first isol forest lof check botha local outlier measur sampl point valu within normal rang entir dataset give local high low valu
702,"Visual representation of univariate anomalies

xx = np.linspace(df['Sales'].min(), df['Sales'].max(), len(df)).reshape(-1,1)

anomaly_score = clf.decision_function(xx)

outlier = clf.predict(xx)

plt.figure(figsize=(7,7))
plt.plot(xx, anomaly_score, label='anomaly score')

plt.fill_between(xx.T[0],np.min(anomaly_score),p.max(anomaly_score),where=outlier==-1,color='r',alpha=.4, label='outlier region')",visual represent univari anomali,"['visual', 'represent', 'univari', 'anomali']",Visual representation of univariate anomalies,xx nplinspacedfsalesmin dfsalesmax lendfreshape11anomalyscor clfdecisionfunctionxxoutli clfpredictxxpltfigurefigsize77 pltplotxx anomalyscor labelanomali scorepltfillbetweenxxt0npminanomalyscorepmaxanomalyscorewhereoutlier1colorralpha4 labeloutli region
703,"Deeper Understanding of Anomalies

> Anomalies and outliers are not same. Anomalies are patterns of different data within given data, whereas Outliers would be merely extreme data points within data. If not aggregated appropriately, anomalies may be neglected as outliers . Anomalies could be explained by few features (may be new features).

> Anomalies are data patterns that are generated by different processes or the experiences for which there is no feature in our dataset to catch the pattern",deeper understand anomali,"['deeper', 'understand', 'anomali']",Deeper Understanding of Anomalies,anomali outlier anomali pattern differ data within given data wherea outlier would mere extrem data point within data aggreg appropri anomali may neglect outlier anomali could explain featur may new featur anomali data pattern generat differ process experi featur dataset catch pattern
704,"Natural Language Understanding (NLU) in five dimensions

Natural language can be oral (speech) or written (text) form

Oral Communication is an informal communication and written Communication is formal communication

The five dimensions of NLU are:

1. Lexical: Lexical is a brach of morphology. Understanding meaning from vocabulary (understanding the meaning of word by analyzing the structure or parts of words such as stems, root words, prefixes, and suffixes)

> Phoneme-smallest unit of sound (e.g. Ch, Ph etc)

> Morpheme-smallest meaningful unit (examples of morphemes would be the parts ""un-"", ""break"", and ""-able"" in the word ""unbreakable"")

2. Syntactic: Understanding meaning from syntax (understanding the meaning of word/ phrase according to syntax or grammer) 

3. Semantic: Understanding meaning from context (understanding the meaning of word/ phrase in relation to context)-used in language model

4. Pragmatic: Understanding meaning from general use (understanding the meaning of word/ phrase according to what people mean by the language they use)

5. NLU -informal has a fifth dimension: Understanding meaning from the feel of sound",natur languag understand nlu five dimens,"['natur', 'languag', 'understand', 'nlu', 'five', 'dimens']",Natural Language Understanding (NLU) in five dimensions,natur languag oral speech written text formor communic inform communic written communic formal communicationth five dimens natur languag understand are1 lexic lexic brach morpholog understand mean vocabulari understand mean word analyz structur part word stem root word prefix suffix phonemesmallest unit sound eg ch ph etc morphemesmallest meaning unit exampl morphem would part un break abl word unbreakable2 syntact understand mean syntax understand mean word phrase accord syntax grammer 3 semant understand mean context understand mean word phrase relat contextus languag model4 pragmat understand mean general use understand mean word phrase accord peopl mean languag use5 natur languag understand inform fifth dimens understand mean feel sound
705,"Important Applications of NLP

1. Question & Answer (Chatbot, Voicebot etc.)

2. Summarizing a paragraph

3. Understanding genuine paragraph

4. Sentiment analysis etc.

> Modern NLP algorithms are based on machine learning, especially statistical machine learning.

> Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.

> For language model, conditional probability is rewritten as joint probability

P(A, B) = P(A)* P(B | A)",import applic natur languag process,"['import', 'applic', 'natur', 'languag', 'process']",Important Applications of NLP,1 question answer chatbot voicebot etc2 summar paragraph3 understand genuin paragraph4 sentiment analysi etc modern natur languag process algorithm base machin learn especi statist machin learn machin learn model design make accur predict possibl statist model design infer relationship variabl languag model condit probabl rewritten joint probabilitypa b pa pb
706,"Feature engineering in NLP

1. Follow 'Steps of Text Pre-processing' as listed in Naive Bayes Classifier

2. During the steps of text pre-processing, along with removing punctuation and stop words, we can perform stemming/ lemmatization (generates the root form of the inflected words) for reducing no. of features/tokens

3. We can also reduce no. of tokens by taking all synonyms as single feature 

4. Removing high frequency words after calculating tf-idf weights

> The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text.",featur engin natur languag process,"['featur', 'engin', 'natur', 'languag', 'process']",Feature engineering in NLP,1 follow step text preprocess list naiv bay classifier2 step text preprocess along remov punctuat stop word perform stem lemmat generat root form inflect word reduc featurestokens3 also reduc token take synonym singl featur 4 remov high frequenc word calcul tfidf weight word general filter process natur languag call stop word actual common word languag like articl preposit pronoun conjunct etc add much inform text
707,"n-gram in NLP

One word tokens are called uni-gram, two words tokens are called Bi-grams, three words tokens are called Tri-grams etc. n-gram is used both in CountVectorizer and TfidfVectorizer.

In the fields of computational linguistics and probability, an n-gram is a contiguous (touching) sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.

Example: 
Sentence is ""This article is on NLP""

uni-gram tokens will be (“This”, “article”, “is”, “on”, “NLP”)  and bi-gram tokens will be (‘This article’, ‘article is’, ‘is on’,’on NLP’).

Code example:

tfid_vectorizer = TfidfVectorizer(ngram_range = (1,3)) # This will consider all unigram(1), bigram (2) and trigram (3) features

count_vectorizer = CountVectorizer(ngram_range = (1,3))",ngram natur languag process,"['ngram', 'natur', 'languag', 'process']",n-gram in NLP,one word token call unigram two word token call bigram three word token call trigram etc ngram use countvector tfidfvectorizerin field comput linguist probabl ngram contigu touch sequenc n item given sampl text speech item phonem syllabl letter word base pair accord applicationexampl sentenc articl nlpunigram token “this” “article” “is” “on” “nlp” bigram token this articl articl is is on''on nlp'code exampletfidvector tfidfvectorizerngramrang 13 consid unigram1 bigram 2 trigram 3 featurescountvector countvectorizerngramrang 13
708,"Understanding TF-IDF

TF-IDF is better technique than CountVectorizer, because in TF-IDF all the documents perspective is also considered. We can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions. However choice of algorithm can decide which vectorizer is working better.

tfid_vectorizer = TfidfVectorizer(max_df = 0.9,min_df = 10)

tfidf_matrix = tfid_vectorizer.fit_transform(my_df['tokenized_message'])

dictionary = tfid_vectorizer.vocabulary_.items()

tf-idf weight is the product of two terms: 

The first term is the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; 

The second term is the Inverse Document Frequency (IDF), computed as the logarithm (ln) of the number of the documents in the corpus divided by the number of documents where the specific term appears.

> Reason of taking the Term Frequency but inverse of Document Frequency

This is because when a term is appearing in a document multiple times, we want to give more weightage, But when that term is appearing in multiple documents we want to give less weightage. ",understand tfidf,"['understand', 'tfidf']",Understanding TF-IDF,tfidf better techniqu countvector tfidf document perspect also consid remov word less import analysi henc make model build less complex reduc input dimens howev choic algorithm decid vector work bettertfidvector tfidfvectorizermaxdf 09mindf 10tfidfmatrix tfidvectorizerfittransformmydftokenizedmessagedictionari tfidvectorizervocabularyitemstfidf weight product two term first term normal term frequenc tf aka number time word appear document divid total number word document second term invers document frequenc idf comput logarithm ln number document corpus divid number document specif term appear reason take term frequenc invers document frequencythi term appear document multipl time want give weightag term appear multipl document want give less weightag
709,"RNN or LSTM in NLP

When the sequence of words matter, we use RNN or LSTM (Long Short Term Memory networks  – a special kind of RNN). 

LSTM and RNN are not exactly same. LSTM networks are a type of RNN that uses special units (input modulation gate and forgot gate) in addition to standard units (input gate and output gate). LSTM units include a 'memory cell' that can maintain information in memory for long periods of time. This memory cell lets them learn longer-term dependencies.

Example:
1. I have to read this book.
2. I have this book to read.

With the first sentence, I am saying that I am obliged to read “this book.” It is something that must be done. 
With the second sentence, I am saying that I am in possession of “this book,” and while I could read it, I am not obliged to do so.

If we want our NLP model to understand the difference between these two sentences. Then we need to use RNN or LSTM. Bag of word techniques like TFIDF or CountVec will not serve this purpose, because in both the sentences frequency of all the words are same.

Nowadays, Transformers are used for this purpose. Latest is GPT-3 (third generation Generative Pre-trained Transformer)

Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for NLP pre-training developed by Google.",recurr neural network long short term memori natur languag process,"['recurr', 'neural', 'network', 'long', 'short', 'term', 'memori', 'natur', 'languag', 'process']",RNN or LSTM in NLP,sequenc word matter use recurr neural network long short term memori long short term memori network – special kind recurr neural network long short term memori recurr neural network exact long short term memori network type recurr neural network use special unit input modul gate forgot gate addit standard unit input gate output gate long short term memori unit includ memori cell maintain inform memori long period time memori cell let learn longerterm dependenciesexampl 1 read book 2 book readwith first sentenc say oblig read “this book” someth must done second sentenc say possess “this book” could read oblig soif want natur languag process model understand differ two sentenc need use recurr neural network long short term memori bag word techniqu like tfidf countvec serv purpos sentenc frequenc word samenowaday transform use purpos latest gpt3 third generat generat pretrain transformerbidirect encod represent transform bert transformerbas machin learn techniqu natur languag process pretrain develop googl
710,"Libraries for NLP

from sklearn import preprocessing

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from sklearn.model_selection import train_test_split, KFold

from nltk.corpus import stopwords

from nltk.stem.snowball import SnowballStemmer

Famous open source libraries for NLP: Hugging Face, spaCy and nltk are used to run large scale NLP",librari natur languag process,"['librari', 'natur', 'languag', 'process']",Libraries for NLP,sklearn import preprocessingfrom sklearnfeatureextractiontext import countvector tfidfvectorizerfrom sklearnmodelselect import traintestsplit kfoldfrom nltkcorpus import stopwordsfrom nltkstemsnowbal import snowballstemmerfam open sourc librari natur languag process hug face spaci nltk use run larg scale natur languag process
711,"Right order for a text classification 

1. Text cleaning or text pre-processing

2. Text annotation or labeling

3. Text to predictors

4. Gradient descent

5. Model tuning

> Text annotation is identifying and labeling sentences with additional information or metadata to define the characteristics of sentences. 
",right order text classif,"['right', 'order', 'text', 'classif']",Right order for a text classification ,1 text clean text preprocessing2 text annot labeling3 text predictors4 gradient descent5 model tune text annot identifi label sentenc addit inform metadata defin characterist sentenc
712,"Measuring the complexity of a sentence

> Number of words in a sentence

> Average length of the words in the sentence",measur complex sentenc,"['measur', 'complex', 'sentenc']",Measuring the complexity of a sentence,number word sentenc averag length word sentenc
713,"Basics of Topic modeling

Topic modeling refers to any technique that discovers the hidden (therefore called latent) semantic structure in a corpus which provides insights into the different themes present in the texts. 

Topic modelling refers to the task of identifying topics that best describes a set of documents. These topics will only emerge during the topic modelling process. And one popular topic modeling technique is known as Latent Dirichlet Allocation (LDA).

> Corpus means a collection of written texts, especially the entire works of a particular author

> Sentiment analysis is the process of identifying the emotions and opinions expressed in a particular text.",basic topic model,"['basic', 'topic', 'model']",Basics of Topic modeling,topic model refer techniqu discov hidden therefor call latent semant structur corpus provid insight differ theme present text topic model refer task identifi topic best describ set document topic emerg topic model process one popular topic model techniqu known latent dirichlet alloc lda corpus mean collect written text especi entir work particular author sentiment analysi process identifi emot opinion express particular text
714,"Basic assumptions of all topic models:

1. Each document is a collection of latent(hidden) topics

2. Each topic is a collection of words",basic assumpt topic model,"['basic', 'assumpt', 'topic', 'model']",Basic assumptions of all topic models:,1 document collect latenthidden topics2 topic collect word
715,"Libraries for Topic Modeling

from IPython.display import display

from tqdm import tqdm

from collections import Counter

import ast

import matplotlib.mlab as mlab

from sklearn.feature_extraction.text import CountVectorizer

from textblob import TextBlob

import scipy.stats as stats

from sklearn.decomposition import TruncatedSVD

from sklearn.decomposition import LatentDirichletAllocation

from sklearn.manifold import TSNE

output_notebook()",librari topic model,"['librari', 'topic', 'model']",Libraries for Topic Modeling,ipythondisplay import displayfrom tqdm import tqdmfrom collect import counterimport astimport matplotlibmlab mlabfrom sklearnfeatureextractiontext import countvectorizerfrom textblob import textblobimport scipystat statsfrom sklearndecomposit import truncatedsvdfrom sklearndecomposit import latentdirichletallocationfrom sklearnmanifold import tsneoutputnotebook
716,"Understanding LDA

LDA takes our document-term matrix as input and yield an  n×N  topic matrix as output, where  ""N""  is the number of topic categories (which we supply as a hyperparameter) and ""n"" is the number of documents.

Thus, document is a normalized vector in the “word” space, while the LDA representation is a normalized vector in the “topic” space. 

LDA is a generative probabilistic process, designed with the specific goal of uncovering latent topic structure in text corpora.

As LDA is generative probabilistic process, we can think LDA as a machine that generates document based on random probability distribution settings alpha and beta. 

A document is a probability distribution of latent topics (alpha) and a topic is a probability distribution of words (beta)

Then the machine finds the best fit of the generated document with the set of documents fitted into by adjusting the parameters alpha and beta.

Then it gives us the topic of the document from the final alpha and beta.

> LDA is a computationally heavy process. So, we take random sample (say 1000 examples) from large text dataset  and then convert it into small_docment_term_matrix by applying countvectorizer/tf-idf vectorizer before passing the text dataset to LDA algorithm

> Dirichlet Distribution is used in LDA. 
> Any set where the numbers sum up to 1 is called  Dirichlet Distribution. 

e.g. {0.6,0.4} is a dirichlet distribution or a probability simplex 

> Probability simplex is a set where each point represents a probability distribution between a finite number of mutually exclusive events.",understand latent dirichlet alloc,"['understand', 'latent', 'dirichlet', 'alloc']",Understanding LDA,latent dirichlet alloc take documentterm matrix input yield n×n topic matrix output n number topic categori suppli hyperparamet n number documentsthus document normal vector “word” space latent dirichlet alloc represent normal vector “topic” space latent dirichlet alloc generat probabilist process design specif goal uncov latent topic structur text corporaa latent dirichlet alloc generat probabilist process think latent dirichlet alloc machin generat document base random probabl distribut set alpha beta document probabl distribut latent topic alpha topic probabl distribut word betathen machin find best fit generat document set document fit adjust paramet alpha betathen give us topic document final alpha beta latent dirichlet alloc comput heavi process take random sampl say 1000 exampl larg text dataset convert smalldocmenttermmatrix appli countvectorizertfidf vector pass text dataset latent dirichlet alloc algorithm dirichlet distribut use latent dirichlet alloc set number sum 1 call dirichlet distribut eg 0604 dirichlet distribut probabl simplex probabl simplex set point repres probabl distribut finit number mutual exclus event
717,"Plate Notation

Plate notation is a concise way of visually representing the dependencies among model parameters (alpha and beta).

Alpha: density of topics generated within documents, 

Beta: density of terms generated within topics

Large rectangle,M denotes the entire set of documents. Small rectange, N denotes sub-set or sample set of examples. Theta and Phi are the multinomial distribution.",plate notat,"['plate', 'notat']",Plate Notation,plate notat concis way visual repres depend among model paramet alpha betaalpha densiti topic generat within document beta densiti term generat within topicslarg rectanglem denot entir set document small rectang n denot subset sampl set exampl theta phi multinomi distribut
718,"Python coding of LDA model 

lda = LatentDirichletAllocation()

grid_params={'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}

lda_model = GridSearchCV(lda,param_grid=grid_params)

lda_model.fit(small_document_term_matrix)

best_lda_model = lda_model.best_estimator_",python code latent dirichlet alloc model,"['python', 'code', 'latent', 'dirichlet', 'alloc', 'model']",Python coding of LDA model ,latent dirichlet alloc latentdirichletallocationgridparamsncompon 10 15 20 25 30 learningdecay 5 7 9latent dirichlet allocationmodel gridsearchcvlat dirichlet allocationparamgridgridparamslat dirichlet allocationmodelfitsmalldocumenttermmatrixbestlat dirichlet allocationmodel latent dirichlet allocationmodelbestestim
719,"Visualization of LDA

!pip install pyLDAvis

import pyLDAvis.sklearn

pyLDAvis.enable_notebook()

lda_panel = pyLDAvis.sklearn.prepare(best_lda_model, small_document_term_matrix,small_count_vectorizer,mds='tsne')
lda_panel",visual latent dirichlet alloc,"['visual', 'latent', 'dirichlet', 'alloc']",Visualization of LDA,pip instal pyldavisimport pyldavissklearnpyldavisenablenotebookldapanel pyldavissklearnpreparebestldamodel smalldocumenttermmatrixsmallcountvectorizermdstsn ldapanel
720,"Cosine Similarity

Machine learning uses Cosine Similarity in applications such as data mining and information retrieval. This allows for a Cosine Similarity measurement to distinguish and compare documents to each other based upon their similarities and overlap of subject matter or context.

Cosine similarity measures the similarity between two vectors by calculating the cosine of the angle between the two vectors.

If the ange between the vectors is zero, then cosine similarity is 1, means the vectors (word, expression or sentence) are fully similar. Consine similarity 0 means no match

> By using cosine similarity,we can easily find how two words (word embedding vectors) are similar or opposite to each other.

from sklearn.metrics.pairwise import cosine_similarity

array_vec_1 = np.array([[12,41,60,11,21]])

array_vec_2 = np.array([[12,41,60,11,14]])

cosine_similarity(array_vec_1 , array_vec_2)

> Finding cosine similarity of whole matrix

cosine_similarity(document_term_matrix[-1:] , document_term_matrix[:-1])[0]

Execution time can reduced tremendously, say from 2000 ms to 50 ms using cosine similarity of whole matrix, in place of using for loop or list comprehension to find pairwise cosine similarity individually",cosin similar,"['cosin', 'similar']",Cosine Similarity,machin learn use cosin similar applic data mine inform retriev allow cosin similar measur distinguish compar document base upon similar overlap subject matter contextcosin similar measur similar two vector calcul cosin angl two vectorsif ang vector zero cosin similar 1 mean vector word express sentenc fulli similar consin similar 0 mean match use cosin similarityw easili find two word word embed vector similar opposit otherfrom sklearnmetricspairwis import cosinesimilarityarrayvec1 nparray1241601121arrayvec2 nparray1241601114cosinesimilarityarrayvec1 arrayvec2 find cosin similar whole matrixcosinesimilaritydocumenttermmatrix1 documenttermmatrix10execut time reduc tremend say 2000 ms 50 ms use cosin similar whole matrix place use loop list comprehens find pairwis cosin similar individu
721,"Basics of Recommender System

The objective of a Recommender System is to recommend relevant items for users, based on their preference. Preference and relevance are subjective, and they are generally inferred by items users have consumed previously

Recommender systems have a problem known as user cold-start, in which it is hard to provide personalized recommendations for users with none or a very few number of consumed items, due to the lack of information about their preferences.",basic recommend system,"['basic', 'recommend', 'system']",Basics of Recommender System,object recommend system recommend relev item user base prefer prefer relev subject general infer item user consum previouslyrecommend system problem known user coldstart hard provid person recommend user none number consum item due lack inform prefer
722,"Popular Recommender Systems

1. Collaborative Filtering

2. Content-Based Filtering

3. Hybrid Approach",popular recommend system,"['popular', 'recommend', 'system']",Popular Recommender Systems,1 collabor filtering2 contentbas filtering3 hybrid approach
723,"Collaborative Filtering

This method makes automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from his/her group (collaborating)",collabor filter,"['collabor', 'filter']",Collaborative Filtering,method make automat predict filter interest user collect prefer tast inform hisher group collabor
724,"Content-Based Filtering

This method uses only information about the description and attributes of the items user has previously consumed to model user's preferences. 

In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present)

Here we are using a very popular technique in information retrieval (search engines) named TF-IDF.

We model an user profile through which we can find the top tokens and their relevance. 

To model the user profile, we take all the item profiles the user has interacted and average them. The average is weighted by the interaction strength, in other words, the items the user has interacted the most (eg. liked or commented) will have a higher strength in the final user profile.

Here, we compute the cosine similarity between the user profile and all item profiles to recommend items for that user.",contentbas filter,"['contentbas', 'filter']",Content-Based Filtering,method use inform descript attribut item user previous consum model user prefer word algorithm tri recommend item similar user like past examin presenther use popular techniqu inform retriev search engin name tfidfw model user profil find top token relev model user profil take item profil user interact averag averag weight interact strength word item user interact eg like comment higher strength final user profileher comput cosin similar user profil item profil recommend item user
725,"Hybrid Approach

Recent research has demonstrated that a hybrid approach, combining collaborative filtering and content-based filtering could be more effective than pure approaches in some cases. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem.",hybrid approach,"['hybrid', 'approach']",Hybrid Approach,recent research demonstr hybrid approach combin collabor filter contentbas filter could effect pure approach case method also use overcom common problem recommend system cold start sparsiti problem
726,"Implementation strategies of Collaborative Filtering

Collaborative Filtering (CF) has two main implementation strategies:

1. Memory-based: This approach uses the memory of previous users interactions to compute users similarities based on items they've interacted (user-based or  User Neighbourhood-based approach) or compute items similarities based on the users that have interacted with them (item-based approach)

> Finds similar users based on cosine similarity or Pearson correlation

2. Model-based: In this approach, models are developed using different machine learning algorithms to recommend items to users. There are many model-based CF algorithms, KNN is the simplest one. Others are Neural Networks, Bayesian Networks, Clustering Techniques, and Latent Factor Models such as Singular Value Decomposition (SVD) and Probabilistic Latent Semantic Analysis.

> Disadvantage of model-based approach: Inference is untraceable because of hidden/latent factors",implement strategi collabor filter,"['implement', 'strategi', 'collabor', 'filter']",Implementation strategies of Collaborative Filtering,collabor filter collabor filter two main implement strategies1 memorybas approach use memori previous user interact comput user similar base item theyv interact userbas user neighbourhoodbas approach comput item similar base user interact itembas approach find similar user base cosin similar pearson correlation2 modelbas approach model develop use differ machin learn algorithm recommend item user mani modelbas collabor filter algorithm knearest neighbor simplest one other neural network bayesian network cluster techniqu latent factor model singular valu decomposit svd probabilist latent semant analysi disadvantag modelbas approach infer untrac hiddenlat factor
727,"Latent factor models

Latent factor models compress user-item matrix into a low-dimensional representation in terms of latent factors.

One advantage of using this approach is that instead of having a high dimensional matrix containing abundant number of missing values we will be dealing with a much smaller matrix in lower-dimensional space.",latent factor model,"['latent', 'factor', 'model']",Latent factor models,latent factor model compress useritem matrix lowdimension represent term latent factorson advantag use approach instead high dimension matrix contain abund number miss valu deal much smaller matrix lowerdimension space
728,"Matrix Factorization (SVD)

Like factorization of any number into prime numbers, matrix can also be factorized into set of matrices(U, sigma, Vt). 

M = U. sigma. Vt

This is called singular value dicomposition.

Singular matrix means a matrix with determinant zero

The higher the number of factors, the more precise is the factorization in the original matrix reconstructions. 

For model generalization (simplification), we need to decrease the no. of factors",matrix factor svd,"['matrix', 'factor', 'svd']",Matrix Factorization (SVD),like factor number prime number matrix also factor set matricesu sigma vt u sigma vtthis call singular valu dicompositionsingular matrix mean matrix determin zeroth higher number factor precis factor origin matrix reconstruct model general simplif need decreas factor
729,"Implementation of SVD

from sklearn.feature_extraction.text import TfidfVectorizer

from scipy.sparse.linalg import svds

Step-1: Data Munging (the procedure for transforming data from erroneous or unusable forms to usable form)

We define a dictionary for event strength and map all the user interactions.

For example, a comment in an article indicates a higher interest of the user on the item than a like, or than a simple view.

Step-2: Then, to model the user interest on a given article, we aggregate all the interactions the user has performed in an item by a weighted sum of interaction type strength and apply a log transformation to smoothen the distribution.

Step-3: Train-test splitting of the dataset

Step-4: Then, we apply pivot method on train df

users_items_pivot_matrix_df = interactions_train_df.pivot(index='personId', columns='contentId',  values='eventStrength')

Step-5: Pivot df to pivot matrix conversion

users_items_pivot_matrix = users_items_pivot_matrix_df.values

Step-6: Performs matrix factorization

U, sigma, Vt = svds(users_items_pivot_matrix, k =15)

where k is the number of matrix factorization

Step-7: After the factorization, we try to to reconstruct the original matrix by multiplying its factors. The resulting matrix is not sparse any more. It was generated predictions for items the user have not yet interaction, which we will exploit for recommendations.

all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) 

Step-8: Writing the class for CFRecommender

class CFRecommender:
  
  def __init__(self, cf_predictions_df, items_df=None):
    .
    .
    return recommendations_df

Step-9: Evaluation

We choose to work with Top-N accuracy metrics, which evaluates the accuracy of the top recommendations provided to a user, comparing to the items the user has actually interacted in test set.",implement singular valu decomposit,"['implement', 'singular', 'valu', 'decomposit']",Implementation of SVD,sklearnfeatureextractiontext import tfidfvectorizerfrom scipysparselinalg import svdsstep1 data mung procedur transform data erron unus form usabl formw defin dictionari event strength map user interactionsfor exampl comment articl indic higher interest user item like simpl viewstep2 model user interest given articl aggreg interact user perform item weight sum interact type strength appli log transform smoothen distributionstep3 traintest split datasetstep4 appli pivot method train dfusersitemspivotmatrixdf interactionstraindfpivotindexpersonid columnscontentid valueseventstrengthstep5 pivot df pivot matrix conversionusersitemspivotmatrix usersitemspivotmatrixdfvaluesstep6 perform matrix factorizationu sigma vt svdsusersitemspivotmatrix k 15where k number matrix factorizationstep7 factor tri reconstruct origin matrix multipli factor result matrix spars generat predict item user yet interact exploit recommendationsalluserpredictedr npdotnpdotu sigma vt step8 write class cfrecommenderclass cfrecommend def initself cfpredictionsdf itemsdfnon return recommendationsdfstep9 evaluationw choos work topn accuraci metric evalu accuraci top recommend provid user compar item user actual interact test set
730,"metrics used for evaluation of Recommender systems

The Top-N accuracy metric is Recall@N which evaluates whether the interacted item is among the top N items (hit) in the ranked list of recommendations for a user. 

For example, if Recall@5 is utilized for measuring the performance, then my click on any of the 5 recommendations gives a positive score.

Other metrics for evaluation: Top-N Precision, MAP@K",metric use evalu recommend system,"['metric', 'use', 'evalu', 'recommend', 'system']",metrics used for evaluation of Recommender systems,topn accuraci metric recalln evalu whether interact item among top n item hit rank list recommend user exampl recall5 util measur perform click 5 recommend give posit scoreoth metric evalu topn precis mapk
731,"Shortcoming of content-based recommender systems

Users will only get recommendations related to their preferences in their profile, and recommender engine may never recommend any item with other characteristics.",shortcom contentbas recommend system,"['shortcom', 'contentbas', 'recommend', 'system']",Shortcoming of content-based recommender systems,user get recommend relat prefer profil recommend engin may never recommend item characterist
732,"Basics of Time Series

Time series is a series of data points indexed (or listed or graphed) in time order.

Time Series Analysis is to predict any future event

Time interval of time series data can be anything from millisecond level to year level or larger

This is a supervised or sudo supervised algorithm, because here the past data is considered as labels

> Graph of time series is called Historigram

> For checking randomness in time series autocorrelation plot is used",basic time seri,"['basic', 'time', 'seri']",Basics of Time Series,time seri seri data point index list graph time ordertim seri analysi predict futur eventtim interv time seri data anyth millisecond level year level largerthi supervis sudo supervis algorithm past data consid label graph time seri call historigram check random time seri autocorrel plot use
733,"Components of Time Series

Time series can be decomposed into four components (T, S, C, I)

1. Secular trend or Trend-'T' (the overall movement of a curve)

2. Seasonal variations or Seasonality-'S' (having period)-variation occurring within parts of a year

3. Cyclical fluctuations-'C' (which correspond to periodical but not seasonal variations)-e.g. Prosperity, Recession, and depression in a business

4. Irregular variations or Noise or Error-'I' or Residuals",compon time seri,"['compon', 'time', 'seri']",Components of Time Series,time seri decompos four compon c i1 secular trend trendt overal movement curve2 season variat season periodvari occur within part year3 cyclic fluctuationsc correspond period season variationseg prosper recess depress business4 irregular variat nois errori residu
734,"Multiplicative Model

The original time series is expressed as the product of trend, seasonal,cyclical and irregular components.

T*S*C*I",multipl model,"['multipl', 'model']",Multiplicative Model,origin time seri express product trend seasonalcycl irregular componentstsci
735,"Ways to approach a Time Series Prediction Problem

1. Time Series Approach

Time Series Approaches are Naive Forecast, Moving Average (MA),  Weighted average, Exponential smoothing, Double exponential smoothing, Econometric approach, Autoregressive Integrated Moving Average (ARIMA) etc. 

2. Machine Learning Approach",way approach time seri predict problem,"['way', 'approach', 'time', 'seri', 'predict', 'problem']",Ways to approach a Time Series Prediction Problem,1 time seri approachtim seri approach naiv forecast move averag weight averag exponenti smooth doubl exponenti smooth econometr approach autoregress integr move averag arima etc 2 machin learn approach
736,"1. Naive Forecast

Naive Forecast is an estimating technique in which the last period's actuals are used as this period's forecast, without adjusting them or attempting to establish causal factors (or unintended contributor to an incident).",1 naiv forecast,"['1', 'naiv', 'forecast']",1. Naive Forecast,naiv forecast estim techniqu last period actual use period forecast without adjust attempt establish causal factor unintend contributor incid
737,"2. Moving Average (MA)

A simple moving average (SMA) is a prediction that takes the k historical points. 

This is one step advancement of Naive forecast.

MA may be used to smoothen a time series. (level of smoothening depends on the value of k)

> The moving average is free from the influences of seasonal and irregular variations

> Simple average method for finding out seasonal indices is good only when the time-series has no trend and cyclic variation",2 move averag,"['2', 'move', 'averag']",2. Moving Average (MA),simpl move averag sma predict take k histor point one step advanc naiv forecastma may use smoothen time seri level smoothen depend valu k move averag free influenc season irregular variat simpl averag method find season indic good timeseri trend cyclic variat
738,"3. Weighted average 

It is a simple modification to the moving average. The weights sum up to 1 with larger weights assigned to more recent observations.

",3 weight averag,"['3', 'weight', 'averag']",3. Weighted average ,simpl modif move averag weight sum 1 larger weight assign recent observ
739,"4. Exponential smoothing

Instead of weighting the last  k  values of the time series, we start weighting all available observations while exponentially decreasing the weights as we move further back in time.

Smoothening parameter alpha is used here instead of weight, w

Like Naive forecast, MA and weighted average, it also predicts one data point in the future.

When we try to predict multiple data points (say, forecasting for horizon, h = 8) in future, general tendency of our model is the accuracy of our prediction decreases with time. Lag 0 prediction accuracy will be very high and lag n prediction accuracy will be very low.",4 exponenti smooth,"['4', 'exponenti', 'smooth']",4. Exponential smoothing,instead weight last k valu time seri start weight avail observ exponenti decreas weight move back timesmoothen paramet alpha use instead weight wlike naiv forecast weight averag also predict one data point futurewhen tri predict multipl data point say forecast horizon h 8 futur general tendenc model accuraci predict decreas time lag 0 predict accuraci high lag n predict accuraci low
740,"5. Double exponential smoothing

It is called Holt's linear trend method

Holt (1957) extended simple exponential smoothing to allow the forecasting of data with a trend. This method involves a forecast equation composed of intercept and trend like linear equation. But here the intercept and trend are functions.

The equation for level or intercept and the equation for trend are called the smoothing equations. Here, we use two smoothening paramaters, alpha and beta.",5 doubl exponenti smooth,"['5', 'doubl', 'exponenti', 'smooth']",5. Double exponential smoothing,call holt linear trend methodholt 1957 extend simpl exponenti smooth allow forecast data trend method involv forecast equat compos intercept trend like linear equat intercept trend functionsth equat level intercept equat trend call smooth equat use two smoothen paramat alpha beta
741,"Assumption in Time Series

To apply any statistical technique (starting from Naive Forecast to Econometric approach) to forecast a time series, the main assumption is that the time seires is stationary.

Stationarity: If a process is stationary, that means it does not change its statistical properties over time, namely its mean, variance and auto-correlation (means correlating current value with previous value). 

Apart from visual inspection of time series to check stationarity, Dickey-Fuller test is there for stationarity check",assumpt time seri,"['assumpt', 'time', 'seri']",Assumption in Time Series,appli statist techniqu start naiv forecast econometr approach forecast time seri main assumpt time seir stationarystationar process stationari mean chang statist properti time name mean varianc autocorrel mean correl current valu previous valu apart visual inspect time seri check stationar dickeyful test stationar check
742,"6. Econometric approach

Realworld time seires are non-stationary in nature. Therefore in econometric approach, we apply different transformation to make it stationary.",6 econometr approach,"['6', 'econometr', 'approach']",6. Econometric approach,realworld time seir nonstationari natur therefor econometr approach appli differ transform make stationari
743,"Understanding ARIMA

Autoregressive Integrated Moving Average model

A statistical model is autoregressive if it predicts future values based on past values.

ARIMA - Parameters

p: Trend autoregression order 
d: Trend difference order
q: Trend moving average order",understand arima,"['understand', 'arima']",Understanding ARIMA,autoregress integr move averag modela statist model autoregress predict futur valu base past valuesarima parametersp trend autoregress order trend differ order q trend move averag order
744,"Understanding SARIMA

Seasonal Autoregressive Integrated Moving Average model

It is used when there is seasonal component in the time series.

SARIMA - Parameters

P: Seasonal autoregressive order.

D: Seasonal difference order.

Q: Seasonal moving average order.",understand sarima,"['understand', 'sarima']",Understanding SARIMA,season autoregress integr move averag modelit use season compon time seriessarima parametersp season autoregress orderd season differ orderq season move averag order
745,"Libraries for Time Series Analysis

from dateutil.relativedelta import relativedelta

from scipy.optimize import minimize

import statsmodels.formula.api as smf

import statsmodels.tsa.api as smt

import statsmodels.api as sm

import scipy.stats as scs

from itertools import product  
                 
from tqdm import tqdm_notebook",librari time seri analysi,"['librari', 'time', 'seri', 'analysi']",Libraries for Time Series Analysis,dateutilrelativedelta import relativedeltafrom scipyoptim import minimizeimport statsmodelsformulaapi smfimport statsmodelstsaapi smtimport statsmodelsapi smimport scipystat scsfrom itertool import product tqdm import tqdmnotebook
746,"Machine learning approach in Time series

To solve time series prediction problem by machine learning approach, we need to convert the two column ts dataset into a supervised learning dataset where there will be independent variables or features (X's).

We can use supervised ml algorithm after creating the features in ts

'S' related features can be created from time stamp by extracting year, month etc.

Autoregression (AR) related features can be lag 1, lag 2, lag 3 and other lag values of y.

'I' related features by taking the differences between lag values

'MA' related features can be MA3, MA4 etc. 

External or exogenous factors, target encoding, forecasts from other models and many more ideas can be features",machin learn approach time seri,"['machin', 'learn', 'approach', 'time', 'seri']",Machine learning approach in Time series,solv time seri predict problem machin learn approach need convert two column ts dataset supervis learn dataset independ variabl featur xswe use supervis machin learn algorithm creat featur tss relat featur creat time stamp extract year month etcautoregress ar relat featur lag 1 lag 2 lag 3 lag valu yi relat featur take differ lag valuesma relat featur ma3 ma4 etc extern exogen factor target encod forecast model mani idea featur
747,"Downside of SARIMA

The biggest downside of SARIMA is we need to manually check if the data is stationary or not for applying SARIMA. But in reallife we need to attempt multiple time series, in the order of thousand to solve one ts problem. So, manual checking of stationarity is not feassible.

Thus, the machine learning approach came into picture.",downsid sarima,"['downsid', 'sarima']",Downside of SARIMA,biggest downsid sarima need manual check data stationari appli sarima reallif need attempt multipl time seri order thousand solv one ts problem manual check stationar feassiblethus machin learn approach came pictur
748,"Understanding Leakage

In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time.

We must avoid leakage always by asking 'will I know the value of the feature during prediction?'",understand leakag,"['understand', 'leakag']",Understanding Leakage,statist machin learn leakag also known data leakag target leakag use inform model train process would expect avail predict timew must avoid leakag alway ask know valu featur predict
749,"Prophet or Facebook Prophet

It is an open-source library for univariate (one variable) time series forecasting developed by Facebook.

> It converts a time series problem into machine learning problem

> In PROPHET, we can specify holidays and many more

https://facebook.github.io/prophet/docs/quick_start.html#python-api

from fbprophet import Prophet

model = Prophet()

> For seasonal smooth pattern generally we use prophet

> For non-seasonal smooth pattern we generally use ml algorithm",prophet facebook prophet,"['prophet', 'facebook', 'prophet']",Prophet or Facebook Prophet,opensourc librari univari one variabl time seri forecast develop facebook convert time seri problem machin learn problem prophet specifi holiday mani morehttpsfacebookgithubioprophetdocsquickstarthtmachin learningpythonapifrom fbprophet import prophetmodel prophet season smooth pattern general use prophet nonseason smooth pattern general use machin learn algorithm
750,"Cross Validation in Time Series

We can not randomly shuffle k-folds cross validation on ts, because here, y is highly dependent on sequential time.

So, here cross validation shall be done by k-folds sequentially. This is called temporal cross-validation.
",cross valid time seri,"['cross', 'valid', 'time', 'seri']",Cross Validation in Time Series,random shuffl kfold cross valid ts high depend sequenti timeso cross valid shall done kfold sequenti call tempor crossvalid
751,"Demand pattern classification in time series

1. Smooth-The demand is very regular in time and in quantity (interval and quantity both constant)

2. Intermittent- The demand history shows very little variation in demand quantity but a high variation in the interval between two demands (interval varying but quantity constant)

3. Erratic- The demand has regular occurrences in time with high quantity variations (interval constant but quantity varying)

4. Lumpy- The demand is characterized by a large variation in quantity and in time (interval and quantity both varying)

> Here constant means little variation",demand pattern classif time seri,"['demand', 'pattern', 'classif', 'time', 'seri']",Demand pattern classification in time series,1 smoothth demand regular time quantiti interv quantiti constant2 intermitt demand histori show littl variat demand quantiti high variat interv two demand interv vari quantiti constant3 errat demand regular occurr time high quantiti variat interv constant quantiti varying4 lumpi demand character larg variat quantiti time interv quantiti vari constant mean littl variat
752,"Product forecastability

To determine a product forecastability, we apply two coefficients:

1. Average Demand Interval (ADI). It measures the demand regularity in time by computing the average interval between two demands.

2. Square of the Coefficient of Variation (CV²). It measures the variation in quantities.

> Smooth demand (ADI < 1.32 and CV² < 0.49)

> Intermittent demand (ADI >= 1.32 and CV² < 0.49)

> Erratic demand (ADI < 1.32 and CV² >= 0.49)

> Lumpy demand (ADI >= 1.32 and CV² >= 0.49)",product forecast,"['product', 'forecast']",Product forecastability,determin product forecast appli two coefficients1 averag demand interv adi measur demand regular time comput averag interv two demands2 squar coeffici variat cv² measur variat quantiti smooth demand adi 132 cv² 049 intermitt demand adi 132 cv² 049 errat demand adi 132 cv² 049 lumpi demand adi 132 cv² 049
753,"Basics of case study

Case study is the discussion regarding any business problem to make a clear structure of the data requirements to solve the problem.

On getting any problem statement, at first we need to fully clarify the problem statement.

Then the first question that should arise in our mind is there any existing formula to solve the dependent variable? For example, to find the amount with simple bank interest we have y = p(1+rt) with features principal, rate of interest, and time.

When we do not have an existing formula, we need to list down the features or independent variables for capturing data. Because understanding the features (Dharma in Sanskrit) is everything for learning from the experiences !

Then what should come to our mind do we need a quick and simple but fair enough solution or an exact solution. For the first case, the human heuristic shall be used to find a formula; for the second case, a machine learning model shall be employed.

The final step will be deciding the trigger point (direct or indirect input) for the model. Trigger is an external signal to control the execution of a model.",basic case studi,"['basic', 'case', 'studi']",Basics of case study,case studi discuss regard busi problem make clear structur data requir solv problemon get problem statement first need fulli clarifi problem statementthen first question aris mind exist formula solv depend variabl exampl find amount simpl bank interest p1rt featur princip rate interest timewhen exist formula need list featur independ variabl captur data understand featur dharma sanskrit everyth learn experi come mind need quick simpl fair enough solut exact solut first case human heurist shall use find formula second case machin learn model shall employedth final step decid trigger point direct indirect input model trigger extern signal control execut model
754,"Categorization of case studies

1. Estimate the size of something

2. Fix something bad 

3. Take a strategic decision

4. Improve something

5. Build something new",categor case studi,"['categor', 'case', 'studi']",Categorization of case studies,1 estim size something2 fix someth bad 3 take strateg decision4 improv something5 build someth new
755,"Guesstimate (Problem Solving Approach)

Guesstimate means an estimate based on a mixture of guesswork and calculation. 

We can understand guesstimate as multiple linear regression problem. Here, we need to estimate the value of a dependent variable. 

We know that for linear regression, 
Y=w0+ w1*x1+w2*x2+...+wn*xn

Now to find the value of Y, we need to understand the features or independent variables (x1, x2,...,xn) and guess feature values (e.g. size of family is 4) and their weights (50% are using smart phone). Feature values and their weights are the assumptions in guesstimate.

Steps are as follows:

1. Clarify the problem statement

> At first, we need to ask all the questions to make the problem statement clear

> Choose between supply side or demand side approach (get confirmation)

Supply side, where the number of units is equal to the number of suppliers x the number of units sold per supplier.

Demand side, where the number of units is equal to the number of customers x the number of units bought per customer.

> Confirm assumptions during each step

2. Break down into sub-problems 

> Check for bottleneck if any (in case of demand side approach)

> Prepare a diagram considering the dependent variable and all independent variables

3. Solve sub-problems 

> Understand the filters if any [Geography and demography (age, gender, life expectancy, income level etc.)]

> Guesstimate the values of all independent variables. Take logical simple numbers (e.g. in place of 45 we can take 50, in place of 89 we can take 100 etc.) for easy calculation

> Avoid zero error using exponents (e.g. 5x10^5, 2X10^3 etc.)

4. Consolidate our findings: Create a learning function (expressing dependent variable as a function of independent variables)

There can be overall five types of feature or independent variable:

i. Geographical features (exact village, town or city), 

ii. Time related features (season, week end, morning, noon, night etc.)

iii. Customer related features (gender, age, employed or not etc.), 

iv. Product related features (length of article, size of product etc.) and  

v. Customer product interaction related features  (like, dislike etc.)",guesstim problem solv approach,"['guesstim', 'problem', 'solv', 'approach']",Guesstimate (Problem Solving Approach),guesstim mean estim base mixtur guesswork calcul understand guesstim multipl linear regress problem need estim valu depend variabl know linear regress yw0 w1x1w2x2wnxnnow find valu need understand featur independ variabl x1 x2xn guess featur valu eg size famili 4 weight 50 use smart phone featur valu weight assumpt guesstimatestep follows1 clarifi problem statement first need ask question make problem statement clear choos suppli side demand side approach get confirmationsuppli side number unit equal number supplier x number unit sold per supplierdemand side number unit equal number custom x number unit bought per custom confirm assumpt step2 break subproblem check bottleneck case demand side approach prepar diagram consid depend variabl independ variables3 solv subproblem understand filter geographi demographi age gender life expect incom level etc guesstim valu independ variabl take logic simpl number eg place 45 take 50 place 89 take 100 etc easi calcul avoid zero error use expon eg 5x105 2x103 etc4 consolid find creat learn function express depend variabl function independ variablesther overal five type featur independ variablei geograph featur exact villag town citi ii time relat featur season week end morn noon night etciii custom relat featur gender age employ etc iv product relat featur length articl size product etc v custom product interact relat featur like dislik etc
756,"Meaning of Diagnosis  

Diagnosis is the art or act of identifying a disease from its signs and symptoms. Computer Aided Diagnosis (CAD) includes multiple elements like concepts of artificial intelligence (AI), computer vision, and medical image processing. The main application of CAD system is finding abnormality in human body. ",mean diagnosi,"['mean', 'diagnosi']",Meaning of Diagnosis  ,diagnosi art act identifi diseas sign symptom comput aid diagnosi cad includ multipl element like concept artifici intellig ai comput vision medic imag process main applic cad system find abnorm human bodi
757,"ML in Healthcare

The implementation of EHR i.e. Electronic health records started around 2010

Adoption of Machine learning in other domains like finance, retail etc is easier as compared to Healthcare

1. Use of Computer Vision 

i.Medical Imaging

ii.Nuclear Medicine Imaging (""radiology done inside out"" means it records radiation emitting from within the body)

2. Use of NLP 

i.Physician Notes

ii.Nurse Notes

iii.Speech to text 

iv.Billing Denials annotation

3. Use of Tabular data based machine learning

i.Prediction of Length of Stay

ii.Readmission of a Patient

iii.Early detection of Disease",machin learn healthcar,"['machin', 'learn', 'healthcar']",ML in Healthcare,implement ehr ie electron health record start around 2010adopt machin learn domain like financ retail etc easier compar healthcare1 use comput vision imed imagingiinuclear medicin imag radiolog done insid mean record radiat emit within body2 use natur languag process iphysician notesiinurs notesiiispeech text ivbil denial annotation3 use tabular data base machin learningipredict length stayiireadmiss patientiiiear detect diseas
758,"Geometric Mean Length of Stay (GMLOS)

GMLOS is the standard LOS (Length of Stay) metric determined and published by CMS (Centers for Medicare & Medicaid Services) for each diagnostic related grouper (DRG)",geometr mean length stay gmlos,"['geometr', 'mean', 'length', 'stay', 'gmlos']",Geometric Mean Length of Stay (GMLOS),geometr mean length stay standard length stay length stay metric determin publish cms center medicar medicaid servic diagnost relat grouper drg
759,"Features for the prediction of LOS

1. Demography related features

i.Patient Age

ii.Gender

iii.Marital Status

iv.Ethnicity etc.

2. Vitals related features

i.Heart Rate

ii.Blood Pressure

iii.Oxygen level

iv.Temperature etc.

3. Utilization related features

i.Surgery Hours

ii.Number of Past visits to Hospitals

iii.Number of Surgeries etc.

4. Lab report related features

i.Creatinine

ii.Platelet Count 

iii.Magnesium 

iv.Phosporus etc. 

5. Diagnosis related features

i.Arthritis

ii.Heart failure 

iii.Diabetes mellitus 

iv.Acute kidney failure etc. ",featur predict length stay,"['featur', 'predict', 'length', 'stay']",Features for the prediction of LOS,1 demographi relat featuresipati ageiigenderiiimarit statusivethn etc2 vital relat featuresiheart rateiiblood pressureiiioxygen levelivtemperatur etc3 util relat featuresisurgeri hoursiinumb past visit hospitalsiiinumb surgeri etc4 lab report relat featuresicreatinineiiplatelet count iiimagnesium ivphosporus etc 5 diagnosi relat featuresiarthritisiiheart failur iiidiabet mellitus ivacut kidney failur etc
760,"Understanding Fraud

“Fraud” is an intended action or set of actions by users to cause harm to any Business

1. Fraud in the context of Credit Risk

It is different from “Loan Default”, which is a genuine inability of the customer to complete a certain action or abide by certain rules

2. Fraud in the context of E-commerce

It is a genuine attempt by the customer to cause financial losses to the company by unnecessarily increasing their purchasing costs, forward and reverse logistics costs etc",understand fraud,"['understand', 'fraud']",Understanding Fraud,“fraud” intend action set action user caus harm business1 fraud context credit riskit differ “loan default” genuin inabl custom complet certain action abid certain rules2 fraud context ecommerceit genuin attempt custom caus financi loss compani unnecessarili increas purchas cost forward revers logist cost etc
761,"WAYS TO CAPTURE FRAUDULENT BEHAVIOURS

For example, during bank account opening, typing speed, swipe patterns, and every click of the mouse tells a story – one of cybercriminal activity or genuine user behavior. 

BioCatch (a company) pulls together advanced behavioral insights to empower organizations with increased visibility into risk. 

Even when we have never seen a user before, BioCatch quickly spots trusted behaviors to create a smooth customer journey during account opening.",way captur fraudul behaviour,"['way', 'captur', 'fraudul', 'behaviour']",WAYS TO CAPTURE FRAUDULENT BEHAVIOURS,exampl bank account open type speed swipe pattern everi click mous tell stori – one cybercrimin activ genuin user behavior biocatch compani pull togeth advanc behavior insight empow organ increas visibl risk even never seen user biocatch quick spot trust behavior creat smooth custom journey account open
762,"Social Engineering Scams

Four ways behavioral biometrics can uncover a real time social engineering scam:

1. Length of session: Sessions are longer and behaviors such as aimless mouse movements are common indicating a person is waiting for instructions

2. Segmented typing: These patterns indicate dictation such as a cybercriminal reading off an account number to transfer funds

3. Hesitation: Longer pauses before performing simple intuitive actions such as clicking on the submit button

4. Displacement: Continuous movement of the phone suggests the user is picking the phone up to take instructions and placing it back down to perform the actions instructed by the cybecriminals",social engin scam,"['social', 'engin', 'scam']",Social Engineering Scams,four way behavior biometr uncov real time social engin scam1 length session session longer behavior aimless mous movement common indic person wait instructions2 segment type pattern indic dictat cybercrimin read account number transfer funds3 hesit longer paus perform simpl intuit action click submit button4 displac continu movement phone suggest user pick phone take instruct place back perform action instruct cybecrimin
763,"Synthetic Minority Over-sampling Technique (SMOTE)

Classification for Modeling for frauds will require SMOTE as incidence rate is pretty low

SMOTE is an approach to the construction of classifiers from imbalanced datasets (stratified sampling is a simple but effective solution in train_test_split for imbalanced data)

> Here, we create resampled dataset X_train_res and y_train_res from X_train and y_train with the help of SMOTE

from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state = 2)

X_train_res, y_train_res = sm.fit_resample(X_train, y_train)",synthet minor oversampl techniqu smote,"['synthet', 'minor', 'oversampl', 'techniqu', 'smote']",Synthetic Minority Over-sampling Technique (SMOTE),classif model fraud requir synthet minor oversampl techniqu incid rate pretti lowsynthet minor oversampl techniqu approach construct classifi imbalanc dataset stratifi sampl simpl effect solut traintestsplit imbalanc data creat resampl dataset xtrainr ytrainr xtrain ytrain help synthet minor oversampl techniquefrom imblearnoversampl import synthet minor oversampl techniquesm synthet minor oversampl techniquerandomst 2xtrainr ytrainr smfitresamplextrain ytrain
764,"C4 -Curious Case of Customer Credit

It means, we are egger to know the whole analysis for customer credit in some banking sector",c4 curious case custom credit,"['c4', 'curious', 'case', 'custom', 'credit']",C4 -Curious Case of Customer Credit,mean egger know whole analysi custom credit bank sector
765,"Overall Objective of Credit Risk

Wishes to optimise the portfolio by reducing the Risk

Solutions are

1. Predictive Analytics

2. Prescriptive Analytics",overal object credit risk,"['overal', 'object', 'credit', 'risk']",Overall Objective of Credit Risk,wish optimis portfolio reduc risksolut are1 predict analytics2 prescript analyt
766,"Predictive Analytics of Credit Risk

Here, we build the model that would predict loan default

i.This is classification problem and

ii.Recall to be given more importance

Features of Predictive Analytics

1.Loan amount, terms

2.Customer default history

a.3 months, 6 months, 9 months

b.Customers outstanding debts/obligations etc

3.Customer demography

a.Age

b.Salary

c.Disposable income

d.PPP (public–private partnership) etc

4.City level features etc",predict analyt credit risk,"['predict', 'analyt', 'credit', 'risk']",Predictive Analytics of Credit Risk,build model would predict loan defaultithi classif problem andiirecal given importancefeatur predict analytics1loan amount terms2custom default historya3 month 6 month 9 monthsbcustom outstand debtsoblig etc3custom demographyaagebsalarycdispos incomedppp public–priv partnership etc4citi level featur etc
767,"Prescriptive Analytics

Prescriptive Analytics helps us draw up specific recommendations by deeply looking into the ""what"" and ""why"" of a potential future outcome.

It utilizes complicated mathematical algorithms and machine learning.
",prescript analyt,"['prescript', 'analyt']",Prescriptive Analytics,prescript analyt help us draw specif recommend deepli look potenti futur outcomeit util complic mathemat algorithm machin learn
768,"Metrics that help in evaluating our model’s accuracy

1. Confusion Matrix

2. F1 Score

3. ROC-AUC

4. Log Loss

5. Gini Coefficient

6. Gain and Lift Charts

7. Kolmogorov Smirnov Chart (KS score)

8. Concordant – Discordant Ratio

9. Root Mean Squared Error etc.",metric help evalu model accuraci,"['metric', 'help', 'evalu', 'model', 'accuraci']",Metrics that help in evaluating our model’s accuracy,1 confus matrix2 f1 score3 rocauc4 log loss5 gini coefficient6 gain lift charts7 kolmogorov smirnov chart ks score8 concord – discord ratio9 root mean squar error etc
769,"Model Health using KS Scores

> Used for classification models. 

> KS is a measure of the degree of separation between the positive and negative distributions.

> The Sooner the Highest KS value occurs, the better the segregation of the goods and the bads

> Gains Chart study coupled with Confusion matrix study will govern final answer
",model health use ks score,"['model', 'health', 'use', 'ks', 'score']",Model Health using KS Scores,use classif model ks measur degre separ posit negat distribut sooner highest ks valu occur better segreg good bad gain chart studi coupl confus matrix studi govern final answer
770,"Decision Making using Risk Bins

Risk appetite is the amount of risk an organization is willing to take in pursuit of objectives it deems have value. 
Depending on risk appetite and customer acquisition targets, we can choose the bins post which we wish to stop booking customers.",decis make use risk bin,"['decis', 'make', 'use', 'risk', 'bin']",Decision Making using Risk Bins,risk appetit amount risk organ will take pursuit object deem valu depend risk appetit custom acquisit target choos bin post wish stop book custom
771,"C4 -Curious Case of Customer Contacts 

It means, we are egger to know the whole analysis for customer contacts in some E-commerce sector",c4 curious case custom contact,"['c4', 'curious', 'case', 'custom', 'contact']",C4 -Curious Case of Customer Contacts ,mean egger know whole analysi custom contact ecommerc sector
772,"Important Features in E-commerce

1.Issue Category Wise analysis

2.Product type wise analysis

3.Customer demographic wise analysis

a.Area

b.Purchase history

c.Mode of payment

4.Price of product

5.Courier Mode

6.Time of order placement

7.Seller profile

a.Seller rating

b.Seller return history

8.Time taken to place the order

9.Return Logistics Partner wise",import featur ecommerc,"['import', 'featur', 'ecommerc']",Important Features in E-commerce,1issu categori wise analysis2product type wise analysis3custom demograph wise analysisaareabpurchas historycmod payment4pric product5couri mode6tim order placement7sel profileasel ratingbsel return history8tim taken place order9return logist partner wise
773,"Predictive Analytics in  E-commerce

1.Model that would predict a contact on a ticket and help solve it before contact happens

a.This is classification problem and

b.Recall to be given more importance

2.Model that predicts number of contacts to streamline workforce

a.This is regression problem and

b.It helps in WFM (Workforce management)",predict analyt ecommerc,"['predict', 'analyt', 'ecommerc']",Predictive Analytics in  E-commerce,1model would predict contact ticket help solv contact happensathi classif problem andbrecal given importance2model predict number contact streamlin workforceathi regress problem andbit help wfm workforc manag
774,"Understanding UNIX Operating System

UNIX (also referred to as UNICS) is UNiplexed Information Computing System

● UNIX Operating System was first developed in the 1960s at AT&T Labs.

● Multi-user, multi-tasking system for servers, desktops and laptops.

● Most popular varieties of UNIX are Sun Solaris, GNU/Linux, and MacOS X.",understand unix oper system,"['understand', 'unix', 'oper', 'system']",Understanding UNIX Operating System,unix also refer unic uniplex inform comput system● unix oper system first develop 1960s att labs● multius multitask system server desktop laptops● popular varieti unix sun solari gnulinux maco x
775,"Kernel and shell of UNIX

Unix Operating System is made up of three parts: the kernel, the shell and the programs.

● Kernel is the main OS program to access the storage (local or remote)

● Shell is the application program for the users to access the storage through kernel.

● Program means all the data or installed application softwares.",kernel shell unix,"['kernel', 'shell', 'unix']",Kernel and shell of UNIX,unix oper system made three part kernel shell programs● kernel main oper system program access storag local remote● shell applic program user access storag kernel● program mean data instal applic softwar
776,"Understanding LINUX Operating System

LINUX stands for Lovable Intellect Not Using XP. Linux was developed by Linus Torvalds and named after him. 

Linux is an open-source and community-developed operating system for computers, servers, mainframes, mobile devices, and embedded devices.

Linux offers great speed and security, on the other hand, Windows offers great ease of use",understand linux oper system,"['understand', 'linux', 'oper', 'system']",Understanding LINUX Operating System,linux stand lovabl intellect use xp linux develop linus torvald name linux opensourc communitydevelop oper system comput server mainfram mobil devic embed deviceslinux offer great speed secur hand window offer great eas use
777,"Time Complexity and  Space Complexity

Performance parameters of any algorithm or application (be it Local or cloud) are as follows:

1. Time complexity or Speed (Time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the input.)

2. Space Complexity (Space complexity of an algorithm quantifies the amount of space or memory taken by an algorithm to run as a function of the length of the input)

3. Ease of use

4. Reliability

5. Security

Time and space complexity depends on lots of things like hardware, operating system, processors, etc. However, we don't consider any of these factors while analyzing the algorithm. We will only consider the time taken and space comsumed by an algorithm.

> Always we need to reduce the iterations in our algorithm to to reduce to run time. We have to think about the loop which is running behind any simple function or method

> We will always take worst case scienario to define the performance of any algorithm

> As a first step of reducing the iterations, we should sort the list or array. Then, we can ignore any part of the list as per our requirement. This will help us to cut the linkage of algorithm's speed with the length of input (length of input may be very large)

> Eficient alternative to loop is writing function for repitivite work. Within the function, we should try to avoid loop and get things done by numpy array and conditional operators

> When we call same function within itself, it is called recursion",time complex space complex,"['time', 'complex', 'space', 'complex']",Time Complexity and  Space Complexity,perform paramet algorithm applic local cloud follows1 time complex speed time complex algorithm quantifi amount time taken algorithm run function length input2 space complex space complex algorithm quantifi amount space memori taken algorithm run function length input3 eas use4 reliability5 securitytim space complex depend lot thing like hardwar oper system processor etc howev dont consid factor analyz algorithm consid time taken space comsum algorithm alway need reduc iter algorithm reduc run time think loop run behind simpl function method alway take worst case scienario defin perform algorithm first step reduc iter sort list array ignor part list per requir help us cut linkag algorithm speed length input length input may larg efici altern loop write function repitivit work within function tri avoid loop get thing done numpi array condit oper call function within call recurs
778,"Ways to connect to an EC2 instance

AWS means Amazon Web Services

AWS free tier EC2 is a free Cloud computer

One EC2 instance is like one remote computer (virtual machine) running generally on Linux and on which we can install whatever software we want.

EC means Elastic Compute. So, we can resize this remote computer as per our requirement.

There are many ways to connect to an EC2 instance

1. Putty

2. Git Bash

3. AWS console panel

4. Command prompt

5. Windows powershell or any other terminal

> windows PowerShell is a more advanced version of cmd (command prompt). It is not only an interface but also a scripting language that is used to carry out administrative tasks more easily.",way connect ec2 instanc,"['way', 'connect', 'ec2', 'instanc']",Ways to connect to an EC2 instance,aw mean amazon web servicesaw free tier ec2 free cloud computeron ec2 instanc like one remot comput virtual machin run general linux instal whatev softwar wantec mean elast comput resiz remot comput per requirementther mani way connect ec2 instance1 putty2 git bash3 aw consol panel4 command prompt5 window powershel termin window powershel advanc version cmd command prompt interfac also script languag use carri administr task easili
779,"Security of Remote Computer

Since multi-user OS have several users accessing the system resources simultaneously, it's very important for the system administrators to implement security features within the system. 

These features could include account separation (by login through key pairs, a identity file), user groups, roles, and permissions.",secur remot comput,"['secur', 'remot', 'comput']",Security of Remote Computer,sinc multius oper system sever user access system resourc simultan import system administr implement secur featur within system featur could includ account separ login key pair ident file user group role permiss
780,"Git Bash Understanding

Git is a revision control software (RCS) (created by the inventor of linux) for tracking changes in any set of files. Means it is a distributed version control system (DVCS).

Git Bash is a command line Windows application to navigate through local or remote computers having windows or linux environment. Through git bash, we can also enter into python environment to write codes.  

Bash means Bourne Again Shell. 

Git Bash is a command line environment used for creating and interacting with Git repository",git bash understand,"['git', 'bash', 'understand']",Git Bash Understanding,git revis control softwar rcs creat inventor linux track chang set file mean distribut version control system dvcsgit bash command line window applic navig local remot comput window linux environ git bash also enter python environ write code bash mean bourn shell git bash command line environ use creat interact git repositori
781,"Introduction to Repository

Repository means the storage location or the folder where codes are to be stored.

GitHub is a Git repository hosting service. It is usually used for coordinating work among programmers collaboratively developing source code during software development.

All files necessary for certain analysis can be held together and people can add in their code, graphs, etc. as the projects develop.

We can review other people’s code, add comments to certain lines or the overall document, and suggest changes. 

For collaborative projects, GitHub allows us to assign tasks to different users, making it clear who is responsible for which part of the analysis. ",introduct repositori,"['introduct', 'repositori']",Introduction to Repository,repositori mean storag locat folder code storedgithub git repositori host servic usual use coordin work among programm collabor develop sourc code softwar developmental file necessari certain analysi held togeth peopl add code graph etc project developw review peopl code add comment certain line overal document suggest chang collabor project github allow us assign task differ user make clear respons part analysi
782,"
Steps to connect to an EC2 instance

1.  From AWS console, create an EC2 instance, Create & download key pairs and let the machine run

2. Through CLI, enter into the directory of local machine where the key pair is saved and login to the EC2 instance

ssh -i 'key_pair_name' user_name@public_IP_DNS

> ssh- means Secure Shell

> i- means identity",step connect ec2 instanc,"['step', 'connect', 'ec2', 'instanc']","
Steps to connect to an EC2 instance",1 aw consol creat ec2 instanc creat download key pair let machin run2 cli enter directori local machin key pair save login ec2 instancessh keypairnam usernamepublicipdn ssh mean secur shell mean ident
783,"Understanding IP address

An IP address is an unique address that identifies a device on the internet or a local network. IP stands for ""Internet Protocol,"" 

EC2 instance public IP address changes when we start stop an instance. Elastic IP's, if attached to the instance, does not change. So, people using the IP address will not get error related to IP address. But there is a charge for elastic IP's also in instance stopped condition.",understand ip address,"['understand', 'ip', 'address']",Understanding IP address,ip address uniqu address identifi devic internet local network ip stand internet protocol ec2 instanc public ip address chang start stop instanc elast ip attach instanc chang peopl use ip address get error relat ip address charg elast ip also instanc stop condit
784,"Basic Bash or Linux commands

> Entering into the computer memory

cd folder_name -to enter into a directory

cd / -to enter into the root directory (cd /f/ -to enter intro f drive)

> To view the list of folders in the directory

ls shows the list of files and folders name

ls -l  shows long details of directory

ls -lh shows long details in more human readable forms

ls -al shows all folders including hidden folders

mkdir folder_name  -to create directory

touch file_name -to create empty file

rm file_name -to remove file

rm -r folder_name (rmdir folder_name may be used for emty directory)

mv old_name new_name -for renaming file/directory

mv file_name /directory - moving a file/directory

ln  -symbolic link (shortcut to the file / folder)

> Few other commands

wget web_address -to download any file from website

top/htop -to see current process uses for all the users

clear- to clear all the commands in git bash

> Reading Files (.txt, .csv or .py)

cat file_name -to print the file contents on the screen

vi file_name -to open the file in vi editor (:q to exit)

nano file_name -to open the file in nano editor

head / tail file_name -to look at top / bottom lines of the file

> Searching Files

grep -to search text in files / search file in the folder structure

> Reading directory path

pwd -to print the path of working directory

> To Compress

To zip single file

gzip file_name 

To zip multiple files

tar -cvf new_file_name.tar file_name1 file_name2

> To reduce the size further 

tar -cvzf file_name.tar.gz

> To Decompress 

gzip -d file_name 

tar -xvf tar_file_name

> file file_name -gives details of file type

>  up and down arrow to go to the command history

> after typing few letters of file name press tab, git bash will automatically type the file name

> Copying files

cp file_name directory_name- to copy in that folder

ftp -to copy file using FTP

> Environment Variables

export -to define the new environment variable

env -to print all environment variables

> command --help to see required argument for that command

ctrl+c for canceling or aborting any command

git clone 'http_address' -to download the code from github repository  in the current directory

bash bash_file_name.sh -to execute the whole code in that file

man command_name -to see the manual for any command for learning purpose (press q to escape the manual)-This command can only be used after logging in into a unix machine

uname to display the operating system name

uname -r to display the os version

du -gives information about how much disk space (data usage) each file in the current directory uses",basic bash linux command,"['basic', 'bash', 'linux', 'command']",Basic Bash or Linux commands,enter comput memorycd foldernam enter directorycd enter root directori cd f enter intro f drive view list folder directoryl show list file folder namel l show long detail directoryl lh show long detail human readabl formsl al show folder includ hidden foldersmkdir foldernam creat directorytouch filenam creat empti filerm filenam remov filerm r foldernam rmdir foldernam may use emti directorymv oldnam newnam renam filedirectorymv filenam directori move filedirectoryln symbol link shortcut file folder commandswget webaddress download file websitetophtop see current process use usersclear clear command git bash read file txt csv pycat filenam print file content screenvi filenam open file vi editor q exitnano filenam open file nano editorhead tail filenam look top bottom line file search filesgrep search text file search file folder structur read directori pathpwd print path work directori compressto zip singl filegzip filenam zip multipl filestar cvf newfilenametar filename1 filename2 reduc size tar cvzf filenametargz decompress gzip filenam tar xvf tarfilenam file filenam give detail file type arrow go command histori type letter file name press tab git bash automat type file name copi filescp filenam directorynam copi folderftp copi file use ftp environ variablesexport defin new environ variableenv print environ variabl command help see requir argument commandctrlc cancel abort commandgit clone httpaddress download code github repoper systemitori current directorybash bashfilenamesh execut whole code fileman commandnam see manual command learn purpoper system press q escap manualthi command use log unix machineunam display oper system nameunam r display oper system versiondu give inform much disk space data usag file current directori use
785,"Few UNIX commands during project setup

Writing into a new file

cat > my_file.csv

Ram, 80
Sham,30
Jadu,40
Ctrl+d

By '>' we, can redirect the output of any command to a file

who > names.txt saves all the users details connected to the server

wc -l < name.txt counts the no. of lines in the file

Pipe character

(pipe command or pipeline command means doing multiple operation in a single command)

who | wc -l

we can use multiple pipes in a single command

To copy file or folder from local to local

cp or cp -r

To copy file from local to remote computer

scp -i 'key_file_name' /local_file_path username@public_IP_DNS:/remote_directory_path

To copy folder from local to remote computer

scp -i 'key_file_name' -r /local_file_path username@public_IP_DNS:/remote_directory_path

To see the python version in the server (for colab)

import sys

sys.version",unix command project setup,"['unix', 'command', 'project', 'setup']",Few UNIX commands during project setup,write new filecat myfilecsvram 80 sham30 jadu40 ctrldbi redirect output command filewho namestxt save user detail connect serverwc l nametxt count line filepip characterpip command pipelin command mean multipl oper singl commandwho wc lwe use multipl pipe singl commandto copi file folder local localcp cp rto copi file local remot computerscp keyfilenam localfilepath usernamepublicipdnsremotedirectorypathto copi folder local remot computerscp keyfilenam r localfilepath usernamepublicipdnsremotedirectorypathto see python version server colabimport syssysvers
786,"File Permission Handling for security

> File permission are of three types read (r), write (w), execute (x) and it is divided among three groups

> Check left most side of the long list (run !ls -l in colab)

> The first (1) character represents file or directory (_ for file, d for directory)

Group-1: The first three (2-4) character represents the permission for the file owner (users,u)

Group-2: Second group of three character (5-7) represents permission for the group (g)

Group-3: The last group of three character (8-10) for everyone else (o)

Change file Permission
(change file mod bits)

chmod u+x o+wx file_name 
or 

chmod 102 file_name

-Three numbers represents three group permission

0- No permission

1- Execute permission

2- Write permission

3- Execute and write permissioin

4- read permission

5- read and execute

6- read and write

7- read+write+execute",file permiss handl secur,"['file', 'permiss', 'handl', 'secur']",File Permission Handling for security,file permiss three type read r write w execut x divid among three group check left side long list run ls l colab first 1 charact repres file directori file directorygroup1 first three 24 charact repres permiss file owner usersugroup2 second group three charact 57 repres permiss group ggroup3 last group three charact 810 everyon els ochang file permiss chang file mod bitschmod ux owx filenam chmod 102 filenamethre number repres three group permission0 permission1 execut permission2 write permission3 execut write permissioin4 read permission5 read execute6 read write7 readwriteexecut
787,"UNIX command in colab notebook with ! Mark

Example: 
!man cd
!ls -l

> Google colab is a cloud computing coding software hosted on Google cloud server. It has no direct access to our local drives.

> However, Colab provides various options to connect to almost any cloud storage we can imagine. 

> We can either clone an entire GitHub repository to our Colab environment or access individual files from their link",unix command colab notebook mark,"['unix', 'command', 'colab', 'notebook', 'mark']",UNIX command in colab notebook with ! Mark,exampl man cd ls l googl colab cloud comput code softwar host googl cloud server direct access local drive howev colab provid various option connect almost cloud storag imagin either clone entir github repositori colab environ access individu file link
788,"Creating a Simple Module

> We can save a single module_name.py file in the session and call the function inside the module with import statement

Suppose add() is defined in a module adder.py. 

To import and use the add() function

from adder import add
result = add(2, 3)

or
import adder
result = adder.add(2, 3)

** If we just create a blank __init__.py file within a folder and keep the module_name.py files in that folder, the folder becomes a python library (folder name=library name).

** The library has to be in the same directory (session) where it is called with import statement",creat simpl modul,"['creat', 'simpl', 'modul']",Creating a Simple Module,save singl modulenamepi file session call function insid modul import statementsuppos add defin modul adderpi import use add functionfrom adder import add result add2 3or import adder result adderadd2 3 creat blank initpi file within folder keep modulenamepi file folder folder becom python librari folder namelibrari name librari directori session call import statement
789,"Basics of modular programming

A module allows us to logically organise our python code. For example, 

1. UI

2. Training

3. API

> Module is a single python file with set of codes that we can import in python

> Modular Programming Application Layout means packaging or foldering procedure

> Main folder or directory is called a python library

> Subdirectories are called python packages.

> All the .py files either inside the main directory or subdirectories are called python modules (module_name.py).

> Layouts are:
1. One-Off Script, 

2. Installable Package and 

3. App with Internal Packages",basic modular program,"['basic', 'modular', 'program']",Basics of modular programming,modul allow us logic organis python code exampl 1 ui2 training3 api modul singl python file set code import python modular program applic layout mean packag folder procedur main folder directori call python librari subdirectori call python packag py file either insid main directori subdirectori call python modul modulenamepi layout 1 oneoff script 2 instal packag 3 app intern packag
790,"One-Off Script Layout

> Here is only one directory with the application_name. This directory includes following files:

1. .gitignore file (available in github. This ensures that certain file types are never committed to the local Git repository)

2. application_name.py file (this script houses all the main codes)

3. LICENSE file (available in github) 

4. README.md file (a markdown document with purpose and usage of the application)

5. requirements.txt file (lists down dependencies)

6. setup.py file (This script houses all the installation realted codes. This is the most crucial files in the folder directory.)

7. tests.py file (This script houses all the test related codes)",oneoff script layout,"['oneoff', 'script', 'layout']",One-Off Script Layout,one directori applicationnam directori includ follow files1 gitignor file avail github ensur certain file type never commit local git repository2 applicationnamepi file script hous main codes3 licens file avail github 4 readmemd file markdown document purpos usag application5 requirementstxt file list dependencies6 setuppi file script hous instal realt code crucial file folder directory7 testspi file script hous test relat code
791,"Installable Package Layout

The only difference here is that our application code is now all held in  application_name subdirectory. This subdirectory includes following files:

1. application_name.py file (this script houses all the main codes)

2. __init__.py file (It sets up how packages or functions will be imported into our other files)

3. helpers.py file

Tests are written in a separate subdirectory",instal packag layout,"['instal', 'packag', 'layout']",Installable Package Layout,differ applic code held applicationnam subdirectori subdirectori includ follow files1 applicationnamepi file script hous main codes2 initpi file set packag function import files3 helperspi filetest written separ subdirectori
792,"App with Internal Packages Layout

Here the subdirectories are:

1. bin (houses executable file if any.  If our package is a pure python one, there is nothing to put here)

2. docs (houses all the.md files)

3. application_name

4. data

5. tests (module_1_test.py, module_2_test.py etc.)

> Sub-packages may be formed if necessary

> src- This is the folder that contains our python package codes (module_1.py, module_2.py etc.).",app intern packag layout,"['app', 'intern', 'packag', 'layout']",App with Internal Packages Layout,subdirectori are1 bin hous execut file packag pure python one noth put here2 doc hous themd files3 applicationname4 data5 test module1testpi module2testpi etc subpackag may form necessari src folder contain python packag code module1pi module2pi etc
793,"Data Science Project Layout

Data Science Project Layouts can vary depending on what all
components it contains. Some of the components are:

a. Data Engineering Pipelines (airflow / celery / API / SQL)

b. Model Training Pipeline (CLI)

c. Model Serving (API)

d. Model Validation (CLI)

e. Visualization

f. Report Generation",data scienc project layout,"['data', 'scienc', 'project', 'layout']",Data Science Project Layout,data scienc project layout vari depend compon contain compon area data engin pipelin airflow celeri applic program interfac sqlb model train pipelin clic model serv applic program interfac model valid clie visualizationf report generat
794,"Using cookiecutter for packaging

We can either manually do the packaging or can do the same with few libraries like cookiecutter

pip install cookiecutter (can be used both for CLI and colab)

from cookiecutter.main import cookiecutter (for colab)

> Navigate to a directory where we want to create the new project, then run following in CLI

cookiecutter gh:claws/cookiecutter-python-project (we can run this command in colab with ! mark)

> Full form of CLI is command line interface",use cookiecutt packag,"['use', 'cookiecutt', 'packag']",Using cookiecutter for packaging,either manual packag librari like cookiecutterpip instal cookiecutt use cli colabfrom cookiecuttermain import cookiecutt colab navig directori want creat new project run follow clicookiecutt ghclawscookiecutterpythonproject run command colab mark full form cli command line interfac
795,"stdin, stdout and stderr

All applications have three unique streams that connect them to the outside world. These are referred to as Standard Input, or stdin; Standard Output, or stdout; and Standard Error, or stderr.

1. Standard input is the default mechanism for getting input into an interactive program. This is typically a direct link to the keyboard when running directly in a terminal, and isn’t connected to anything otherwise.

2. Standard output is the default mechanism for writing output from a program. This is typically a link to the output terminal but is often buffered for performance reasons. 

3. The standard error is an alternative mechanism for writing output from a program for errors. 

In Python, whenever we use print() the text is written to Python’s sys.stdout, whenever input() is used, it comes from sys.stdin, and whenever exceptions occur it is written to sys.stderr. ",stdin stdout stderr,"['stdin', 'stdout', 'stderr']","stdin, stdout and stderr",applic three uniqu stream connect outsid world refer standard input stdin standard output stdout standard error stderr1 standard input default mechan get input interact program typic direct link keyboard run direct termin isn't connect anyth otherwise2 standard output default mechan write output program typic link output termin often buffer perform reason 3 standard error altern mechan write output program error python whenev use print text written python sysstdout whenev input use come sysstdin whenev except occur written sysstderr
796,"Reading a static data file from inside a Python package

import pkgutil   # provides binary data

from io import StringIO # for binary to high level data conversion

bytes_data = pkgutil.get_data(__name__, ""my_data.csv"")

s=str(bytes_data,'utf-8')
data = StringIO(s) 
my_df_clean=pd.read_csv(data)",read static data file insid python packag,"['read', 'static', 'data', 'file', 'insid', 'python', 'packag']",Reading a static data file from inside a Python package,import pkgutil provid binari datafrom io import stringio binari high level data conversionbytesdata pkgutilgetdatanam mydatacsvsstrbytesdatautf8 data stringio mydfcleanpdreadcsvdata
797,"Code for Creating python library 

 For final packaging or creating distributable version (wheel file) of python model,

> At first do the basic packaging 

creating README.txt, LICENCE.txt, setup.py and __init__.py files

- Always mension the version of dependencies in the setup.py file install_requires

> Run the following code in CLI (powershell/anaconda prompt/gitbash etc.) in the project file where setup.py file is present, not in the package file where __init__.py file is present

pip install wheel (required during first run)

python setup.py sdist bdist_wheel

 # This will create a dist folder with .whl file for uploading in pypi. 

> Then upload as follows

 python -m pip install --upgrade twine (required during first run)

twine upload --repository testpypi dist/* (shall be used for testing)

twine upload dist/* (for actual deployment in pypi.org)

>  In case hanging problem arise during the connection with pypi.org in any CLI, try any other terminal",code creat python librari,"['code', 'creat', 'python', 'librari']",Code for Creating python library ,final packag creat distribut version wheel file python model first basic packag creat readmetxt licencetxt setuppi initpi file alway mension version depend setuppi file installrequir run follow code cli powershellanaconda promptgitbash etc project file setuppi file present packag file initpi file presentpip instal wheel requir first runpython setuppi sdist bdistwheel creat dist folder whl file upload pypi upload follow python pip instal upgrad twine requir first runtwin upload repositori testpypi dist shall use testingtwin upload dist actual deploy pypiorg case hang problem aris connect pypiorg cli tri termin
798,"Testing during packaging of our own python library

Test-1: Initial packaging without setup.py file

> create a folder 'package_name' in the session with __init__.py file

> upload data_file (.csv,.xlsx etc.) file in the session (if needed by the code)

> !pip install libraries # all dependencies

> from package_name import my_function

> call the my_function

Test-2: Installation through setup.py file

> create a folder 'package_name' in the session with __init__.py file

> upload README, LICENSE and setup.py file in the session

> upload data_file (.csv,.xlsx etc.) file in the session (if needed by the code)

> !pip install package_name # Installing our own python library

> from package_name import my_function

> call the my_function

Test-3: Installation through wheel file

Must contain __init__.py file and data_file (.csv,.xlsx etc.)

> upload the wheel file in the session

> Checking files inside the source distribution file

import tarfile

tar = tarfile.open('my_package-0.0.1.tar.gz')

tar.getnames() 

> Checking files inside final distribution file or wheel file

import pprint

from zipfile import ZipFile

path = 'my_package-0.0.1-py3-none-any.whl'

names = ZipFile(path).namelist()
pprint.pprint(names)

> !pip install package_name-0.0.1-py3-none-any.whl

> from package_name import my_function

> call the my_function

Test-4: Installation through testpypi

> !pip install -i https://test.pypi.org/simple/ package_name

> from package_name import my_function

> call the my_function

Test-5: Installation through pypi (final testing)

> !pip install package_name

> from package_name import my_function

> call the my_function",test packag python librari,"['test', 'packag', 'python', 'librari']",Testing during packaging of our own python library,test1 initi packag without setuppi file creat folder packagenam session initpi file upload datafil csvxlsx etc file session need code pip instal librari depend packagenam import myfunct call myfunctiontest2 instal setuppi file creat folder packagenam session initpi file upload readm licens setuppi file session upload datafil csvxlsx etc file session need code pip instal packagenam instal python librari packagenam import myfunct call myfunctiontest3 instal wheel filemust contain initpi file datafil csvxlsx etc upload wheel file session check file insid sourc distribut fileimport tarfiletar tarfileopenmypackage001targztargetnam check file insid final distribut file wheel fileimport pprintfrom zipfil import zipfilepath mypackage001py3noneanywhlnam zipfilepathnamelist pprintpprintnam pip instal packagename001py3noneanywhl packagenam import myfunct call myfunctiontest4 instal testpypi pip instal httpstestpypiorgsimpl packagenam packagenam import myfunct call myfunctiontest5 instal pypi final test pip instal packagenam packagenam import myfunct call myfunct
799,"CVCS and DVCS

Version control allows us to keep track of our work and helps us to easily explore the changes we have made, be it data, coding scripts, notes, etc.

Manual version control with time stamps in the folder is prone to error. That is why Centralised Version Control System (CVCS) and Distributed Version Control System (DVCS) which keeps the track of file history.

> CVCS is old technology. There is a single “central” copy of the project somewhere (probably on a server), and programmers will “commit” their changes to this central copy. Example: SVN (SubVersioN)

> DVCS is most popular because of empowering offline work by cloning the repository. Example: Git

> There aren’t really revision numbers in DVCS

> Every repo has its own change numbers depending on the changes.

> We can tag releases with meaningful names.

Git bash+Github together make a DVCS.",central version control system distribut version control system,"['central', 'version', 'control', 'system', 'distribut', 'version', 'control', 'system']",CVCS and DVCS,version control allow us keep track work help us easili explor chang made data code script note etcmanu version control time stamp folder prone error centralis version control system central version control system distribut version control system distribut version control system keep track file histori central version control system old technolog singl “central” copi project somewher probabl server programm “commit” chang central copi exampl svn subvers distribut version control system popular empow offlin work clone repositori exampl git aren't realli revis number distribut version control system everi repo chang number depend chang tag releas meaning namesgit bashgithub togeth make distribut version control system
800,"GitHub workflow

The GitHub workflow can be summarised by the “commit-pull-push” mantra.

1. Commit-

Once we’ve saved our files, we need to commit them - this means the changes we have made to files in our repo will be saved as a version of the repo

2. Pull (git pull)-

Now, before we send our changes to Github, we need to pull, i.e. make sure we are completely up to date with the latest version

> git fetch is the command that tells your local git to retrieve the latest meta-data info from the original (yet doesn't do any file transferring. It's more like just checking to see if there are any changes available). git pull on the other hand does that AND brings (copy) those changes from the remote repository.

> git clone is used to set up a local repository whereas git pull is used to sync remote and local repositories.

3. Push (git push)-

Once we are up to date, we can push our changes ",github workflow,"['github', 'workflow']",GitHub workflow,github workflow summaris “commitpullpush” mantra1 commitonc we'v save file need commit mean chang made file repo save version repo2 pull git pullnow send chang github need pull ie make sure complet date latest version git fetch command tell local git retriev latest metadata info origin yet doesnt file transfer like check see chang avail git pull hand bring copi chang remot repositori git clone use set local repositori wherea git pull use sync remot local repositories3 push git pushonc date push chang
801,"Steps of creating local git repository

1. mkdir my_project

2. cd my_project

3. git init (initializes a hidden git repository which tracks all changes)

4. Copy all the project files here

5. git add files_or_folder_name

6. git status

7. git config --global user.name 'user name'

8. git config --global user.email 'emailid'

9. git commit file_name -m 'my_message'

10. git log -to see the log of the history (git log -n to show limited number of commits)

Git repository never forgets its history. So, we can restore any deleted file or older version file.

git reset commitID",step creat local git repositori,"['step', 'creat', 'local', 'git', 'repositori']",Steps of creating local git repository,1 mkdir myproject2 cd myproject3 git init initi hidden git repositori track changes4 copi project file here5 git add filesorfoldername6 git status7 git config global usernam user name8 git config global useremail emailid9 git commit filenam mymessage10 git log see log histori git log n show limit number commitsgit repositori never forget histori restor delet file older version filegit reset commitid
802,"Concept of Branch

Master branch (for releasing)- ready to deploy codes

Branch (developer/feature branch)- experimentals codes

> Git commands for creating, activating and entering into the branch

git branch branch_name

git checkout branch_name

git branch",concept branch,"['concept', 'branch']",Concept of Branch,master branch releas readi deploy codesbranch developerfeatur branch experiment code git command creat activ enter branchgit branch branchnamegit checkout branchnamegit branch
803,"Synchronising local git repository with Github (remote)

git remote add origin github_repository_address

git push --set-upstream origin master",synchronis local git repositori github remot,"['synchronis', 'local', 'git', 'repositori', 'github', 'remot']",Synchronising local git repository with Github (remote),git remot add origin githubrepositoryaddressgit push setupstream origin master
804,"Merging operation in git

A merge happens when combining two branches. Git will take two (or more) commit pointers and attempt to find a common base commit between them. Once Git finds a common base commit it will create a new ""merge commit"" that combines the changes of the specified merge commits.

Followings commands are used in git merging:

git merge -s resolve

git merge -s ours

git merge -s octopus

> Rebasing is an alternative to merging 

> When multiple people want to commit code file with different changes, in the same workflow, then merge conflict occurs.",merg oper git,"['merg', 'oper', 'git']",Merging operation in git,merg happen combin two branch git take two commit pointer attempt find common base commit git find common base commit creat new merg commit combin chang specifi merg commitsfollow command use git merginggit merg resolvegit merg oursgit merg octopus rebas altern merg multipl peopl want commit code file differ chang workflow merg conflict occur
805,"Continuous Integration/ Continuous Deployment or Delivery (CI/CD)

Through github Actions we can build, test and deploy our codes continuously or regularly.

> Concept of master branch and branch introduced for this purpose",continu integr continu deploy deliveri cicd,"['continu', 'integr', 'continu', 'deploy', 'deliveri', 'cicd']",Continuous Integration/ Continuous Deployment or Delivery (CI/CD),github action build test deploy code continu regular concept master branch branch introduc purpos
806,"Understanding API

> An API (Application Programming Interface) is a set of functions that allows applications to access data
and interact with external software components, operating systems, or microservices.

> Application means the production deployment of software development. There are two types of Apps: Mobile Apps and Web Apps

> API delivers a user request to a system and sends the system’s response back to a user. Thus, API makes a web page dynamic (web app)

Popular Examples

• YouTube API

• Twitter API

• Facebook API

• Microsoft 365 Graph API",understand applic program interfac,"['understand', 'applic', 'program', 'interfac']",Understanding API,applic program interfac applic program interfac set function allow applic access data interact extern softwar compon oper system microservic applic mean product deploy softwar develop two type app mobil app web app applic program interfac deliv user request system send system respons back user thus applic program interfac make web page dynam web apppopular examples• youtub applic program interface• twitter applic program interface• facebook applic program interface• microsoft 365 graph applic program interfac
807,"Types of web pages

1. Static webpage (can be created using only html or html+css). It is also called website

2. Dynamic webpage (can be created using html+ flask/JavaScript). This is also called web app

> A website provides visual and text content that users can view and read.  Web application is designed for interaction with end users.

HTML tags-These are added to web documents to control their appearance

HTTP- The rules governing the conversion between a web client and a web server

> Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.",type web page,"['type', 'web', 'page']",Types of web pages,1 static webpag creat use html htmlcss also call website2 dynam webpag creat use html flaskjavascript also call web app websit provid visual text content user view read web applic design interact end usershtml tagsthes ad web document control appearancehttp rule govern convers web client web server web scrape web harvest web data extract data scrape use extract data websit
808,"Machine Learning Services

All Machine Learning Services are available using API Interface:

○ Azure Cognitive Services

○ Google Cloud AI and Machine Learning Services

○ AWS AI Services",machin learn servic,"['machin', 'learn', 'servic']",Machine Learning Services,machin learn servic avartifici intelligencel use applic program interfac interface○ azur cognit services○ googl cloud artifici intellig machin learn services○ aw artifici intellig servic
809,"Types of API (Package for creating Apps)

SOAP API -Simple Object Access Protocol, XML based protocol, heavy weight

REST API- Representational State Transfer, HTTP based protocol, light weight",type applic program interfac packag creat app,"['type', 'applic', 'program', 'interfac', 'packag', 'creat', 'app']",Types of API (Package for creating Apps),soap applic program interfac simpl object access protocol xml base protocol heavi weightrest applic program interfac represent state transfer http base protocol light weight
810,"Basics of Flask

Flask is a python based framework (microframework) to build web application. 

> It is a very light weight, REST API

> It is based on Werkzeug and Jinja2

> Flask supports database powered application (RDBMS)

Django is a similar web api python package like flask. Flask provides simplicity, flexibility and fine-grained control. Flask is easy to learn and mainly used for learning purpose.

Django is used when we are focused on the final product. Its framework structure is more conventional.
",basic flask,"['basic', 'flask']",Basics of Flask,flask python base framework microframework build web applic light weight rest applic program interfac base werkzeug jinja2 flask support databas power applic rdbmsdjango similar web applic program interfac python packag like flask flask provid simplic flexibl finegrain control flask easi learn main use learn purposedjango use focus final product framework structur convent
811,"Benefits of using the Flask framework

1. It supports Unicode.

2. It has an inbuilt development server.

3. It has a tiny API and can be quickly learned by a web developer.

4. It has vast third-party extensions",benefit use flask framework,"['benefit', 'use', 'flask', 'framework']",Benefits of using the Flask framework,1 support unicode2 inbuilt develop server3 tini applic program interfac quick learn web developer4 vast thirdparti extens
812,"Popular HTTP Requests

1. GET – Gathers information (Pulling all Coupon Codes)

2. PUT –  Updates pieces of data (e.g. Updating Product pricing)

3. POST – Creates (e.g. Creating a new Product Category)

4. DELETE – (e.g. Deleting a blog post)

We can send api request through postman like app or directly through python interpreter with following command:

import requests

response=requests.get('https://api.github.com')

response.text

-Requests library  allows us to send HTTP/1.1 requests using Python
",popular http request,"['popular', 'http', 'request']",Popular HTTP Requests,1 get – gather inform pull coupon codes2 put – updat piec data eg updat product pricing3 post – creat eg creat new product category4 delet – eg delet blog postw send applic program interfac request postman like app direct python interpret follow commandimport requestsresponserequestsgethttpsappl program interfacegithubcomresponsetextrequest librari allow us send http11 request use python
813,"Use of Virtual Environment

A virtual environment is a tool that helps to keep dependencies required by different projects separate by creating isolated python virtual environments for them. 

As a best practice, create a virtual environment for any new project and install packages (dependencies) as required through pip install command

",use virtual environ,"['use', 'virtual', 'environ']",Use of Virtual Environment,virtual environ tool help keep depend requir differ project separ creat isol python virtual environ best practic creat virtual environ new project instal packag depend requir pip instal command
814,"Creating a virtual environment

1. Creating a virtual environment in local machine (through CLI)

> To create a virtual environment, python must be installed in that computer.

> make a project directory, enter into that directory and then run following command

python -m venv env

> Activate the environment

source env/Scripts/activate

env/Scripts/activate (powershell command)

> Deactivate the environment

deactivate

2. Creating a virtual environment in EC2 instance (through CLI)

> Here python3 is installed, 

> make a project directory, enter into that directory and then run the following command

python3 -m venv env

> Then activate the environment by sourcing the activate file in the  directory.

source env/bin/activate",creat virtual environ,"['creat', 'virtual', 'environ']",Creating a virtual environment,1 creat virtual environ local machin cli creat virtual environ python must instal comput make project directori enter directori run follow commandpython venv env activ environmentsourc envscriptsactivateenvscriptsactiv powershel command deactiv environmentdeactivate2 creat virtual environ ec2 instanc cli python3 instal make project directori enter directori run follow commandpython3 venv env activ environ sourc activ file directorysourc envbinactiv
815,"Understanding pip install

pip install package_name

pip stands for ""preferred installer program"".

This is the Python Package Installer (also used for uninstalling)

Usually, pip is automatically installed if we are working in a virtual environment.

pip show package_name (to see the version of package installed)

> colab notebook is a python virtual environment, so we can use pip without creating a virtual environment

If the project directory has setup.py file, that means we can install the package.

1. For installing a static package, as a tester,

pip install package_name

2. For installing an editable package, as a developer (keep the  setup.py and README.txt, LICENCE.txt files in the session),

pip install -e .

This will install the dependencies
or 

Direct installation from wheel file

pip install package_name-0.0.1-py3-none-any.whl",understand pip instal,"['understand', 'pip', 'instal']",Understanding pip install,pip instal packagenamepip stand prefer instal programthi python packag instal also use uninstallingusu pip automat instal work virtual environmentpip show packagenam see version packag instal colab notebook python virtual environ use pip without creat virtual environmentif project directori setuppi file mean instal package1 instal static packag testerpip instal packagename2 instal edit packag develop keep setuppi readmetxt licencetxt file sessionpip instal e instal depend direct instal wheel filepip instal packagename001py3noneanywhl
816,"Creating a Web App in Flask

> after activating virtual environment, enter into the project directory and run following command

!pip install flask

> write the code for the Web App in app.py file and save in the project directory, 

from flask import Flask, redirect, url_for, render_template, request

app=Flask(__name__)
@app.route('/')
def home_page():
    return 'prediction=200'
app.run() # gives the link for the webapp

> We can integrate html pages in the above coding. Then we need to return render_template('my_file.html')

> url_for -used for dynamic url generation

if total_score>=50:
res='success'
else:
res='fail'
return redirect(url_for(res,score=total_score))

> Creating a dynamic button

@app.route('/submit', method= ['POST','GET'])

> Adding mailing feature in the Flask Application

pip install Flask-Mail

> Flask default port is 5000

> Flask default host is a localhost (127.0.0.1)

> FastAPI default port is 8000",creat web app flask,"['creat', 'web', 'app', 'flask']",Creating a Web App in Flask,activ virtual environ enter project directori run follow commandpip instal flask write code web app apppi file save project directori flask import flask redirect urlfor rendertempl requestappflasknam approut def homepag return prediction200 apprun give link webapp integr html page code need return rendertemplatemyfilehtml urlfor use dynam url generationif totalscore50 ressuccess els resfail return redirecturlforresscoretotalscor creat dynam buttonapproutesubmit method postget ad mail featur flask applicationpip instal flaskmail flask default port 5000 flask default host localhost 127001 fastapi default port 8000
817,"Jinja techniques

> Jinja tag is written as 

{% ... %} -For any statement
{{  }} -expressions to print output
{#...#} -for internal comments

> Jinja techniques are used to integrate dynamic coding within html",jinja techniqu,"['jinja', 'techniqu']",Jinja techniques,jinja tag written statement express print output intern comment jinja techniqu use integr dynam code within html
818,"Ways to edit script code (.py file) in Colab notebook (alternative to vs code)

> Make a copy the script file in google drive

> Mount drive with colab

> !cat 'file_path'

> copy the output code and paste in a new colab file and start editing

> Then, download the .py file

> Colab notebook is a development server, we should not use it in making app.

> VS Code desktop app is very useful for offline editing of python files",way edit script code py file colab notebook altern vs code,"['way', 'edit', 'script', 'code', 'py', 'file', 'colab', 'notebook', 'altern', 'vs', 'code']",Ways to edit script code (.py file) in Colab notebook (alternative to vs code),make copi script file googl drive mount drive colab cat filepath copi output code past new colab file start edit download py file colab notebook develop server use make app vs code desktop app use offlin edit python file
819,"Common file formats used in Data Science:

.csv

.txt

.json

.xlsx

.sas

.sql

.pickle

.dta

.h5

.xml

.html

.zip

.pdf

.docx

.jpeg, .png, .gif etc.",common file format use data scienc,"['common', 'file', 'format', 'use', 'data', 'scienc']",Common file formats used in Data Science:,csvtxtjsonxlsxsassqlpickledtah5xmlhtmlzippdfdocxjpeg png gif etc
820,"Flaskr as a basic blog application

Flaskr is a basic blog application inside Flask package. Here, users are be able to register, log in, create posts, and edit or delete their own posts (basic five actions).

Flaskr directory has following files and sub-directories:

1. flask app object defined in __init__.py

2. table intilization in db.py

3. table structure is defined in schema.sql

4. user authentication is defined in auth.py

5. creating, editing or deleting a post is defined in blog.py

6. template sub-directory contains all the .html template files

7. static directory contains all the static .css files",flaskr basic blog applic,"['flaskr', 'basic', 'blog', 'applic']",Flaskr as a basic blog application,flaskr basic blog applic insid flask packag user abl regist log creat post edit delet post basic five actionsflaskr directori follow file subdirectories1 flask app object defin initpy2 tabl intil dbpy3 tabl structur defin schemasql4 user authent defin authpy5 creat edit delet post defin blogpy6 templat subdirectori contain html templat files7 static directori contain static css file
821,"Hyper Text Markup Language

The Hyper Text Markup Language, or HTML is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by  Cascading Style Sheets (CSS) and JavaScript.
We can create static html page as follows:

> we can simply use notepad (or vs code) and write the code and save the file with file_name.html or we can copy the html source code and paste it in notepad/ vs code and start editing

Template inheritance allows us to build a base “skeleton” template that contains all the common elements of our site and defines blocks that child templates can override.",hyper text markup languag,"['hyper', 'text', 'markup', 'languag']",Hyper Text Markup Language,hyper text markup languag html standard markup languag document design display web browser assist cascad style sheet css javascript creat static html page follow simpli use notepad vs code write code save file filenamehtml copi html sourc code past notepad vs code start editingtempl inherit allow us build base “skeleton” templat contain common element site defin block child templat overrid
822,"Understanding FastAPI

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.

FastAPI is similar to Flask but FastAPI is really fast and very usefull for ml engineers to present their work in a working way through ml app.

> after activating virtual environment, enter into the project directory

pip install fastapi[all]
pip install uvicorn[standard]

> write the code for the Web App in app.py file and save in the project directory, 
example python code:

from fastapi import FastAPI
app=FastAPI()
@app.get('/')
async def root():
    return 'prediction=200'

> Then run the server with

uvicorn app:app --reload # app(is the file name):app(is the object name)",understand fastapi,"['understand', 'fastapi']",Understanding FastAPI,fastapi modern fast highperform web framework build api python 36 base standard python type hintsfastapi similar flask fastapi realli fast useful machin learn engin present work work way machin learn app activ virtual environ enter project directorypip instal fastapial pip instal uvicornstandard write code web app apppi file save project directori exampl python codefrom fastapi import fastapi appfastapi appget async def root return prediction200 run server withuvicorn appapp reload appi file nameappi object name
823,"WSGI vs ASGI

WSGI stands for Web Server Gateway Interface. It is synchronous. Fask, Django uses WSGI

ASGI stands for Asynchronous Server Gateway Interface. It is asynchronous. FastAPI use ASGI",web server gateway interfac vs asynchron server gateway interfac,"['web', 'server', 'gateway', 'interfac', 'vs', 'asynchron', 'server', 'gateway', 'interfac']",WSGI vs ASGI,web server gateway interfac stand web server gateway interfac synchron fask django use web server gateway interfaceasynchron server gateway interfac stand asynchron server gateway interfac asynchron fastapi use asynchron server gateway interfac
824,"Features of FastAPI

● Interactive API Docs (Swagger UI)

url/docs # url stands for Uniform Resource Locator

● Data Type Validation",featur fastapi,"['featur', 'fastapi']",Features of FastAPI,● interact applic program interfac doc swagger uiurldoc url stand uniform resourc locator● data type valid
825,"Pydantic Data Model

from pydantic import BaseModel

class Item(BaseModel)

name:str
description: Optional[str] = None

price: float

tax: Optional[float] = None

> pydantic guarantees the types and constraints of the output model, not the input data",pydant data model,"['pydant', 'data', 'model']",Pydantic Data Model,pydant import basemodelclass itembasemodelnamestr descript optionalstr nonepric floattax optionalfloat none pydant guarante type constraint output model input data
826,"FastAPI Working with SQL (RDBMS)

> FastAPI works with any database and any style of library to talk to the database.

> A common pattern is to use an ""ORM"": an ""object-relational mapping"" library. e.g. SQLAlchemy",fastapi work structur queri languag rdbms,"['fastapi', 'work', 'structur', 'queri', 'languag', 'rdbms']",FastAPI Working with SQL (RDBMS),fastapi work databas style librari talk databas common pattern use orm objectrel map librari eg sqlalchemi
827,"VS Code for python coding

> Open VS Code and then open a folder (newly created)  in any location of the computer

> Then inside that workspace (folder), create a file with .py extension

> Write or copy paste the code

> Create and activate a virtual environment in that folder and pip install all the required packages through gitbash

> from command palette> select python interpreter> select recommended virtual env

> right click on the code and click Run python file in terminal

>> From view tab> terminal (Ctrl+`), we can open the CLI, here we can write all the git/terminal commands

> Different extensions are used in vs code to give us multiple extra features. We can also build vs code extension and contribute",vs code python code,"['vs', 'code', 'python', 'code']",VS Code for python coding,open vs code open folder newli creat locat comput insid workspac folder creat file py extens write copi past code creat activ virtual environ folder pip instal requir packag gitbash command palett select python interpret select recommend virtual env right click code click run python file termin view tab termin ctrl open cli write gittermin command differ extens use vs code give us multipl extra featur also build vs code extens contribut
828,"Ways to use python in local machine through git bash (when anaconda installed)

conda init bash 
(in anaconda prompt)

python --version
(checking the version of python)

ipython

exit()

",way use python local machin git bash anaconda instal,"['way', 'use', 'python', 'local', 'machin', 'git', 'bash', 'anaconda', 'instal']",Ways to use python in local machine through git bash (when anaconda installed),conda init bash anaconda promptpython version check version pythonipythonexit
829,"Docker Basics

Solomon Hykes developed Docker in 2013

Docker is the company which is pioneer in container technology. So container and docker is used interchangeably

Docker is standardized packaging for software and dependencies

This is the most popular packaging technique and very light weight

Docker file helps in packaging our project as a container.  ",docker basic,"['docker', 'basic']",Docker Basics,solomon hyke develop docker 2013docker compani pioneer contain technolog contain docker use interchangeablydock standard packag softwar dependenciesthi popular packag techniqu light weightdock file help packag project contain
830,"Image or Docker Image

Similar like setup.py which creates a wheel file during normal project packaging, Docker file is a text document containing few lines of codes (command line) for creating a docker image.

There are mainly six commands (FROM, COPY, EXPOSE,WORKDIR,RUN,CMD) inside a docker file. 

EXPOSE informs docker that container listens on specified network ports at runtime

CMD provides default for an executing container viz. an executable and an ENTRYPOINT instruction as well

Image has the task to create a container.

We can create multiple containers from the same image

It is a read only binary file which defines all the steps for running the app including instruction for installling  packages and libraries as a dependencies of the application. 

Docker file-> Docker image-> Docker Container-> Installation of Packages and libraries",imag docker imag,"['imag', 'docker', 'imag']",Image or Docker Image,similar like setuppi creat wheel file normal project packag docker file text document contain line code command line creat docker imagether main six command copi exposeworkdirruncmd insid docker file expos inform docker contain listen specifi network port runtimecmd provid default execut contain viz execut entrypoint instruct wellimag task creat containerw creat multipl contain imageit read binari file defin step run app includ instruct install packag librari depend applic docker file docker imag docker contain instal packag librari
831,"Container or Docker Container

It is the running instance of the docker image

Container is similar as virtual machine (instance) but very different from VM.

Instance is heavy weight but container is very light weight.

In one heavy instance with high RAM, high processor or core, high storage and docker installed, we can run multiple applications in multiple dockers.

All the docker containers share same resources (i.e computational resources like OS kernel, RAM, CPU and Hard disk)

Thus docker container creates boundaries in the instance for separating multiple applications",contain docker contain,"['contain', 'docker', 'contain']",Container or Docker Container,run instanc docker imagecontain similar virtual machin instanc differ vminstanc heavi weight contain light weightin one heavi instanc high ram high processor core high storag docker instal run multipl applic multipl dockersal docker contain share resourc ie comput resourc like oper system kernel ram central process unit hard diskthus docker contain creat boundari instanc separ multipl applic
832,"Introduction to dockerhub

Just like github there is dockerhub for shipping docker image for the application

The easiest way to make our images available for use by others inside or outside any organization is to use a Docker registry, such as Docker Hub, or by running our own private registry. 

> Docker registry uses port 5000

> Docker Hub allows only one private repository

> Docker engine is the docker software (an open source containerisation technology)",introduct dockerhub,"['introduct', 'dockerhub']",Introduction to dockerhub,like github dockerhub ship docker imag applicationth easiest way make imag avail use other insid outsid organ use docker registri docker hub run privat registri docker registri use port 5000 docker hub allow one privat repositori docker engin docker softwar open sourc containeris technolog
833,"Ways to install docker in EC2 instance

> Before installing, check if the docker is already installed

docker --version

> Then run the following commands

sudo yum update -y

sudo amazon-linux-extras install docker

sudo service docker start

sudo usermod -a -G docker ec2-user",way instal docker ec2 instanc,"['way', 'instal', 'docker', 'ec2', 'instanc']",Ways to install docker in EC2 instance,instal check docker alreadi installeddock version run follow commandssudo yum updat ysudo amazonlinuxextra instal dockersudo servic docker startsudo usermod g docker ec2us
834,"Basic Docker Commands

$ docker pull python:3.7

$ docker images

$ docker run –d –p 5000:5000 –name node node:latest

$ sudo docker run -i -t alpine /bin/bash (for running images as container)

$ docker ps -a

$ docker stop container_id (or docker kill)

$ docker rm container_ id (for removing the containers)

$ docker rmi image_id (for removing the images)

$ docker build –t node:2.0

$ docker push node:2.0

$ docker ps (to see all running container in Docker)

$ docker --help

> A node is a device or data point in a larger network.

> Multiple nodes means that there are multiple virtual machines.",basic docker command,"['basic', 'docker', 'command']",Basic Docker Commands,docker pull python37 docker imag docker run –d –p 50005000 –name node nodelatest sudo docker run alpin binbash run imag contain docker ps docker stop containerid docker kill docker rm contain id remov contain docker rmi imageid remov imag docker build –t node20 docker push node20 docker ps see run contain docker docker help node devic data point larger network multipl node mean multipl virtual machin
835,"Docker or Container volume

> Volumes mount a directory on the host into the container at a specific location

> Mount local source code into a running container",docker contain volum,"['docker', 'contain', 'volum']",Docker or Container volume,volum mount directori host contain specif locat mount local sourc code run contain
836,"Docker Bridge Networking and Port Mapping

 docker container run -p 8080:80 

# 8080 (host_port):80(container_port)",docker bridg network port map,"['docker', 'bridg', 'network', 'port', 'map']",Docker Bridge Networking and Port Mapping,docker contain run p 808080 8080 hostport80containerport
837,"Docker Compose

Docker Compose is a tool that was developed to help define and share multi-container applications. With Compose, we can create a YAML file to define the services and with a single command, can spin everything up or tear it all down.",docker compos,"['docker', 'compos']",Docker Compose,docker compos tool develop help defin share multicontain applic compos creat yaml file defin servic singl command spin everyth tear
838,"Docker swarm

Docker swarm allows the user to manage multiple containers deployed across multiple host machines (native clustering). ",docker swarm,"['docker', 'swarm']",Docker swarm,docker swarm allow user manag multipl contain deploy across multipl host machin nativ cluster
839,"Understanding Microservices

Microservices are used when the application is huge (generally for e-commerce application)

Earlier there were monolithic applications implemented using a single development stack

Microservices is the architecture of an entire software project. ML application is only one part of that main application (main service). 

In microservices architecture, all the part applications are loosely coupled, so their deployment schedule can be independent.

Loosely coupled means, all the part applications are individually complete application and they interact with each other through api service

All the micro services don't need to share the same technology stack, libraries, or frameworks

> The knowledge of microservices is important for ML engineer, not that much important for a data scientist",understand microservic,"['understand', 'microservic']",Understanding Microservices,microservic use applic huge general ecommerc applicationearli monolith applic implement use singl develop stackmicroservic architectur entir softwar project machin learn applic one part main applic main servic microservic architectur part applic loos coupl deploy schedul independentloos coupl mean part applic individu complet applic interact applic program interfac serviceal micro servic dont need share technolog stack librari framework knowledg microservic import machin learn engin much import data scientist
840,"API Gateway

In microservice architecture, all the part applications do not directly interact with each other. They interact with each other through a coordinator, called API Gateway",applic program interfac gateway,"['applic', 'program', 'interfac', 'gateway']",API Gateway,microservic architectur part applic direct interact interact coordin call applic program interfac gateway
841,"Development of Microservices

Use domain analysis to define our microservice boundaries. 

In this stage we list down the requirement of api's for every part application. 

Here, we actually understand the inputs and outputs of every part application and their interrelation.

> Domain means the part applications or microservices

> Design service includes Scheduler service",develop microservic,"['develop', 'microservic']",Development of Microservices,use domain analysi defin microservic boundari stage list requir api everi part applic actual understand input output everi part applic interrel domain mean part applic microservic design servic includ schedul servic
842,"Kubernetes

Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It uses Docker

It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation.",kubernet,['kubernet'],Kubernetes,kubernet also known k8s opensourc system autom deploy scale manag container applic use dockerit origin design googl maintain cloud nativ comput foundat
843,"Python code for creating a Streamlit file

Streamlit is a python library. The fastest way to build Web App

# import module

import streamlit as st # All the text cell will be displayed after this import statement

# Title

st.title(""Hello GeeksForGeeks !!!"")

# Header

st.header(""This is a header"")

# Subheader

st.subheader(""This is a subheader"")

# Text

st.text(""Hello GeeksForGeeks!!!"")

# Markdown

st.markdown(""### This is a markdown"")

# success

st.success(""Success"")

# success

st.info(""Information"")

# success

st.warning(""Warning"")

# success

st.error(""Error"")

# Write text

st.write(""Text with write"")

# Writing python inbuilt function 

range()
st.write(range(10))

# Display Images

# import Image from pillow to open images

from PIL import Image

img = Image.open(""streamlit.png"")

# display image using streamlit

# width is used to set the width of an image

st.image(img, width=200)

# checkbox

# check if the checkbox is checked

# title of the checkbox is 'Show/Hide'
if st.checkbox(""Show/Hide""):

# display the text if the checkbox returns True value

st.text(""Showing the widget"")

# radio button

# first argument is the title of the radio button

# second argument is the options for the ratio button

status = st.radio(""Select Gender: "", ('Male', 'Female'))

# conditional statement to print

# Male if male is selected else print female

# show the result using the success function

if (status == 'Male'):
 st.success(""Male"")
else:
 st.success(""Female"")

# Selection box

# first argument takes the titleof the selectionbox

# second argument takes options

hobby = st.selectbox(""Hobbies: "",
     ['Dancing', 'Reading', 'Sports'])

# print the selected hobby

st.write(""Your hobby is: "", hobby)

# multi select box

# first argument takes the box title

# second argument takes the options to show

hobbies = st.multiselect(""Hobbies: "",
      ['Dancing', 'Reading', 'Sports'])

# write the selected options

st.write(""You selected"", len(hobbies), 'hobbies')

# Create a simple button that does nothing

st.button(""Click me for no reason"")

# Create a button, that when clicked, shows a text

if(st.button(""About"")):
 st.text(""Welcome To GeeksForGeeks!!!"")

# Text Input

# save the input text in the variable 'name'

# first argument shows the title of the text input box

# second argument displays a default text inside the text input area

name = st.text_input(""Enter Your name"", ""Type Here ..."")

# display the name when the submit button is clicked

# .title() is used to get the input text string

if(st.button('Submit')):
 result = name.title()
 st.success(result)

# slider

# first argument takes the title of the slider

# second argument takes the starting of the slider

# last argument takes the end number

level = st.slider(""Select the level"", 1, 5)

# print the level

# format() is used to print value

# of a variable at a specific position

st.text('Selected: {}'.format(level))

> streamlit is for demo development not for api development.

> Not only for demo but streamlit is also very useful for our ml model validation

> Using streamlit we can deploy any machine learning model and any python project with ease and without worrying about the frontend
",python code creat streamlit file,"['python', 'code', 'creat', 'streamlit', 'file']",Python code for creating a Streamlit file,streamachin learningit python librari fastest way build web app import moduleimport streamachin learningit st text cell display import statement titlesttitlehello geeksforgeek headerstheaderthi header subheaderstsubheaderthi subhead textsttexthello geeksforgeek markdownstmarkdown markdown successstsuccesssuccess successstinfoinform successstwarningwarn successsterrorerror write textstwritetext write write python inbuilt function rang stwriterange10 display imag import imag pillow open imagesfrom pil import imageimg imageopenstreamachin learningitpng display imag use streamachin learningit width use set width imagestimageimg width200 checkbox check checkbox check titl checkbox showhid stcheckboxshowhid display text checkbox return true valuesttextshow widget radio button first argument titl radio button second argument option ratio buttonstatus stradioselect gender male femal condit statement print male male select els print femal show result use success functionif status male stsuccessmal els stsuccessfemal select box first argument take titleof selectionbox second argument take optionshobbi stselectboxhobbi danc read sport print select hobbystwriteyour hobbi hobbi multi select box first argument take box titl second argument take option showhobbi stmultiselecthobbi danc read sport write select optionsstwriteyou select lenhobbi hobbi creat simpl button nothingstbuttonclick reason creat button click show textifstbuttonabout sttextwelcom geeksforgeek text input save input text variabl name first argument show titl text input box second argument display default text insid text input areanam sttextinputent name type display name submit button click titl use get input text stringifstbuttonsubmit result nametitl stsuccessresult slider first argument take titl slider second argument take start slider last argument take end numberlevel stsliderselect level 1 5 print level format use print valu variabl specif positionsttextselect formatlevel streamachin learningit demo develop applic program interfac develop demo streamachin learningit also use machin learn model valid use streamachin learningit deploy machin learn model python project eas without worri frontend
844,"Creating a file for streamlit inside the session

%%writefile file_name.py

import streamlit as st

st.title('The Taste of My Mango')

!pip install streamlit

!streamlit run file_name.py",creat file streamlit insid session,"['creat', 'file', 'streamlit', 'insid', 'session']",Creating a file for streamlit inside the session,writefil filenamepyimport streamlit ststtitleth tast mangopip instal streamlitstreamlit run filenamepi
845,"Creating a Streamlit Project

> Commit the following code as app.py along with requirements.txt, README and data file (may also be accessed via url) in github and deploy the demo app in https://share.streamlit.io/

from drona import text_process

import streamlit as st 

st.title(""Welcome to 'drona' text_process"")

text = st.text_input(""Provide your text"")

text=text.title() # .title() is used to get the input string

ans = text_process(text)

if(st.button('Submit')):   # display the processed text when the submit button is clicked
  st.success(ans)",creat streamlit project,"['creat', 'streamlit', 'project']",Creating a Streamlit Project,commit follow code apppi along requirementstxt readm data file may also access via url github deploy demo app httpssharestreamlitiofrom drona import textprocessimport streamlit st sttitlewelcom drona textprocesstext sttextinputprovid texttexttexttitl titl use get input stringan textprocesstextifstbuttonsubmit display process text submit button click stsuccessan
846,"Computer Networking

In computer networking, a port is a communication endpoint. At the software level, within an operating system, a port is a logical construct that identifies a specific process or a type of network service. A port is identified for each transport protocol and address combination by a 16-bit unsigned number, known as the port number. 

> A port number is always associated with an IP address of a host and the type of transport protocol used for communication

192.168.0.1:80 (Socket address)

192.168.0.1 (IP address)

80 (port number)",comput network,"['comput', 'network']",Computer Networking,comput network port communic endpoint softwar level within oper system port logic construct identifi specif process type network servic port identifi transport protocol address combin 16bit unsign number known port number port number alway associ ip address host type transport protocol use communication1921680180 socket address19216801 ip address80 port number
847,"Deploying streamlit app in Google Cloud

> At first we need to create an account in GCP.

> It needs following files:

1. app.py

2. requirements.txt

3. README (md or txt file)

4. Dockerfile (available in github)

> Command to build the application (change the ProjectID and AppName):

ProjectID needs to be copied from gcp console

gcloud builds submit --tag gcr.io/<ProjectID>/<AppName>  --project=<ProjectID>

> Command to deploy the application:

gcloud run deploy --image gcr.io/<ProjectID>/<AppName> --platform managed  --project=<ProjectID> --allow-unauthenticated",deploy streamlit app googl cloud,"['deploy', 'streamlit', 'app', 'googl', 'cloud']",Deploying streamlit app in Google Cloud,first need creat account gcp need follow files1 apppy2 requirementstxt3 readm md txt file4 dockerfil avail github command build applic chang projectid appnameprojectid need copi gcp consolegcloud build submit tag gcrioprojectidappnam projectprojectid command deploy applicationgcloud run deploy imag gcrioprojectidappnam platform manag projectprojectid allowunauthent
848,"Deploying streamlit app in AWS EC2

> Create an AWS account

> Create an instance in AWS console (Config custom TCP port 8501) and download the key pairs (.pem file)

> Enter into the local project directory where the .pem file is present and login into the instance through CLI

ssh -i 'key_pair_name' ec2-user@public_IP_DNS

> Check python version in EC2 to confirm python is installed in that machine

python3

> Copy all the files (app.py, README, requirements.txt) from github

sudo yum install git

git clone https://repository_path

> Install all dependencies 

sudo pip3 install -r requirements.txt

In case need to install or uninstall dependencies individually, enter into the ec2 home directory,

sudo pip3 install package_name

> Then, check streamlit version to confirm the installation of streamlit in the ec2 instance

streamlit version

> Use Tmux to keep the app running

sudo yum install tmux

tmux new -s st_instance

> Enter into the directory where app.py file is present and run the app

streamlit run app.py

> Attaching the session

tmux ls

tmux attach-session -t st_instance

streamlit run app.py

> Config custom domain name is done during actual deployment, not for demo app deployment",deploy streamlit app aw ec2,"['deploy', 'streamlit', 'app', 'aw', 'ec2']",Deploying streamlit app in AWS EC2,creat aw account creat instanc aw consol config custom tcp port 8501 download key pair pem file enter local project directori pem file present login instanc clissh keypairnam ec2userpublicipdn check python version ec2 confirm python instal machinepython3 copi file apppi readm requirementstxt githubsudo yum instal gitgit clone httpsrepositorypath instal depend sudo pip3 instal r requirementstxtin case need instal uninstal depend individu enter ec2 home directorysudo pip3 instal packagenam check streamlit version confirm instal streamlit ec2 instancestreamlit version use tmux keep app runningsudo yum instal tmuxtmux new stinstanc enter directori apppi file present run appstreamlit run apppi attach sessiontmux lstmux attachsess stinstancestreamlit run apppi config custom domain name done actual deploy demo app deploy
849,"ngrok

> ngrok is a cross-platform application that enables developers to expose a local development server to the Internet for end user with minimal effort.",ngrok,['ngrok'],ngrok,ngrok crossplatform applic enabl develop expos local develop server internet end user minim effort
850,"4 Phases of ML Lifecycle

Phase 1 is Project Planning and Project Setup

Phase 2 is Data Collection and Data Labeling

Phase 3 is Model Training, Testing and Model Debugging (error analysis and model explainability check)

Phase 4 is Model Deployment (generally in the Cloud) and Model Real Time Testing",4 phase machin learn lifecycl,"['4', 'phase', 'machin', 'learn', 'lifecycl']",4 Phases of ML Lifecycle,phase 1 project plan project setupphas 2 data collect data labelingphas 3 model train test model debug error analysi model explain checkphas 4 model deploy general cloud model real time test
851,"Challenges with ML during development

> Development, training and deployment environment can be different

> Tools, libraries or dependencies can complicate deployment

> Tracking and analyzing experiment can become tedious to handle

> Difficult to reproduce experiment as input data changes

As a result, ML Code may end up in a spaghetti jungle.",challeng machin learn develop,"['challeng', 'machin', 'learn', 'develop']",Challenges with ML during development,develop train deploy environ differ tool librari depend complic deploy track analyz experi becom tedious handl difficult reproduc experi input data changesa result machin learn code may end spaghetti jungl
852,"Challenges with ML in production

> Live data is not equal to training data

> Feature engineering pipeline must match between training and serving infrastructure

> Seamlessly scale up and scale down deployed model

> Continuous training and champion challenger model deployment

> Different technology landscape between development and deployment (check versions of the libraries)",challeng machin learn product,"['challeng', 'machin', 'learn', 'product']",Challenges with ML in production,live data equal train data featur engin pipelin must match train serv infrastructur seamless scale scale deploy model continu train champion challeng model deploy differ technolog landscap develop deploy check version librari
853,"Data drift and Model drift

Data drift is the change in model input data that leads to model performance degradation

Model drift refers to the degradation of model performance due to changes in data and relationships between input and output variables.

In model deployment, there can be data drift or model drift issues. 

So, drift analysis must be done.
",data drift model drift,"['data', 'drift', 'model', 'drift']",Data drift and Model drift,data drift chang model input data lead model perform degradationmodel drift refer degrad model perform due chang data relationship input output variablesin model deploy data drift model drift issu drift analysi must done
854,"Machine learning engineering

MLE is the process of using software engineering principles with analytical and data science knowledge together.

This is to take a ML model that’s created and making it available as a product for the end users.
",machin learn engin,"['machin', 'learn', 'engin']",Machine learning engineering,machin learn engin process use softwar engin principl analyt data scienc knowledg togetherthi take machin learn model that creat make avail product end user
855,"Cloud computing advantages

● Reliability: Depending on the service-level agreement that we choose, our cloud-based applications can provide a continuous user experience with no apparent downtime even when things go wrong.

● Scalability: Applications in the cloud can be scaled in two ways, while taking advantage of autoscaling:

>> Vertically: Computing capacity can be increased by adding RAM or CPUs to a virtual machine.

>> Horizontally: Computing capacity can be increased by adding instances of a resource, such as adding more virtual machines.

● Elasticity: Cloud-based applications can be configured to always have the resources they need.

● Agility: Cloud-based resources can be deployed and configured quickly as our application requirements change.

● Geo-distribution: Applications and data can be deployed to regional datacenters around the globe, so our customers always have the best performance in their region.

● Disaster recovery: By taking advantage of cloud-based backup services, data replication, and geo-distribution, we can deploy our applications with the confidence that comes from knowing that our data is safe in the event of disaster.",cloud comput advantag,"['cloud', 'comput', 'advantag']",Cloud computing advantages,● reliabl depend servicelevel agreement choos cloudbas applic provid continu user experi appar downtim even thing go wrong● scalabl applic cloud scale two way take advantag autosc vertic comput capac increas ad ram cpus virtual machin horizont comput capac increas ad instanc resourc ad virtual machines● elast cloudbas applic configur alway resourc need● agil cloudbas resourc deploy configur quick applic requir change● geodistribut applic data deploy region datacent around globe custom alway best perform region● disast recoveri take advantag cloudbas backup servic data replic geodistribut deploy applic confid come know data safe event disast
856,"Cloud service models

IaaS-Infrastructure as a Service (Provides storage, computational power and networking (like DNS, Azure VMs and storage services)

PaaS-Platform as a Service (along with IaaS services, it provides OS and runtime)-This is the most popular one

SaaS-Software as a Service. (along with PaaS services, it provides applications as needed. Example is Office 365)",cloud servic model,"['cloud', 'servic', 'model']",Cloud service models,iaasinfrastructur servic provid storag comput power network like dns azur vms storag servicespaasplatform servic along iaa servic provid oper system runtimethi moper systemt popular onesaassoftwar servic along paa servic provid applic need exampl offic 365
857,"runtime system

A runtime system refers to the collection of software and hardware resources that enable a software program to be executed on a computer system. 

Ex. Docker is a runtime system",runtim system,"['runtim', 'system']",runtime system,runtim system refer collect softwar hardwar resourc enabl softwar program execut comput system ex docker runtim system
858,"CPU vs GPU

The main difference between CPU and GPU architecture is that a CPU is designed to handle a wide-range of tasks quickly, a GPU (Graphics processing unit) is designed to quickly render high-resolution images and video concurrently.

CPU reads Cache Memory data before reading RAM (RAM is a hardware element where the data being currently used is stored. It is a volatile memory.).",central process unit vs graphic process unit,"['central', 'process', 'unit', 'vs', 'graphic', 'process', 'unit']",CPU vs GPU,main differ central process unit graphic process unit architectur central process unit design handl widerang task quick graphic process unit graphic process unit design quick render highresolut imag video concurrentlycentr process unit read cach memori data read ram ram hardwar element data current use store volatil memori
859,"Azure machine learning platform 

-It has end to end service means it provides all services staring from development to deployment

-It is getting very popular day by day because it helps us to build, train and deploy our ML models faster and foster team collaboration.",azur machin learn platform,"['azur', 'machin', 'learn', 'platform']",Azure machine learning platform ,end end servic mean provid servic stare develop deploymentit get popular day day help us build train deploy machin learn model faster foster team collabor
860,"Azure ML Workspace and Azure ML piplines

> In azureml every run is called an Experiment

> Environment file includes the list of all the dependent libraries and packages for the deployment of our application

> from the environment file azure creates the docker image

> Azure Resource Manager and Azure Portal are used to manage resources.",azur machin learn workspac azur machin learn piplin,"['azur', 'machin', 'learn', 'workspac', 'azur', 'machin', 'learn', 'piplin']",Azure ML Workspace and Azure ML piplines,azureml everi run call experi environ file includ list depend librari packag deploy applic environ file azur creat docker imag azur resourc manag azur portal use manag resourc
861,"Connecting to azure ml

We can access azureml through web interface or through python coding by using azureml web api

> !pip install azureml-sdk (enables us to connect to azureml from python) # SDK means Software Development Kit


> import azureml.core

> from azureml.core import Environment, Workspace, Experiment, ScriptRunConfig

experiment_name = 'my_experiment'

ws = Workspace.from_config()

experiment = Experiment(workspace=ws, name=experiment_name)

myenv = Environment.get(workspace=ws, name=experiment_name)

src = ScriptRunConfig(source_directory=project_folder, script='train.py',
compute_target=my_compute_target, environment=myenv)",connect azur machin learn,"['connect', 'azur', 'machin', 'learn']",Connecting to azure ml,access azureml web interfac python code use azureml web api pip instal azuremlsdk enabl us connect azureml python sdk mean softwar develop kit import azuremlcor azuremlcor import environ workspac experi scriptrunconfigexperimentnam myexperimentw workspacefromconfigexperi experimentworkspacew nameexperimentnamemyenv environmentgetworkspacew nameexperimentnamesrc scriptrunconfigsourcedirectoryprojectfold scripttrainpi computetargetmycomputetarget environmentmyenv
862,"Blob storage

Blob stands for Binary Large Object",blob storag,"['blob', 'storag']",Blob storage,blob stand binari larg object
863,"File handling in python

The OS module in Python provides functions for interacting with the operating system. OS comes under Python's standard utility modules.

import os

# To check the name of the operating system

os.name

# Get the current working directory
 
os.getcwd()

# Creating a new folder

path = os.path.join(""path_of_target_location"",""folder_name"")

 os.mkdir(path)

# For checking list of directories

os.listdir()

# For removing a folder
 
path = os.path.join(""path_of_target_location"",""folder_name"")

os.rmdir(path)

# Creating a new file (in the current location)

with open('file_name.txt', 'w') as fp:
    pass

# For removing a file

path = os.path.join(""path_of_target_location"",""file_name.txt"")

os.remove(path)",file handl python,"['file', 'handl', 'python']",File handling in python,oper system modul python provid function interact oper system oper system come python standard util modulesimport oper system check name oper systemoper systemnam get current work directori oper systemgetcwd creat new folderpath oper systempathjoinpathoftargetlocationfoldernam oper systemmkdirpath check list directoriesoper systemlistdir remov folder path oper systempathjoinpathoftargetlocationfoldernameoper systemrmdirpath creat new file current locationwith openfilenametxt w fp pass remov filepath oper systempathjoinpathoftargetlocationfilenametxtoper systemremovepath
864,"Different cloud deployment models

1. Private cloud: Privately owned and manged with restricted access

2. Public Cloud: Service provider owned and managed.

3. Hybrid cloud: combination of private and public cloud",differ cloud deploy model,"['differ', 'cloud', 'deploy', 'model']",Different cloud deployment models,1 privat cloud privat own mang restrict access2 public cloud servic provid own managed3 hybrid cloud combin privat public cloud
865,"DevOps and MLOps

(DevOps)

DevOps meaning software development and IT operation

Software development process includes Plan, Code, Build (an executable program that can be used to carry out proper machine level outputs) and Test (Building is also called programming)

IT operation process includes Release, Deploy, Operate and Monitor

DevOps breaks the wall of confusion between the development team and operation team by automating the process of delivery or deployment through CI/CD

(MLOps)

MLOps in simple term is DevOps for Machine Learning

MLOps is one of the field in ML lifecycle, mainly a part of ML engineering.

MLOps reduces technical debt across machine learning models
(Technical debt (or design debt or code debt) reflects the implied cost of additional rework caused by choosing an easy (limited) solution now instead of using a better approach that would take longer.)

ML Lifecycle management tools are MLFlow, Microsoft Azure ML, AWS Sagemaker, Google Cloud ML etc.

> When we only use github we can not compare metrics between different versions of our model. But when we use github along with ML lifecycle management tools, we can compare metrics",devop ml lifecycl,"['devop', 'ml', 'lifecycl']",DevOps and MLOps,devopsdevop mean softwar develop operationsoftwar develop process includ plan code build execut program use carri proper machin level output test build also call programmingit oper process includ releas deploy oper monitordevop break wall confus develop team oper team autom process deliveri deploy cicdmachin learningopsmachin learningop simpl term devop machin learningmachin learningop one field machin learn lifecycl main part machin learn engineeringmachin learningop reduc technic debt across machin learn model technic debt design debt code debt reflect impli cost addit rework caus choos easi limit solut instead use better approach would take longermachin learn lifecycl manag tool machin learningflow microsoft azur machin learn aw sagemak googl cloud machin learn etc use github compar metric differ version model use github along machin learn lifecycl manag tool compar metric
866,"Agile software development

Agile software development refers to a group of software development methodologies based on iterative development, where requirements and solutions evolve through collaboration between self-organizing cross-functional teams.

Agile is a process of colaboration and DevOps is an automation of colaboration

MLOps enables the application of agile principles to machine learning projects.",agil softwar develop,"['agil', 'softwar', 'develop']",Agile software development,agil softwar develop refer group softwar develop methodolog base iter develop requir solut evolv collabor selforgan crossfunct teamsagil process colabor devop autom colaborationmlop enabl applic agil principl machin learn project
867,"Key components of MLOps

1. Machine learning

2. DevOps (IT)

3. Data Engineering",key compon ml lifecycl,"['key', 'compon', 'ml', 'lifecycl']",Key components of MLOps,1 machin learning2 devop it3 data engin
868,"Advantages of MLOps

MLOps is more vibrant than DevOps (ML + DevOps = MLOps), however MLOps is simpler than DevOps

1. Data/Schema versioning apart from code versioning

2. Experiment tracking (Model hyperparameters, Data Distribution, Model performance, feature importance etc)

3. Model artifacts versioning

4. Monitor continuously for data and model drift

5. Continuous re-training of model

6. Capture sensitivity of key features to target",advantag ml lifecycl,"['advantag', 'ml', 'lifecycl']",Advantages of MLOps,ml lifecycl vibrant devop ml devop ml lifecycl howev ml lifecycl simpler devops1 dataschema version apart code versioning2 experi track model hyperparamet data distribut model perform featur import etc3 model artifact versioning4 monitor continu data model drift5 continu retrain model6 captur sensit key featur target
869,"Key outcomes of MLOps

1. Model Packaging and Validation

2. Model Reproducibility

3. Model deployment 

4. Model Explainability, Monitoring and Re-training",key outcom ml lifecycl,"['key', 'outcom', 'ml', 'lifecycl']",Key outcomes of MLOps,1 model packag validation2 model reproducibility3 model deploy 4 model explain monitor retrain
870,"Model registry and Metadata 

Model registry is for model versioning

(There are three kinds of versioning: Data versioning, model versioning and source code versioning)

ML metadata store is for tracking experiments

> Metadata is a set of data that describes and gives information about the data. That means it is feature understanding of the data.",model registri metadata,"['model', 'registri', 'metadata']",Model registry and Metadata ,model registri model versioningther three kind version data version model version sourc code versioningml metadata store track experi metadata set data describ give inform data mean featur understand data
871,"MLFLow Basics

MLFLow starts from modeling to deployment.

>> It is an open source platform for the machine learning lifecycle. 

>> It has four components: Tracking (experiment, code, data, config, results), Projects (packaging), Models (deployment) and Model registry

>> We can log different metrics using mlflow library and each run of our ml project is saved as individual version. So, we can compare the metrics of different version.

!pip install mlflow

import mlflow

import mlflow.sklearn

import logging

logging.basicConfig(level=logging.WARN)

logger = logging.getLogger(__name__)

mlflow.log_metric(""rmse"", rmse)",mlflow basic,"['mlflow', 'basic']",MLFLow Basics,machin learningflow start model deploy open sourc platform machin learn lifecycl four compon track experi code data config result project packag model deploy model registri log differ metric use machin learningflow librari run machin learn project save individu version compar metric differ versionpip instal machin learningflowimport machin learningflowimport machin learningflowsklearnimport loggingloggingbasicconfiglevelloggingwarnlogg logginggetloggernamemachin learningflowlogmetricrms rmse
872,"Apache Spark (PySpark)

Apache Spark is an open-source unified analytics engine for large-scale data processing which requires multiple virtual machines. 

It is very popular analytics tool. 

Spark and its RDDs were developed in 2012 in response to limitations in the Hadoop MapReduce cluster computing paradigm

Hadoop is an open-source software framework for storing and processing big data in a distributed computing environment. The core of Hadoop consists of a storage part “HDFS” (Hadoop Distributed File System) and a processing part “MapReduce”. ",apach spark pyspark,"['apach', 'spark', 'pyspark']",Apache Spark (PySpark),apach spark opensourc unifi analyt engin largescal data process requir multipl virtual machin popular analyt tool spark rdds develop 2012 respons limit hadoop mapreduc cluster comput paradigmhadoop opensourc softwar framework store process big data distribut comput environ core hadoop consist storag part “hdfs” hadoop distribut file system process part “mapreduce”
873,"Advantages and Disadvantages of Spark

1. Speed: Spark is 100 times faster than MapReduce (because spark uses RAM)

2. Ease of use: allows to quickly write applications in Scala (best), Java, R and Python

3. Advanced analytics: support sql quaries, streaming data and advanced analytics

Disadvantages:

1.  We need to optimize the code manually since it doesn’t have any automatic code optimization process. 

2. Apache Spark doesn’t come with its own file management system. It depends on some other platforms like Hadoop or other cloud-based platforms.",advantag disadvantag spark,"['advantag', 'disadvantag', 'spark']",Advantages and Disadvantages of Spark,1 speed spark 100 time faster mapreduc spark use ram2 eas use allow quick write applic scala best java r python3 advanc analyt support structur queri languag quari stream data advanc analyticsdisadvantages1 need optim code manual sinc doesn't automat code optim process 2 apach spark doesn't come file manag system depend platform like hadoop cloudbas platform
874,"Spark Modules

1. Spark Core: It is the underlying general execution engine. It provides an RDD (Resilient Distributed Dataset) and in-memory (RAM) computing capabilities.

2. Spark SQL and DataFrame

3. Streaming 

4. Mllib: MLlib is a scalable machine learning library

Spark Core is responsible for:

● Memory management and fault recovery

● Scheduling, distributing and monitoring jobs on a cluster

● Interacting with storage systems",spark modul,"['spark', 'modul']",Spark Modules,1 spark core under general execut engin provid resili distribut dataset resili distribut dataset inmemori ram comput capabilities2 spark structur queri languag dataframe3 stream 4 mllib mllib scalabl machin learn libraryspark core respons for● memori manag fault recovery● schedul distribut monitor job cluster● interact storag system
875,"RDD operations

A cluster manager breaks the main csv file into multiple RDD blocks for parallel processing in multiple nodes. Then different transformation occurs on RDD's and generates new RDD's. Then through actions we extract our required datasets

> Here the concept of driver node and worker node (can run application code in the cluster) comes in.

> RDD is an immutable and fault-tolerant system",resili distribut dataset oper,"['resili', 'distribut', 'dataset', 'oper']",RDD operations,cluster manag break main csv file multipl resili distribut dataset block parallel process multipl node differ transform occur resili distribut dataset generat new resili distribut dataset action extract requir dataset concept driver node worker node run applic code cluster come resili distribut dataset immut faulttoler system
876,"Anatomy of Spark Application

Apache Spark Architecture is based on two main abstractions:

1. Resilient Distributed Dataset (RDD)

2. Directed Acyclic Graph (DAG)

Components of Spark:

1. Master node: In our master node, we have the driver program, which drives our application. 

Inside driver program there is spark context. We can assume the Spark context as a gateway to all the Spark functionalities. It is similar to our database connection. Any command we execute in your database goes through the database connection. Likewise, anything we do on Spark goes through Spark context.

2. Resource Manager or Cluster Manager: Cluster manager is a platform where we can run Spark applications. For example AWS EC2, AWS EMR (Elastic Map Reduce)

3. Executors or Worker nodes: These are the slave nodes whose job is to basically execute the tasks. ",anatomi spark applic,"['anatomi', 'spark', 'applic']",Anatomy of Spark Application,apach spark architectur base two main abstractions1 resili distribut dataset rdd2 direct acycl graph dagcompon spark1 master node master node driver program drive applic insid driver program spark context assum spark context gateway spark function similar databas connect command execut databas goe databas connect likewis anyth spark goe spark context2 resourc manag cluster manag cluster manag platform run spark applic exampl aw ec2 aw emr elast map reduce3 executor worker node slave node whose job basic execut task
877,"Spark application coding (in colab)

Spark dataframe functions and methods are very similar as pandas dataframe (but they are distributed and immutable)

!pip install pyspark

import pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master(""local"")\
        .appName(""Colab"")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

spark  # To check the version

my_spark_df = spark.read.csv(""file_path"", header=True, inferSchema=True)

my_spark_df.show(5)

> Git command for linking the Python API to Spark Core and initializing SparkContext

PySpark Shell",spark applic code colab,"['spark', 'applic', 'code', 'colab']",Spark application coding (in colab),spark datafram function method similar panda datafram distribut immutablepip instal pysparkimport pysparkfrom pysparksql import sparksessionspark sparksessionbuild masterloc appnamecolab configsparkuiport 4050 getorcreatespark check versionmysparkdf sparkreadcsvfilepath headertru inferschematruemysparkdfshow5 git command link python applic program interfac spark core initi sparkcontextpyspark shell
878,"Directed Acyclic Graph (DAG)

A directed acyclic graph is a directed graph with no directed cycles. That is, it consists of vertices (nodes) and edges, with each edge directed from one vertex (node) to another, such that following those directions will never form a closed loop.

When a task is defined with simple pandas df like commands, spark builds a logical flow of operations that can be represented as directed acyclic graph (DAG) where node represents a RDD partition and the edge represents a data transformation.

> We can check the DAG for job details till the deepest level in a spark web interface

> Task is a defined unit of work within a DAG; it is represented as a node in the DAG graph, and it is written in Python.

> DAG is a collection of all the tasks we want to run, organized in a way that reflects their relationships and dependencies.

> In graph view, we can visualize each and every step of our workflow with their dependencies and their current status",direct acycl graph dag,"['direct', 'acycl', 'graph', 'dag']",Directed Acyclic Graph (DAG),direct acycl graph direct graph direct cycl consist vertic node edg edg direct one vertex node anoth follow direct never form close loopwhen task defin simpl panda df like command spark build logic flow oper repres direct acycl graph direct acycl graph node repres resili distribut dataset partit edg repres data transform check direct acycl graph job detail till deepest level spark web interfac task defin unit work within direct acycl graph repres node direct acycl graph graph written python direct acycl graph collect task want run organ way reflect relationship depend graph view visual everi step workflow depend current status
879,"Azure Databricks

Azure Databricks is a data analytics platform optimized for the Microsoft Azure cloud services platform. 

Azure Databricks offers three environments for developing data intensive applications: 

1. Databricks SQL, 

2. Databricks Data Science & Engineering, and 

3. Databricks Machine Learning.

> Databricks was built on top of Spark and adds High reliability ",azur databrick,"['azur', 'databrick']",Azure Databricks,azur databrick data analyt platform optim microsoft azur cloud servic platform azur databrick offer three environ develop data intens applic 1 databrick sql 2 databrick data scienc engin 3 databrick machin learn databrick built top spark add high reliabl
880,"Spark application deploy modes

There are two deploy modes that can be used to launch Spark applications on Apache Hadoop YARN - client mode and cluster mode",spark applic deploy mode,"['spark', 'applic', 'deploy', 'mode']",Spark application deploy modes,two deploy mode use launch spark applic apach hadoop yarn client mode cluster mode
881,"Apache Airflow

This platform is to programmatically author, schedule and monitor workflows

Luigi, Jenkins, AWS Step Functions, and Pachyderm are the most popular alternatives and competitors to Airflow.

> Airbnb created Apache Airflow",apach airflow,"['apach', 'airflow']",Apache Airflow,platform programmat author schedul monitor workflowsluigi jenkin aw step function pachyderm popular altern competitor airflow airbnb creat apach airflow
882,"Introduction to Jenkins

Jenkins is an open source automation server. It helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration (CI) and continuous deployment or delivery (CD).

Jenkins creates workflows using Declarative Pipelines, which are similar to GitHub Actions workflow. ",introduct jenkin,"['introduct', 'jenkin']",Introduction to Jenkins,jenkin open sourc autom server help autom part softwar develop relat build test deploy facilit continu integr ci continu deploy deliveri cdjenkin creat workflow use declar pipelin similar github action workflow
883,"Use of Apache Airflow

1. Manage scheduling and running jobs and data pipelines

2. Provides mechanisms for tracking the state of jobs and recovering from failure

3. Ensures jobs are ordered correctly based on dependencies

>> The strongest point of airflow is it can trigger any workflow including spark",use apach airflow,"['use', 'apach', 'airflow']",Use of Apache Airflow,1 manag schedul run job data pipelines2 provid mechan track state job recov failure3 ensur job order correct base depend strongest point airflow trigger workflow includ spark
884,"Airflow Components

Airflow Components are Web server, Scheduler, Executor and Metadata database",airflow compon,"['airflow', 'compon']",Airflow Components,airflow compon web server schedul executor metadata databas
885,"Web Server and Scheduler

● Web Server and Scheduler: The Airflow web server and Scheduler are separate processes run (in this
case) on the local machine and interact with the database mentioned above.

Scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete (scheduling the execution of DAGs). 

● airflow.cfg is the Airflow configuration file which is accessed by the Web Server, Scheduler, and
Workers.

● DAGs refers to the DAG files containing Python code, representing the data pipelines to be run by Airflow. The location of these files is specified in the Airflow configuration file, but they need to be accessible by the Web Server, Scheduler, and Workers.

> By Airflow we can create variables where we can store and retrieve data at runtime in the multiple DAGS",web server schedul,"['web', 'server', 'schedul']",Web Server and Scheduler,● web server schedul airflow web server schedul separ process run case local machin interact databas mention aboveschedul monitor task direct acycl graph trigger task instanc depend complet schedul execut direct acycl graph ● airflowcfg airflow configur file access web server schedul workers● direct acycl graph refer direct acycl graph file contain python code repres data pipelin run airflow locat file specifi airflow configur file need access web server schedul worker airflow creat variabl store retriev data runtim multipl direct acycl graph
886,"Metadata Database

● Metadata Database: Airflow uses a SQL database to store metadata about the data pipelines being run.
Postgres is extremely popular with Airflow. Alternate databases supported with Airflow include MySQL.",metadata databas,"['metadata', 'databas']",Metadata Database,● metadata databas airflow use structur queri languag databas store metadata data pipelin run postgr extrem popular airflow altern databas support airflow includ mystructur queri languag
887," Executor, Worker and Operator

● The Executor is shown separately in architecture but runs within the Scheduler.

Executors are the mechanism by which task instances get run. e.g. SequentialExecutor, Celery Executor, Kubernetes Executor

● The Worker(s) are separate processes which also interact with the other components of the Airflow architecture and the metadata repository

● Operators determine what actually gets done by a task. e.g. Bash Operator, Python Operator

> Azure Kubernetes Service (AKS) and few others offers serverless capabilities

> By Task duration we will be able to compare the duration of our tasks run at different time intervals",executor worker oper,"['executor', 'worker', 'oper']"," Executor, Worker and Operator",● executor shown separ architectur run within schedulerexecutor mechan task instanc get run eg sequentialexecutor celeri executor kubernet executor● worker separ process also interact compon airflow architectur metadata repository● oper determin actual get done task eg bash oper python oper azur kubernet servic ak other offer serverless capabl task durat abl compar durat task run differ time interv
888,"Applications of Airflow

Apache Airflow can be used to schedule and execute complex workflows

● ETL pipelines that extract data from multiple sources and run Spark jobs or any other data
transformations

● Training machine learning models

● Report generation

● Backups and similar DevOps operations",applic airflow,"['applic', 'airflow']",Applications of Airflow,apach airflow use schedul execut complex workflows● etl pipelin extract data multipl sourc run spark job data transformations● train machin learn models● report generation● backup similar devop oper
889,"Understanding Celery

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.

Celery instance / Celery application or just celery app is used as the entry-point for everything we want to do in Celery, like creating tasks and managing workers, it must be possible for other modules to import it.",understand celeri,"['understand', 'celeri']",Understanding Celery,celeri simpl flexibl reliabl distribut system process vast amount messag provid oper tool requir maintain systemceleri instanc celeri applic celeri app use entrypoint everyth want celeri like creat task manag worker must possibl modul import
890,"Basic of Deep Neural Networks

Deep Neural Network/Deep learning models are supervised learning models. Means we need labeled data for it's functioning.

> A small neural network can also beat the performance of any other ML model. That is why deep learning is gaining its popularity, but it is a blackbox model.

> It is called a blackbox model because there is no interpretability

> In deep learning, at a very basic level, we convert an unstructured input data (text, audio, image) into structured input data (matrix)

> Neuron applies some forms of non-linearity on the inputs

> The hidden layers of the neural networks actually does some feature engineering inside (learns different weights), about which we don't know. We only get the output

> Here, multiple layers of processing are used to extract progressively higher level features from data.

> To get better performance we must have large neural networks and more data.

> If our model takes weeks to get trained, then it is worthy to buy faster computers that could speed up our teams' iteration speed and thus our team's productivity.",basic deep neural network,"['basic', 'deep', 'neural', 'network']",Basic of Deep Neural Networks,deep neural networkdeep learn model supervis learn model mean need label data function small neural network also beat perform machin learn model deep learn gain popular blackbox model call blackbox model interpret deep learn basic level convert unstructur input data text audio imag structur input data matrix neuron appli form nonlinear input hidden layer neural network actual featur engin insid learn differ weight dont know get output multipl layer process use extract progress higher level featur data get better perform must larg neural network data model take week get train worthi buy faster comput could speed team iter speed thus team product
891,"ANN, CNN and RNN

Types of Neural Network

1. ANN-Artificial Neural Network  (used for normal classification and regression problems on large dataset (>1 lakh experiences)). ANN is also called Standard NN which includes ""shallow"" neural network and DNN.

2. CNN-Convolutional Neural Network (used for image related inputs)

3. RNN-Recurrent Neural Network (used for sequencial input like text, audio etc.)

4. Custom NN (for solving complex problems, like autonomous driving)

> Image captioning model is a CNN+RNN model",ann convolut neural network recurr neural network,"['ann', 'convolut', 'neural', 'network', 'recurr', 'neural', 'network']","ANN, CNN and RNN",type neural network1 artifici neural networkartifici neural network use normal classif regress problem larg dataset 1 lakh experi artifici neural network also call standard neural network includ shallow neural network dneural network2 cneural networkconvolut neural network use imag relat inputs3 rneural networkrecurr neural network use sequenci input like text audio etc4 custom neural network solv complex problem like autonom drive imag caption model cneural networkrneur network model
892,"Channels of RGB image

An RGB image has three channels: red, green, and blue. RGB channels roughly follow the color receptors in the human eye, and are used in computer displays and image scanners.

R channel gives us a matrix with the intensity of red in every pexel.

G channel gives us a matrix with the intensity of green in every pexel.

B channel gives us a matrix with the intensity of blue in every pexel.

Seven colours of Rainbow:

Red, Orange, Yellow, Green, Blue, Indigo and Violet.

Red, Green and Blue are additive primary colours because they can make all other colors, even yellow. When mixed together, red, green and blue colours make white. ",channel rgb imag,"['channel', 'rgb', 'imag']",Channels of RGB image,rgb imag three channel red green blue rgb channel rough follow color receptor human eye use comput display imag scannersr channel give us matrix intens red everi pexelg channel give us matrix intens green everi pexelb channel give us matrix intens blue everi pexelseven colour rainbowr orang yellow green blue indigo violetr green blue addit primari colour make color even yellow mix togeth red green blue colour make white
893,"Writing sigmoid function

def sigmoid(z):
   
    s= 1/(1+np.exp(-z))
   
    return s",write sigmoid function,"['write', 'sigmoid', 'function']",Writing sigmoid function,def sigmoidz 11npexpz return
894,"Deeper understanding of deep neural network

We can think a neural network as ensembles of linear regression with kernel (or logistic regression) and does feature engineering in multiple layers. 

As we initialize the weights of the features randomly, every node of the first layer generates different activations, means every node is generating a new feature with the combination of input features with feature importance (weights). This process goes on in multiple layers for generating higher level feature. 

In backpropagation, feature importances (weights) are adjusted to get the desired label.

Regularization can be used during training to prevent the algorithm overfitting the training dataset.",deeper understand deep neural network,"['deeper', 'understand', 'deep', 'neural', 'network']",Deeper understanding of deep neural network,think neural network ensembl linear regress kernel logist regress featur engin multipl layer initi weight featur random everi node first layer generat differ activ mean everi node generat new featur combin input featur featur import weight process goe multipl layer generat higher level featur backpropag featur import weight adjust get desir labelregular use train prevent algorithm overfit train dataset
895,"Writing initialize weight function 

> for single neuron(output layer)

def initialize_with_zeros(dim):
        
        w = np.full((dim,1),0)
    b = 0.0
    
    return w, b

> For one hidden and one output layer

def initialize_parameters(n_x, n_h, n_y):

    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))
    
    parameters = {""W1"": W1,
                  ""b1"": b1,
                  ""W2"": W2,
                  ""b2"": b2}
    
    return parameters",write initi weight function,"['write', 'initi', 'weight', 'function']",Writing initialize weight function ,singl neuronoutput layerdef initializewithzerosdim w npfulldim10 b 00 return w b one hidden one output layerdef initializeparametersnx nh ny w1 nprandomrandnnh nx 001 b1 npzerosnh 1 w2 nprandomrandnni nh 001 b2 npzerosni 1 paramet w1 w1 b1 b1 w2 w2 b2 b2 return paramet
896,"Writing optimization function

def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):
    
        w = copy.deepcopy(w)
    b = copy.deepcopy(b)
    
    costs = []
    
    for i in range(num_iterations):
        
        # Cost and gradient calculation 
        
        grads, cost = propagate(w, b, X, Y)
     
         # Retrieve derivatives from grads
        dw = grads[""dw""]
        db = grads[""db""]
        
        # update rule 
        w = w - learning_rate* dw
        b = b - learning_rate* db
       
        # Record the costs
        if i % 100 == 0:
            costs.append(cost)
        
            # Print the cost every 100 training iterations
            if print_cost:
                print (""Cost after iteration %i: %f"" %(i, cost))
    
    params = {""w"": w,
              ""b"": b}
    
    grads = {""dw"": dw,
             ""db"": db}
    
    return params, grads, costs

>> For multi-layer network, optimization function is generally written as update_parameters(parameters, grads, learning_rate) function which returns the updated parameters",write optim function,"['write', 'optim', 'function']",Writing optimization function,def optimizew b x numiterations100 learningrate0009 printcostfals w copydeepcopyw b copydeepcopyb cost rangenumiter cost gradient calcul grad cost propagatew b x retriev deriv grad dw gradsdw db gradsdb updat rule w w learningr dw b b learningr db record cost 100 0 costsappendcost print cost everi 100 train iter printcost print cost iter f cost param w w b b grad dw dw db db return param grad cost multilay network optim function general written updateparametersparamet grad learningr function return updat paramet
897,"Loss function and Cost Function in DNN

When Loss function is denoted by L(ŷ,y) and 'm' is the number of observations or examples,

Cost Function, 

J(w,b)=1/m∑L(ŷ(i),y(i)), i=1,m

'w' is the weight and 'b' is the intercept

Partial derivative of J, dJ/dw is referred as 'dw' and dJ/db is referred as 'db'

Loss function for logistic regression

L(ŷ,y)= -[ylogŷ-(1-y)log(1-ŷ)]

Hyperparameter tuning for neural network is an iterative process, for minimizing the cost fuction(loss or error)",loss function cost function deep neural network,"['loss', 'function', 'cost', 'function', 'deep', 'neural', 'network']",Loss function and Cost Function in DNN,loss function denot lŷi number observ examplescost function jwb1m∑lŷiyi i1mw weight b interceptparti deriv j djdw refer dw djdb refer dbloss function logist regressionlŷi ylogŷ1ylog1ŷhyperparamet tune neural network iter process minim cost fuctionloss error
898,"Writing propagate function

def propagate(w, b, X, Y):
    
    m = X.shape[1]
    
    # FORWARD PROPAGATION (FROM X TO COST)
    A = sigmoid(np.dot(w.T, X) + b) # use of previously written sigmoid function
    cost = -((np.sum(np.dot(Y, np.log(A).T)+np.dot((1-Y), (np.log(1-A)).T)))/m)

    # BACKWARD PROPAGATION (TO FIND GRAD)
    dw = (np.dot(X,((A-Y).T)))/m
    db = np.sum((A-Y))/m

    cost = np.squeeze(np.array(cost)) # to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array)
    
    grads = {""dw"": dw,
             ""db"": db}
    
    return grads, cost

>> Backpropagation in two layer network

    dZ2 = A2 -Y
    dW2 = (np.dot(dZ2,A1.T))/m
    db2 = np.sum(dZ2, axis=1, keepdims=True)/m
    dZ1 = np.dot(W2.T, dZ2)*(1 - np.power(A1, 2))
    dW1 = (np.dot(dZ1,X.T))/m
    db1 = np.sum(dZ1, axis=1, keepdims=True)/m

>> For multi-layer, propagate function is generally broken into three functions e.g. linear_activation_forward(A_prev, W, b, activation), compute_cost(AL, Y), linear_activation_backward(dA, cache, activation)",write propag function,"['write', 'propag', 'function']",Writing propagate function,def propagatew b x xshape1 forward propag x cost sigmoidnpdotwt x b use previous written sigmoid function cost npsumnpdoti nplogatnpdot1i nplog1atm backward propag find grad dw npdotxaytm db npsumaym cost npsqueezenparraycost remov redund dimens case singl float reduc zerodimens array grad dw dw db db return grad cost backpropag two layer network dz2 a2 dw2 npdotdz2a1tm db2 npsumdz2 axis1 keepdimstruem dz1 npdotw2t dz21 nppowera1 2 dw1 npdotdz1xtm db1 npsumdz1 axis1 keepdimstruem multilay propag function general broken three function eg linearactivationforwardaprev w b activ computecost linearactivationbackwardda cach activ
899,"Computational graph

A computational graph is defined as a directed graph where the nodes correspond to mathematical operations.

> We can create the computation graph for linear regression or logistic regression or deep learning model

> feed_dict feeds external data into computational graphs

> To perform caculations in TensorFlow we launch the computational graph in a session

> Calculations can be done in parallel in computational graph",comput graph,"['comput', 'graph']",Computational graph,comput graph defin direct graph node correspond mathemat oper creat comput graph linear regress logist regress deep learn model feeddict feed extern data comput graph perform cacul tensorflow launch comput graph session calcul done parallel comput graph
900,"Calculation behind gradient descent

> It calculates 'z' values, 'a' values and cost function in forward propagation considering random weights

> Then, it calculates da, dz, dw and db in back propagation 

> Then, it calculates new weight of w and b for learning rate (α) and the gradient of cost function dw and db. Then another forward propagation begins

In every step, weight is updated by 
w= w -α*dw
b= b -α*db

Process continues till we reach global minima for the cost function (cost function means the average error in prediction for all the observations)

If our learning rate is set too low, training will progress very slowly as we are making very tiny updates to the weights in our network. However, if our learning rate is set too high, it can cause undesirable divergent behavior in our loss function.

> Gradient at a given layer is the product of all gradients at the previous layers.

> Weight between input and hidden layer have a constant input in each epoch of training a deep learning model

> Effect of false minima is reduced by stochastic update of weights

> For a non-continuous objective during optimization in deep neural net, subgradient method is used

> In feedforward NN (FNN), information flow is unidirectional",calcul behind gradient descent,"['calcul', 'behind', 'gradient', 'descent']",Calculation behind gradient descent,calcul z valu valu cost function forward propag consid random weight calcul da dz dw db back propag calcul new weight w b learn rate α gradient cost function dw db anoth forward propag beginsin everi step weight updat w w αdw b b αdbprocess continu till reach global minima cost function cost function mean averag error predict observationsif learn rate set low train progress slowli make tini updat weight network howev learn rate set high caus undesir diverg behavior loss function gradient given layer product gradient previous layer weight input hidden layer constant input epoch train deep learn model effect fals minima reduc stochast updat weight noncontinu object optim deep neural net subgradi method use feedforward neural network fneural network inform flow unidirect
901,"Need of vectorization

> Z=W-transpose.X+b

> X is a matrix (or vector) in which each row is one feature and each column is one training example. (For other ml algorithm it is transposed)

> Shape of X is (nx,m) where nx= total no. of features and m= total no. of observations

> For Z calculation in non-vectorized form, we use explicit for loop (explicit means describing something that is very clear), which is series calculation and very slow. Thus the need of vectorization comes into picture. However, in a deeper network, we cannot avoid a for loop iterating over the layers.

> During numpy dot product of two vectors W-transpose and X, SIMD is utilized making the caculation very fast",need vector,"['need', 'vector']",Need of vectorization,zwtransposexb x matrix vector row one featur column one train exampl machin learn algorithm transpos shape x nxm nx total featur total observ z calcul nonvector form use explicit loop explicit mean describ someth clear seri calcul slow thus need vector come pictur howev deeper network cannot avoid loop iter layer numpi dot product two vector wtranspos x simd util make cacul fast
902,"Vectorized implementation of forward propagation for layer l

Z [l]=W [l]A [l - 1]+b [l]

A [l]=g [l] (Z [l])

> Input layer is called the activation zero. A[0]=X.

> First hidden layer is the first layer and output layer is the final layer

> A neural network with 1 input layer, 2 hidden layers and 1 output layer have total 3 layers",vector implement forward propag layer l,"['vector', 'implement', 'forward', 'propag', 'layer', 'l']",Vectorized implementation of forward propagation for layer l,z lw la l 1b la lg l z l input layer call activ zero a0x first hidden layer first layer output layer final layer neural network 1 input layer 2 hidden layer 1 output layer total 3 layer
903,"Notations in DNN

> When we represent a single observation, we use small letter notation (z,w,x)

> When we represent multiple observations, we use capital letter notation (Z,W,X). So, Z,W and X are all vectors or matrices, not single numbers",notat deep neural network,"['notat', 'deep', 'neural', 'network']",Notations in DNN,repres singl observ use small letter notat zwx repres multipl observ use capit letter notat zwx zw x vector matric singl number
904,"SIMD units

SIMD means Single Instruction Multiple Data (parallel processing of CPU or GPU)

SIMD units refer to hardware components that perform the same operation on multiple data operands concurrently. 

Typically, a SIMD unit receives as input two vectors (each one with a set of operands), performs the same operation on both sets of operands (one operand from each vector), and outputs a vector.

Numpy utilizes this ability of computer.",simd unit,"['simd', 'unit']",SIMD units,simd mean singl instruct multipl data parallel process central process unit gpusimd unit refer hardwar compon perform oper multipl data operand concurr typic simd unit receiv input two vector one set operand perform oper set operand one operand vector output vectornumpi util abil comput
905,"Writing predict function

def predict(w, b, X):
        
    m = X.shape[1]
    Y_prediction = np.zeros((1, m))
    w = w.reshape(X.shape[0], 1)
    
    # Compute vector ""A"" predicting the probabilities of a cat being present in the picture
    
    A = sigmoid(np.dot(w.T, X) + b)
   
    for i in range(A.shape[1]):
        
               if A[0, i] > 0.5 :
            Y_prediction[0,i] = 1
        else:
            Y_prediction[0,i] = 0
        
    return Y_prediction

> We must avoid using for loop in our algorithm by using following code in predict function

def predict(parameters, X):

    A2, cache = forward_propagation(X, parameters) # use of propagate or forward_propagation function

    predictions = np.where(A2 > 0.5, 1, 0)

return predictions",write predict function,"['write', 'predict', 'function']",Writing predict function,def predictw b x xshape1 ypredict npzeros1 w wreshapexshape0 1 comput vector predict probabl cat present pictur sigmoidnpdotwt x b rangeashape1 a0 05 yprediction0i 1 els yprediction0i 0 return ypredict must avoid use loop algorithm use follow code predict functiondef predictparamet x a2 cach forwardpropagationx paramet use propag forwardpropag function predict npwherea2 05 1 0return predict
906,"Element-wise matrix multiplication

It is different from common matrix product

a = np.random.randn(3, 3)

b = np.random.randn(3, 1)

c = a * b

c.shape=(3,3)

In another case,

a = np.random.randn(1, 3)

b = np.random.randn(3, 3)

c = a * b is not possible because it is not possible to broadcast more than one dimension",elementwis matrix multipl,"['elementwis', 'matrix', 'multipl']",Element-wise matrix multiplication,differ common matrix producta nprandomrandn3 3b nprandomrandn3 1c bcshape33in anoth casea nprandomrandn1 3b nprandomrandn3 3c b possibl possibl broadcast one dimens
907,"Notation for multiple layers

> In the superscript of Z, W, X or b if we write in parenthesis, it denotes the observation no.

Z(1) means the Z value for the first observation or training example

> In the superscript of Z, W, X or b if we write in square bracket, it denotes the layer no.

Z[1] means the Z value in the first layer

Say, layers no. = l

g[l]= activation function of layer l

Multiple layer neural network is called deep neural network

parameters (which the model learns) for a neural network are

W[1], b[1], W[2], b[2],…. W[l], b[l]

> In the subscript we denote the node number",notat multipl layer,"['notat', 'multipl', 'layer']",Notation for multiple layers,superscript z w x b write parenthesi denot observ noz1 mean z valu first observ train exampl superscript z w x b write squar bracket denot layer noz1 mean z valu first layersay layer lgl activ function layer lmultipl layer neural network call deep neural networkparamet model learn neural network arew1 b1 w2 b2… wl bl subscript denot node number
908,"Activation functions

The activation function maps linear regression function, z(x) (z=y_pred in linear regression) and generates an activation/output value f(z) 

> In general activation function is denoted by 'g'

1. sigmoid function

Sigmoid function is only used for binary classification problems

if z is large in positive side then σ(z) or g(z)≈1

if z is large in negative side then σ(z) or g(z)≈0

*** a=g(z), g(z) is called the 'a' value

2. Tanh function

This activation function is similar as sigmoid, but the range is -1 to 1

But tanh function is better than sigmoid function, because it centralizes the data and it is useful for second layer learning. (Hyperbolic Tangent activation function also centralizes the data)

3. ReLU and Leaky ReLU function

Rectified Linear Unit (ReLU) is function where the output is non-linear for a certain period and then becomes linear.

ReLU is better activation function than Sigmoid and Tanh, because learnig becomes very slow for larger values of z as the gradient vanishes (flatens) in the case of sigmoid and tanh

The range of ReLU is 0 to z

The range of Leaky ReLU is -0.01z to z

> In different layer we can use different activation functions if required",activ function,"['activ', 'function']",Activation functions,activ function map linear regress function zx zypr linear regress generat activationoutput valu fz general activ function denot g1 sigmoid functionsigmoid function use binari classif problemsif z larg posit side σz gz≈1if z larg negat side σz gz≈0 agz gz call value2 tanh functionthi activ function similar sigmoid rang 1 1but tanh function better sigmoid function central data use second layer learn hyperbol tangent activ function also central data3 relu leaki relu functionrectifi linear unit relu function output nonlinear certain period becom linearrelu better activ function sigmoid tanh learnig becom slow larger valu z gradient vanish flaten case sigmoid tanhth rang relu 0 zthe rang leaki relu 001z z differ layer use differ activ function requir
909,"Linear and nonlinear activation function

We apply linear activation function (similar to not applying any activation) in neural network for regression problems

Non linear activation functions (Sigmoid, Tanh, ReLU or Leaky ReLU) are called kernel approximation or kernel functions.",linear nonlinear activ function,"['linear', 'nonlinear', 'activ', 'function']",Linear and nonlinear activation function,appli linear activ function similar appli activ neural network regress problemsnon linear activ function sigmoid tanh relu leaki relu call kernel approxim kernel function
910,"Keras and TensorFlow

Keras is a neural network library while TensorFlow is the open-source library for a number of various tasks in machine learning. 

TensorFlow provides both high-level and low-level APIs while Keras provides only high-level APIs.  Keras is built in Python which makes it way more user-friendly than TensorFlow.

Keras is a deep learning wrapper on TensorFlow

Wrapper methods measure the “usefulness” of features based on the classifier performance

>> TensorFlow architecture works in 3 parts (Data pre-processing, Model building and Train & estimate the model)

>> Variables in TensorFlow are also known as tensor objects",kera tensorflow,"['kera', 'tensorflow']",Keras and TensorFlow,kera neural network librari tensorflow opensourc librari number various task machin learn tensorflow provid highlevel lowlevel api kera provid highlevel api kera built python make way userfriend tensorflowkera deep learn wrapper tensorflowwrapp method measur “usefulness” featur base classifi perform tensorflow architectur work 3 part data preprocess model build train estim model variabl tensorflow also known tensor object
911,"Problem of zero initialization

> When all the intial weights are considered as zero, all the nodes in a particular layer will learn the same function even after multiple iterations of gradient descent. This is same as taking single node.

> That is why random weights are initialized but very small values. This breaks the symmetry and allows different nodes to learn independently of each other 

> Also for other ml model if all the weights are initiallized as same, after many iterations the model learns same weight for all the features. This is called the “symmetry”",problem zero initi,"['problem', 'zero', 'initi']",Problem of zero initialization,intial weight consid zero node particular layer learn function even multipl iter gradient descent take singl node random weight initi small valu break symmetri allow differ node learn independ also machin learn model weight initial mani iter model learn weight featur call “symmetry”
912,"Initializing parameters for the model (for deep network)

Suppose we store the values of n[l] in layer_dims

layer_dims=[6,4,3,1] # 6 is the number of input features

parameters = {}

for l in range(1, len(layer_dims)):

    parameters['W'+str(l)]= np.random.randn(layer_dims[l],layer_dims[l-1])*0.01

    parameters['b'+str(l)]= np.zeros((layer_dims[l], 1))",initi paramet model deep network,"['initi', 'paramet', 'model', 'deep', 'network']",Initializing parameters for the model (for deep network),suppos store valu nl layerdimslayerdims6431 6 number input featuresparamet l range1 lenlayerdim parameterswstrl nprandomrandnlayerdimsllayerdimsl1001 parametersbstrl npzeroslayerdimsl 1
913," Deep Learning capabilities:

1. Classification Only (C)

2. Classification with Memory (CM)

3. Classification with Knowledge (CK)

4. Classification with Imperfect Knowledge (CIK)

5. Collaborative Classification with Imperfect Knowledge (CCIK)-ensembles of weak learners

> As range of hidden layers boom, model capability will increase",deep learn capabl,"['deep', 'learn', 'capabl']", Deep Learning capabilities:,1 classif c2 classif memori cm3 classif knowledg ck4 classif imperfect knowledg cik5 collabor classif imperfect knowledg ccikensembl weak learner rang hidden layer boom model capabl increas
914,"RNN for machine translation

Why is an RNN used for machine translation?

> It can be trained as a supervised learning problem.

> RNN is applicable when the input data is a sequence and machine translation is a sequence of words",recurr neural network machin translat,"['recurr', 'neural', 'network', 'machin', 'translat']",RNN for machine translation,recurr neural network use machin translat train supervis learn problem recurr neural network applic input data sequenc machin translat sequenc word
915,"Layer dimension notation in Neural Network

n[l] means no. of nodes in l th layer

n[0]= no. of nodes in the input layer (=no. of features in the dataset)

n[1]=no. of nodes in the first hidden layer",layer dimens notat neural network,"['layer', 'dimens', 'notat', 'neural', 'network']",Layer dimension notation in Neural Network,nl mean node l th layern0 node input layer featur datasetn1no node first hidden layer
916,"Weight notation in Neural Network

For a two layer network, say there are three input nodes (X1,X2 and X3) and one hidden layer with two nodes, h1[1] and h2[1] and one output layer, h1[2]

> then the weights learned by h1 will be w11,w21,w31 and weights learned by h2 will be w12,w22,w32 for the same set of inputs

Initialized weight vector, W= [[w11,w12], [w21,w22], [w31,w32]] or 

[list_of_weight_for_input_variable_1, list_of_weight_for_input_variable_2, list_of_weight_for_input_variable_3]

Thus, the shape of W is (3,2)

Weight vector for the first layer,

W[1]=W.T= [[w11,w21,w31],[w12,w22,w32]]

Thus, the shape of W[1] is (2,3) or shape of W[l] is (n[l], n[l-1])",weight notat neural network,"['weight', 'notat', 'neural', 'network']",Weight notation in Neural Network,two layer network say three input node x1x2 x3 one hidden layer two node h11 h21 one output layer h12 weight learn h1 w11w21w31 weight learn h2 w12w22w32 set inputsiniti weight vector w w11w12 w21w22 w31w32 listofweightforinputvariable1 listofweightforinputvariable2 listofweightforinputvariable3thus shape w 32weight vector first layerw1wt w11w21w31w12w22w32thus shape w1 23 shape wl nl nl1
917,"Bias notation in Neural Network

b[1]= [[b1],[b2]]

shape of b[l] is (n[l],1)

for all the training example, m, shape of b[l] is (n[l],m)

> Shape of Z and A will be same as shape of b",bias notat neural network,"['bias', 'notat', 'neural', 'network']",Bias notation in Neural Network,b1 b1b2shape bl nl1for train exampl shape bl nlm shape z shape b
918,"Network notation for i th experience

A two layer network, say there are three input nodes (X1,X2 and X3) and one hidden layer with two nodes, h1[1] and h2[1] and one output layer, h1[2] is denoted as follows:

X1(i)--a1[1]--
X2(i)--a2[1]--a1[2]--y(i)
x3(i)--",network notat th experi,"['network', 'notat', 'th', 'experi']",Network notation for i th experience,two layer network say three input node x1x2 x3 one hidden layer two node h11 h21 one output layer h12 denot followsx1ia11 x2ia21a12yi x3i
919,"Forward propagation and backpropagation

During forward propagation, in the forward function for layer II we need to know what is the activation function in a layer.

During backpropagation, the corresponding backward function also needs to know what is the activation function for layer II, since the gradient depends on it.

Backpropagation uses reversed range to calculate the gradients

for l in reversed(range(L-1)): # L is the number of layers

We use cache to pass variables computed during forward propagation to corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives

linear_cache stores the values of (A, W, b) as a tuple and activation_cache stores the value of Z as numpy.ndarray",forward propag backpropag,"['forward', 'propag', 'backpropag']",Forward propagation and backpropagation,forward propag forward function layer ii need know activ function layerdur backpropag correspond backward function also need know activ function layer ii sinc gradient depend itbackpropag use revers rang calcul gradientsfor l reversedrangel1 l number layersw use cach pass variabl comput forward propag correspond backward propag step contain use valu backward propag comput derivativeslinearcach store valu w b tupl activationcach store valu z numpyndarray
920,"shallow"" neural network

""shallow"" neural network is a term used to describe NN that usually have only one hidden layer (SNN) as opposed to deep NN which have several hidden layers, often of various types.

If we need a large shallow network (large no. of nodes or logic gates) to compute any function, using deep NN will be exponentially smaller.",shallow neural network,"['shallow', 'neural', 'network']","shallow"" neural network",shallow neural network term use describ neural network usual one hidden layer sneural network oppos deep neural network sever hidden layer often various typesif need larg shallow network larg node logic gate comput function use deep neural network exponenti smaller
921,"Basics of Improving Deep Neural Networks

We can split the dataset into three parts: Training, Development and Testing (fully untouched one, used as unseen data) as an improvement method

Always make sure that our train, dev and test have similar distribution of data

In case of big data, there is no need to follow 70-30 or 80-20 train-test split (old way of spliting data). We may take 98 % in training, 1% in development and 1% in testing, because here 1% is also a huge number and testing data does not contribute in learning, it is only for evaluation.

> But for human learning model, testing data also contribute in learning when we perform error analysis. This is the case like reinforcement learning model",basic improv deep neural network,"['basic', 'improv', 'deep', 'neural', 'network']",Basics of Improving Deep Neural Networks,split dataset three part train develop test fulli untouch one use unseen data improv methodalway make sure train dev test similar distribut datain case big data need follow 7030 8020 traintest split old way splite data may take 98 train 1 develop 1 test 1 also huge number test data contribut learn evalu human learn model test data also contribut learn perform error analysi case like reinforc learn model
922,"Basic 'recipe' for all machine learning models

> We need to build our first simple model quickly and then slowly keep on increasing complexity

> Then we should use Bias/Variance analysis and error analysis to prioritize next step 

1. Check the model has avoidable bias or not (comparing trainig error with human level error) and take action.

2. Check the model has high variance or not (comparing training and dev error) and take action

Our model should not have 

1. High bias, low variance

2. Low bias, high variance

3. High bias, high variance (also possible)

Our optimal model must have low bias and low variance (optimum)",basic recip machin learn model,"['basic', 'recip', 'machin', 'learn', 'model']",Basic 'recipe' for all machine learning models,need build first simpl model quick slowli keep increas complex use biasvari analysi error analysi priorit next step 1 check model avoid bias compar trainig error human level error take action2 check model high varianc compar train dev error take actionour model 1 high bias low variance2 low bias high variance3 high bias high varianc also possibleour optim model must low bias low varianc optimum
923,"DNN model complexity

> Big neural network may make our model overfitting (so, we can easily control model complexity by adjusting the layers and nodes. We can also reduce model complexity of big neural network by applying regularization)

During regularization, for extreamly large lambda, most of the weights will become zero and the model will become very simple.

For classification problem, every neuron in a neural network learns a classification boundary. More the classification boundary becomes non-linear, more the model becomes complex. 

>  Less data with large no. of dimensions can also make our model overfitting

For normal machine learning models, after a threshold limit of data, model performance saturate. But for deep neural network there is no limit of data. More data means more accuracy.",deep neural network model complex,"['deep', 'neural', 'network', 'model', 'complex']",DNN model complexity,big neural network may make model overfit easili control model complex adjust layer node also reduc model complex big neural network appli regularizationdur regular extream larg lambda weight becom zero model becom simplefor classif problem everi neuron neural network learn classif boundari classif boundari becom nonlinear model becom complex less data larg dimens also make model overfittingfor normal machin learn model threshold limit data model perform satur deep neural network limit data data mean accuraci
924,"Dropout regularization

Apart from L1 and L2 regularization, there is Dropout regularization.

Here we randomly make some node zero to make the model simpler.",dropout regular,"['dropout', 'regular']",Dropout regularization,apart l1 l2 regular dropout regularizationher random make node zero make model simpler
925,"Implementing dropout ('inverted dropout'): 

Here in a certain hidden layer (l=3), if we define keep prob=0.8. It means, with probability 0.2 the nodes will become zero in layer 3. So, if in layer 3 there were 10 nodes, 2 nodes will be deleted randomly

> There is no implementation of dropout during test, it is only implemented during training

> Generally used in computer vision (CNN architecture)

> Dropout can not be used in input and output layers

> forward_propagation_with_dropout

 D1 = np.random.rand(A1.shape[0], A1.shape[1])      # Step 1: initialize matrix D1
D1 = (D1 < keep_prob).astype(int)                  # Step 2: convert entries of D1 to 0 or 1
A1 = np.multiply(A1, D1)                           # Step 3: shut down some neurons of A1
A1 = A1/keep_prob                                  # Step 4: scale the value of neurons that haven't been shut down

> backward_propagation_with_dropout

dA2 = np.multiply(dA2, D2)     # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation
    dA2 = dA2 / keep_prob          # Step 2: Scale the value of neurons that haven't been shut down",implement dropout invert dropout,"['implement', 'dropout', 'invert', 'dropout']",Implementing dropout ('inverted dropout'): ,certain hidden layer l3 defin keep prob08 mean probabl 02 node becom zero layer 3 layer 3 10 node 2 node delet random implement dropout test implement train general use comput vision cnn architectur dropout use input output layer forwardpropagationwithdropout d1 nprandomranda1shape0 a1shape1 step 1 initi matrix d1 d1 d1 keepprobastypeint step 2 convert entri d1 0 1 a1 npmultiplya1 d1 step 3 shut neuron a1 a1 a1keepprob step 4 scale valu neuron havent shut backwardpropagationwithdropoutda2 npmultiplyda2 d2 step 1 appli mask d2 shut neuron forward propag da2 da2 keepprob step 2 scale valu neuron havent shut
926,"Data Augmentation

When there is less data and more data can not be made available (generally in case of images), we generate more data by twisting, rotating or zooming the orginal iamge. This is an useful technique to reduce variance",data augment,"['data', 'augment']",Data Augmentation,less data data made avail general case imag generat data twist rotat zoom orgin iamg use techniqu reduc varianc
927,"Early stopping technique to stop overfitting

When the cost function for train and dev set diverge after certain no. of iterations, then we need to stop training at that iteration.

> This is not a very good technique to deal with overfitting",earli stop techniqu stop overfit,"['earli', 'stop', 'techniqu', 'stop', 'overfit']",Early stopping technique to stop overfitting,cost function train dev set diverg certain iter need stop train iter good techniqu deal overfit
928,"Importance of normalized inputs

When cost function is drawn from normalized inputs, the curve becomes symmetric and gradient descent reaches minima faster",import normal input,"['import', 'normal', 'input']",Importance of normalized inputs,cost function drawn normal input curv becom symmetr gradient descent reach minima faster
929,"Exploding and Vanishing gradient

If there is very large no. of layers, 
Then lower (<1) weights becomes very very small or vanishes when multiplied in every layer to reach final activation and thus the gradient also vanishes (curve of y or J becomes parallel to x or weight axis). Means with the change in feature value there is no change in prediction. 

And higher (>1) weights becomes very very large or explodes when multiplied in every layer to reach final activation and thus the gradient also explodes (curve of y or J becomes perpendicular to x or weight axis).

> Exploding gradient problem (weights and activations are all taking on the value of NaN) can be managed by gradient clippping 

> Vanishing gradient problem of RNN is managed by Gated Recurrent Unit (GRU)",explod vanish gradient,"['explod', 'vanish', 'gradient']",Exploding and Vanishing gradient,larg layer lower 1 weight becom small vanish multipli everi layer reach final activ thus gradient also vanish curv j becom parallel x weight axi mean chang featur valu chang predict higher 1 weight becom larg explod multipli everi layer reach final activ thus gradient also explod curv j becom perpendicular x weight axi explod gradient problem weight activ take valu nan manag gradient clipp vanish gradient problem recurr neural network manag gate recurr unit gru
930,"Gradient checking

Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).

Gradient checking is slow, so we don't want to run it in every iteration of training. We would usually run it only to make sure our code is correct, then turn it off and use backprop for the actual learning process.",gradient check,"['gradient', 'check']",Gradient checking,gradient check verifi close gradient backpropag numer approxim gradient comput use forward propagationgradi check slow dont want run everi iter train would usual run make sure code correct turn use backprop actual learn process
931,"Out of Bag (OOB) score

OOB score is a way of validating the Random forest model.

Here each of the OOB sample rows is passed through every DT that did not contain the OOB sample row in its bootstrap training data and a majority prediction is noted for each row.

And lastly, the OOB score is computed as the number of correctly predicted rows from the out of bag sample.

Occasionally the dataset is not big enough and hence set aside a part of it for validation is unaffordable. 

Consequently, in cases where we do not have a large dataset and want to consume it all as the training dataset, the OOB score provides a good trade-off.",bag oob score,"['bag', 'oob', 'score']",Out of Bag (OOB) score,oob score way valid random forest modelher oob sampl row pass everi dt contain oob sampl row bootstrap train data major predict note rowand last oob score comput number correct predict row bag sampleoccasion dataset big enough henc set asid part valid unafford consequ case larg dataset want consum train dataset oob score provid good tradeoff
932,"Optimization Algorithms

Optimization algorithms in deep learning are as follows:

1. Mini-Batch gradient descent

2. Gradient Descent with momentum (AdaGrad)

3. RMSprop

4. Adam Optimization Algorithm 

5. Learning rate decay",optim algorithm,"['optim', 'algorithm']",Optimization Algorithms,optim algorithm deep learn follows1 minibatch gradient descent2 gradient descent momentum adagrad3 rmsprop4 adam optim algorithm 5 learn rate decay
933,"1. Mini-Batch gradient descent

For extremely large dataset, instead of passing the entire dataset at once, we can pass the dataset in small batches or mini batches.

This will solve the memory issue (RAM) for this extremely large computation

> In case of batch gradient descent, the cost function smoothly decreases with increase in no. of iterations 

> In case of mini-batch gradient descent, the cost function decreases with oscillation with increase in no. of iterations (because here iterations are performed with mini batches. Thus for some iteration cost function increases slightly and for some iteration decreases slightly)

> Average of the training samples produces stable error gradients and convergence.",1 minibatch gradient descent,"['1', 'minibatch', 'gradient', 'descent']",1. Mini-Batch gradient descent,extrem larg dataset instead pass entir dataset pass dataset small batch mini batchesthi solv memori issu ram extrem larg comput case batch gradient descent cost function smooth decreas increas iter case minibatch gradient descent cost function decreas oscil increas iter iter perform mini batch thus iter cost function increas slight iter decreas slight averag train sampl produc stabl error gradient converg
934,"> Notation of Mini-batch

In the superscript of Z, W, X or b if we write in curly braces, it denotes the mini-batch no.

Z{1} means the Z value for the first mini-batch",notat minibatch,"['notat', 'minibatch']",> Notation of Mini-batch,superscript z w x b write cur brace denot minibatch noz1 mean z valu first minibatch
935,"> Choosing our mini-batch size

If mini-batch size=m: Batch gradient descent (we face memory shortage issue)-(here m is the total no. of observations)

If mini-batch size=1: stochastic gradient descent (here we loose the speed of vectorization)

Thus, choosing the mini-batch size in between 1 and m is important

> For <2k data points (for images or audio), we can use mini-batch size=m

> For larger dataset, we typically use mini-batch size=64(2^6), 128(2^7), 256(2^8), 512(2^9) etc.",choos minibatch size,"['choos', 'minibatch', 'size']",> Choosing our mini-batch size,minibatch sizem batch gradient descent face memori shortag issueher total observationsif minibatch size1 stochast gradient descent loos speed vectorizationthus choos minibatch size 1 import 2k data point imag audio use minibatch sizem larger dataset typic use minibatch size6426 12827 25628 51229 etc
936,"2. Gradient Descent with momentum (AdaGrad)

> To speed up our algorithm to reach the global minima, we use weighted gradient in the weight update equation, in place of using simple gradients (dw & db)

Hyperparameter =α

Additional hyperparameter=  β # It controls the amount of history (momentum) 

> With increase in the value of momemtum (β), learning becomes fast to reach the global minima for the cost function

> If β =0, it is like simple gradient descent with lowest learning speed.",2 gradient descent momentum adagrad,"['2', 'gradient', 'descent', 'momentum', 'adagrad']",2. Gradient Descent with momentum (AdaGrad),speed algorithm reach global minima use weight gradient weight updat equat place use simpl gradient dw dbhyperparamet αaddit hyperparamet β control amount histori momentum increas valu momemtum β learn becom fast reach global minima cost function β 0 like simpl gradient descent lowest learn speed
937,"Exponentially weighted averages

weighted avg. of 't' th observation= β*weighted avg. of (t-1) th observation+ (1-β)*value of 't' th observation

-We can smoothen the curve of weighted avg. by taking higer values of β (this can be utilized for noisy data)

> Higher β means giving more imporatance to the past

> Increasing β will shift the weighted avg. line slightly to the right.

> We use a bias correction factor to the weighted avg. to handle the bias in the initial period.

Corrected weighted avg. of 't' th observation= weighted avg. of 't' th observation/(1- β^t)",exponenti weight averag,"['exponenti', 'weight', 'averag']",Exponentially weighted averages,weight avg th observ βweight avg t1 th observ 1βvalu th observationw smoothen curv weight avg take higer valu β util noisi data higher β mean give imporat past increas β shift weight avg line slight right use bias correct factor weight avg handl bias initi periodcorrect weight avg th observ weight avg th observation1 βt
938,"
3. RMSprop

> To speed up our algorithm to reach the global minima, we use RMS gradient in the weight update equation, in place of using simple gradients (dw & db) in Root Mean Square Propagation (RMSProp)

Hyperparameter =β≈0.9

RMSprop shows similar accuracy to that of Adam but with comparatively much larger computation time.",3 rmsprop,"['3', 'rmsprop']","
3. RMSprop",speed algorithm reach global minima use rms gradient weight updat equat place use simpl gradient dw db root mean squar propag rmsprophyperparamet β≈09rmsprop show similar accuraci adam compar much larger comput time
939,"4. Adam Optimization Algorithm 

The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.

It is a combination of Gradient Descent with momentum (AdaGrad) and RMSprop

Adam stands for Adaptive moment estimation

Hyperparameter =β1≈0.9,β2≈0.999,ε=10^-8

> Here, batch gradient descent or mini batch gradient descent can be used.

The adam optimizer shows the best accuracy in a satisfactory amount of time.",4 adam optim algorithm,"['4', 'adam', 'optim', 'algorithm']",4. Adam Optimization Algorithm ,adam optim algorithm extens stochast gradient descent recent seen broader adopt deep learn applic comput vision natur languag processingit combin gradient descent momentum adagrad rmspropadam stand adapt moment estimationhyperparamet β1≈09β2≈0999ε108 batch gradient descent mini batch gradient descent usedth adam optim show best accuraci satisfactori amount time
940,"5. Learning rate decay

SGD wonder arround the global minima, it never reaches global minima.

To solve this issue, if the learning rate (step size) decays with progress, SGD will wonder arround very close vicinity of the global minima.

We can define any function of alpha so that its value decreases with increase in epoch",5 learn rate decay,"['5', 'learn', 'rate', 'decay']",5. Learning rate decay,stochast gradient descent wonder arround global minima never reach global minimato solv issu learn rate step size decay progress stochast gradient descent wonder arround close vicin global minimaw defin function alpha valu decreas increas epoch
941,"Problem of local optima

In neural network, it is unlikely to get stuck in a bad local optima. 

There are large number of weights in neural network, so it is impossible to get all the partial derivates as zero at one point giviing a bad local optima.

> But problem of plateau can make the learning slow",problem local optima,"['problem', 'local', 'optima']",Problem of local optima,neural network unlik get stuck bad local optima larg number weight neural network imposs get partial deriv zero one point givi bad local optima problem plateau make learn slow
942,"Hyperparameters tuning in DNN 

There is a long list of hyperparameters in neural network to get the best set of weights

> To get the best values of hyperparameters we should not use grid search CV (takes all the combinations of X's), we should use random search CV (takes combination of X's randomly) for neural network.

> Tuning scale varies from hyperparameter to hyperparameter and it is not uniform for most of the hyperparameters. Scale may be logarithmic or exponential.

> Retest the values of hyperparameters occationally

> During hyperparameter search, whether we try to babysit one model (“Panda"" strategy) or train a lot of models in parallel (“Caviar"") is largely determined by the amount of computational power we can access",hyperparamet tune deep neural network,"['hyperparamet', 'tune', 'deep', 'neural', 'network']",Hyperparameters tuning in DNN ,long list hyperparamet neural network get best set weight get best valu hyperparamet use grid search cross valid take combin xs use random search cross valid take combin xs random neural network tune scale vari hyperparamet hyperparamet uniform hyperparamet scale may logarithm exponenti retest valu hyperparamet occate hyperparamet search whether tri babysit one model “panda strategi train lot model parallel “caviar larg determin amount comput power access
943,"Batch normalization

This is generally a standardization technique of feature scaling.

We know, normalizing of inputs for a layer is to make the learning faster. So the idea is, we can also normalize the outputs of a layer which are entering to the next layer through activation, to make the learning fastest.

> Thus the 'z' values for a layer are normalized to get z-tilda for that layer and then through activation function it enters the next layer

> Here we introduce two more learnable parameters γ (gamma) and β

> γ and β can be learned using Mini-batch gradient descent, Gradient descent with momentum, RMSprop or Adam, not just with gradient descent.

> γ and β set the mean and variance of the linear variable z[l] of a given layer.

> Batch norm has a slight regularization effect on the model

> In the normalization formula, we use epsilon (ε) to avoid division by zero

z_norm = (z – μ) /√(σ^2 - ε) # z is the output for i th observation

After training a neural network with Batch Norm, at test time, to evaluate the neural network on a new example we should perform the needed normalizations, use μ and σ^2 estimated using an exponentially weighted average across mini-batches seen during training",batch normal,"['batch', 'normal']",Batch normalization,general standard techniqu featur scalingw know normal input layer make learn faster idea also normal output layer enter next layer activ make learn fastest thus z valu layer normal get ztilda layer activ function enter next layer introduc two learnabl paramet γ gamma β γ β learn use minibatch gradient descent gradient descent momentum rmsprop adam gradient descent γ β set mean varianc linear variabl zl given layer batch norm slight regular effect model normal formula use epsilon ε avoid divis zeroznorm z – μ √σ2 ε z output th observationaft train neural network batch norm test time evalu neural network new exampl perform need normal use μ σ2 estim use exponenti weight averag across minibatch seen train
944,"Multi-class classification

> For multi-class classification in neural network, softmax activation function works well",multiclass classif,"['multiclass', 'classif']",Multi-class classification,multiclass classif neural network softmax activ function work well
945,"Deep learning programming framework

Caffe/Caffe2

CNTK

DL4J

Keras (simplest one)

Lasagne

mxnet

PaddlePaddle

TensorFlow

Theano

Torch",deep learn program framework,"['deep', 'learn', 'program', 'framework']",Deep learning programming framework,caffecaffe2cntkdl4jkera simplest onelasagnemxnetpaddlepaddletensorflowtheanotorch
946,"Python coding for DNN

import numpy as np

import tensorflow as tf

# define all the tf.Variables and cost function

train = tf.train.GradientDescentOptimizer(0.01).minimize(my_cost_function)

init = tf.global_variables_initializer()

session = tf.Session()

# for initializing the weights

session.run(init)

print(session.run(w))

# for starting the first iteration

session.run(train)

print(session.run(w))

# run a for loop for 'n' no. of iteration

Alternate session initialization

with tf.Session() as session:
session.run(init)
print(session.run(w))",python code deep neural network,"['python', 'code', 'deep', 'neural', 'network']",Python coding for DNN,import numpi npimport tensorflow tf defin tfvariabl cost functiontrain tftraingradientdescentoptimizer001minimizemycostfunctioninit tfglobalvariablesinitializersess tfsession initi weightssessionruninitprintsessionrunw start first iterationsessionruntrainprintsessionrunw run loop n iterationaltern session initializationwith tfsession session sessionruninit printsessionrunw
947,"Different vector operations in tensor flow

For matrix multiplication,

tf.matmul(m1, m2)

For matrix addition,

tf.add(m1,m2)

Thus Linear Regression equation can be represented as follows:

Y=tf.add(tf.matmul(W,X), b)

For reducing along rows,

tf.reduce_sum(x, 0) 

# means we are squeezing from bottom and top so that two separate rows become one row.",differ vector oper tensor flow,"['differ', 'vector', 'oper', 'tensor', 'flow']",Different vector operations in tensor flow,matrix multiplicationtfmatmulm1 m2for matrix additiontfaddm1m2thus linear regress equat repres followsytfaddtfmatmulwx bfor reduc along rowstfreducesumx 0 mean squeez bottom top two separ row becom one row
948,"Cross entropy loss function (for classification problem), 

C= Σ y(i)*log(y_pred(i)) can be written as follows:

C= tf.reduce_sum(Y*tf.log(out))

",cross entropi loss function classif problem,"['cross', 'entropi', 'loss', 'function', 'classif', 'problem']","Cross entropy loss function (for classification problem), ",c σ yilogypredi written followsc tfreducesumytflogout
949,"Google Text to Speech

from gtts import gTTS 
from IPython.display import Audio 
tts = gTTS('Your are like mango', lang='en') 
tts.save('1.mp3') 
sound_file = '1.mp3'
Audio(sound_file, autoplay=True)",googl text speech,"['googl', 'text', 'speech']",Google Text to Speech,gtts import gtts ipythondisplay import audio tts gttsyour like mango langen ttssave1mp3 soundfil 1mp3 audiosoundfil autoplaytru
950,"Basics Structuring ML project

Structuring ML project means, having clear strategies for creating a good learning model

As there are too many parameters to tune in a neural network, a clear strategy must be decided.",basic structur machin learn project,"['basic', 'structur', 'machin', 'learn', 'project']",Basics Structuring ML project,structur machin learn project mean clear strategi creat good learn modela mani paramet tune neural network clear strategi must decid
951,"Orthogonalization Basics

Orthogonalization is a system design property which ensures that modification of an instruction or an algorithm component (hyperparameter) does not create or propagate side effects to other system components (hyperparameter)",orthogon basic,"['orthogon', 'basic']",Orthogonalization Basics,orthogon system design properti ensur modif instruct algorithm compon hyperparamet creat propag side effect system compon hyperparamet
952,"Fundamental assumptions of supervised learning

1. We can fit training set well on cost function

2. The training set performance generalizes pretty well to the dev/test set (means the trained model generalizes well to new/ unknown data).",fundament assumpt supervis learn,"['fundament', 'assumpt', 'supervis', 'learn']",Fundamental assumptions of supervised learning,1 fit train set well cost function2 train set perform general pretti well devtest set mean train model general well new unknown data
953,"Setting up our goal

1. Single number evaluation metric

For example, if a metric shows multiple numbers of evaluation like precision and recall for different models, it will be difficult for us to decide the best model. But if the metric shows single number evaluation like F1 score, we can decide the best model esily.

2. Satisficing and optimizing metrics

When the requirement of the model is not suitable for single number evaluation metric, then we shall create one optimizing meric and (N-1) satisficing merics meeting different criteria (N is the total no. of metrics). ",set goal,"['set', 'goal']",Setting up our goal,1 singl number evalu metricfor exampl metric show multipl number evalu like precis recal differ model difficult us decid best model metric show singl number evalu like f1 score decid best model esily2 satisf optim metricswhen requir model suitabl singl number evalu metric shall creat one optim meric n1 satisf meric meet differ criteria n total metric
954,"Positional Encoding in Transformer

Position and order of words are the essential parts of any language. They define the grammar and thus the actual semantics of a sentence. Recurrent Neural Networks (RNNs) inherently take the order of word into account; They parse a sentence word by word in a sequential manner. 

But the Transformer architecture ditched the recurrence mechanism in favor of multi-head self-attention mechanism. Avoiding the RNNs’ method of recurrence will result in massive speed-up in the training time. 

As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word. Consequently, there’s still the need for a way to incorporate the order of the words into our model.

The concept of Self-Attention is that given a word, its neighbouring words are used to compute its context by summing up the word values to map the Attention related to that given word.

One possible solution to give the model some sense of order is to add a piece of information to each word about its position in the sentence. We call this “piece of information”, the positional encoding.

> A good positional encoding algorithm should have consistent distance between any two time-steps (word's position in a sentence) for all sentence lengths.

> The algorithm should be able to generalize to longer sentences.

> It must be deterministic",posit encod transform,"['posit', 'encod', 'transform']",Positional Encoding in Transformer,posit order word essenti part languag defin grammar thus actual semant sentenc recurr neural network rnns inher take order word account pars sentenc word word sequenti manner transform architectur ditch recurr mechan favor multihead selfattent mechan avoid rnns method recurr result massiv speedup train time word sentenc simultan flow transform encoderdecod stack model doesn't sens positionord word consequ there still need way incorpor order word modelth concept selfattent given word neighbour word use comput context sum word valu map attent relat given wordon possibl solut give model sens order add piec inform word posit sentenc call “piec information” posit encod good posit encod algorithm consist distanc two timestep word posit sentenc sentenc length algorithm abl general longer sentenc must determinist
955,"Changing dev/test sets and metrics

> If there is any special criteria (say, in no way the model should classify pornographic image as cat) for choosing the best model, then the weightage of that criteria must be included in our evaluation metric (the weightage may also be included in the cost function for the improvement in model performance) ",chang devtest set metric,"['chang', 'devtest', 'set', 'metric']",Changing dev/test sets and metrics,special criteria say way model classifi pornograph imag cat choos best model weightag criteria must includ evalu metric weightag may also includ cost function improv model perform
956,"Comparing to human level performance

> For structured data, our model generally takes less time to reach human level performance

> For natural perception  (audio or images), it takes lot of time to surpass human level performance and reach bayes optimal error level. (There is very less difference between human error and bayes error for natural perception)",compar human level perform,"['compar', 'human', 'level', 'perform']",Comparing to human level performance,structur data model general take less time reach human level perform natur percept audio imag take lot time surpass human level perform reach bay optim error level less differ human error bay error natur percept
957,"Human level error and avoidable bias

Human level error does not indicate to the performance of a single human, it indicates to the performance of whole human kind or team of expert human beings.

> Thus human level error is the closest proxy for bayes error.

> Difference between training error and human level error is the avoidable bias.

> Difference between training error and dev error is the variance

Problems where ML  significantly surpasses human-level performance

-Online advertising

-Product recommendations

-Logistics (predicting transit time)

-Loan approvals",human level error avoid bias,"['human', 'level', 'error', 'avoid', 'bias']",Human level error and avoidable bias,human level error indic perform singl human indic perform whole human kind team expert human be thus human level error closest proxi bay error differ train error human level error avoid bias differ train error dev error varianceproblem machin learn signific surpass humanlevel performanceonlin advertisingproduct recommendationslogist predict transit timeloan approv
958,"Error Analysis

> Look at dev examples (which our model misclassified) to get ideas to improve performance

> Evaluate multiple ideas in parallel preparing a table

> If there is incorrectly labeled data (substantial percentage), clean up incorrectly labeled data 

> Systematic mislabeled data/example in the training set, hamper our model performance. So, we must pay attention.

> But if there is randomly mislabeled data/example, they do not impack our model much.

> If we correct the incorrectly labeled data on the dev set, then we should also correct the incorrectly labeled data in the test set.",error analysi,"['error', 'analysi']",Error Analysis,look dev exampl model misclassifi get idea improv perform evalu multipl idea parallel prepar tabl incorrect label data substanti percentag clean incorrect label data systemat mislabel dataexampl train set hamper model perform must pay attent random mislabel dataexampl impack model much correct incorrect label data dev set also correct incorrect label data test set
959,"Transformer Network

> A Transformer Network can ingest entire sentences (not word by word like RNN and LSTM) all at the same time.

> Transformer Network methodology is taken from Attention Mechanism and CNN style of processing.

> The key inputs for computing the attention value for each word are called the query (Q), key (K), and value (V). 

Q = interesting questions about the words in a sentence, 

K = qualities of words given a Q, 

V = specific representations of words given a Q

> Output of the encoder block contain contextual semantic embedding and positional encoding information

> Output of the decoder block contain Linear layer (fully connected layer) followed by a softmax layer",transform network,"['transform', 'network']",Transformer Network,transform network ingest entir sentenc word word like recurr neural network lstm time transform network methodolog taken attent mechan convolut neural network style process key input comput attent valu word call queri q key k valu v q interest question word sentenc k qualiti word given q v specif represent word given q output encod block contain contextu semant embed posit encod inform output decod block contain linear layer fulli connect layer follow softmax layer
960,"Mismatched training and dev/test data (if the training and dev data are from different distribution)

If we have a large dataset, then we should mix and shuffle randomly all the data coming from different regions before train/dev/test distribution.

Most Deep Learning models are robust enough to allow for slightly different distributions between train and dev/test.

Choose a dev set and test set to reflect data we expect to get in the future and consider important to do well on. (If there is some distribution of data which is expected to get during model deployment, testing data must come from that distribution)

> Size of Dev set

Size of the dev set to be big enough to detect differences in algorithm/models we are trying out and select the best model.

> Size of test set

Size of the test set to be big enough to give high confidence in the overall performance of our selected model.

> In this case, how can we identify how much error is due to variance and how much error is due to mismatched data

Here the concept of Training-dev set is introduced.  Training-dev set has same distribution as training set, but not used for training

> Then compare the errors for all the sets for clear understanding

If we have a large data-mismatch problem, then our model does a lot better on the training-dev set than on the dev set",mismatch train devtest data train dev data differ distribut,"['mismatch', 'train', 'devtest', 'data', 'train', 'dev', 'data', 'differ', 'distribut']",Mismatched training and dev/test data (if the training and dev data are from different distribution),larg dataset mix shuffl random data come differ region traindevtest distributionmost deep learn model robust enough allow slight differ distribut train devtestchoos dev set test set reflect data expect get futur consid import well distribut data expect get model deploy test data must come distribut size dev setsiz dev set big enough detect differ algorithmmodel tri select best model size test setsiz test set big enough give high confid overal perform select model case identifi much error due varianc much error due mismatch dataher concept trainingdev set introduc trainingdev set distribut train set use train compar error set clear understandingif larg datamismatch problem model lot better trainingdev set dev set
961,"Learning from multiple tasks
 
Ways to learn from multiple tasks:

1. Transfer learning

2. Multi-task learning",learn multipl task way learn multipl task,"['learn', 'multipl', 'task', 'way', 'learn', 'multipl', 'task']","Learning from multiple tasks
 
Ways to learn from multiple tasks:",1 transfer learning2 multitask learn
962,"1. Transfer learning 

(here learned weights for one task is utilized for classifying another task)

Transfer Learning is a machine learning technique where we use a pre-trained neural network to solve a problem that is similar to the problem the network was originally trained to solve. We can use partial network and edit the final layer to fit our objectives.

Many pre-trained models are available online.

e.g. LeNet-5, AlexNet, VGG, ResNet, Inception etc.

When transfer learning from Task A to Task B makes sense?

> Task A&B has same input X

> We have lot more data for Task A than Task B

> Lower level features from A could be helpful for learning B

(Low-level features are minor details of the image, like lines or dots, that can be picked up.  High-level features are built on top of low-level features to detect objects and larger shapes in the image)

> For transfer learning in CNN, people always borrow the entire 20+ layers networks and only train the last output layer to customize towards their use cases. This is not the case for LSTM with different vocabularies. But widely used in (NLP) LSTM with similar vocabularies

> Transfer learning from CNN to RNN is not possible",1 transfer learn,"['1', 'transfer', 'learn']",1. Transfer learning ,learn weight one task util classifi anoth tasktransf learn machin learn techniqu use pretrain neural network solv problem similar problem network origin train solv use partial network edit final layer fit objectivesmani pretrain model avail onlineeg lenet5 alexnet vgg resnet incept etcwhen transfer learn task task b make sens task ab input x lot data task task b lower level featur could help learn blowlevel featur minor detail imag like line dot pick highlevel featur built top lowlevel featur detect object larger shape imag transfer learn convolut neural network peopl alway borrow entir 20 layer network train last output layer custom toward use case case long short term memori differ vocabulari wide use nlp long short term memori similar vocabulari transfer learn convolut neural network recurr neural network possibl
963,"2. Multi-task learning

> For identifying multiple objects from a picture, we can utilize multiple neural network with binary classification.

> Else, we can utilize single neural network with multi-task learning. Here the final layer will be have no. of nodes equal to no. of tasks

> It is not particularly feasible to build one dataset to rule them all that is fully labeled with every category we would ever need.

use of multi-task neural network 

> Training on a set of tasks that could benefit from having shared lower-level features.

> Usually, amount of data we have for each task is quite similar",2 multitask learn,"['2', 'multitask', 'learn']",2. Multi-task learning,identifi multipl object pictur util multipl neural network binari classif els util singl neural network multitask learn final layer node equal task particular feasibl build one dataset rule fulli label everi categori would ever needus multitask neural network train set task could benefit share lowerlevel featur usual amount data task quit similar
964,"End to end deep learning

> The biggest advantage of deep learning is that, if we have big enough data, there is no need of feature engineering and lots of human efforts to develop a model, a neural network can do every thing to achieve the goal. This is called end to end deep learning.

Applying end-to-end deep learning

> Key question: Do we have sufficient data to learn a function of the complexity need to map x to y?",end end deep learn,"['end', 'end', 'deep', 'learn']",End to end deep learning,biggest advantag deep learn big enough data need featur engin lot human effort develop model neural network everi thing achiev goal call end end deep learningappli endtoend deep learn key question suffici data learn function complex need map x
965,"Pros and cons of end to end deep learning

Pros: 

> Let the data speak

> Less hand designing of components needed

Cons:

> May need large amount of data

> Excludes potentially useful hand-designed components which may be useful to improve model performance",pros con end end deep learn,"['pros', 'con', 'end', 'end', 'deep', 'learn']",Pros and cons of end to end deep learning,pros let data speak less hand design compon neededcon may need larg amount data exclud potenti use handdesign compon may use improv model perform
966,"Difference between Multi-class and multi-task learning

Multi-class learning is the terminology used when we predict a single label for each input, but each label is a single element from a set of possible labels (e.g. for an image possible labels could be child, adult or old). 

Multi-task learning is when we have different problems that need to be solved simultaneously (multiple labels for each input, e.g. identifying road, traffic signal and car in a single image)",differ multiclass multitask learn,"['differ', 'multiclass', 'multitask', 'learn']",Difference between Multi-class and multi-task learning,multiclass learn terminolog use predict singl label input label singl element set possibl label eg imag possibl label could child adult old multitask learn differ problem need solv simultan multipl label input eg identifi road traffic signal car singl imag
967,"Testing the model for the entire dataset

This would cause the dev and test set distributions to become different. This is a bad idea because we’re not aiming where we want to hit

If a teacher is teaching mathematics to a child. She teaches him some examples such as 1+1=2, 2+2=4 and 1+3 = 4. If she takes a test of that child and asks him only 1+1 or 2+2 or 1+3, he will give the exact answer. Based on this test she can not tell if the child has learned mathematics or not.",test model entir dataset,"['test', 'model', 'entir', 'dataset']",Testing the model for the entire dataset,would caus dev test set distribut becom differ bad idea we'r aim want hitif teacher teach mathemat child teach exampl 112 224 13 4 take test child ask 11 22 13 give exact answer base test tell child learn mathemat
968,"Perceptual task 

Perceptual tasks consist of studies aimed at distinguishing various biomarkers of perception including visual-spatial attention, auditory attention and olfactory attention

Here, perceptual task means computer vision through CNN

> CNN is used in

1. Image classification

2. Text classification

3. Object Detection

> A Convolution matches or surpasses the output of an individual neuron to a visual stimuli. 

> Convolution means a thing that is complex and difficult to follow

> When a high resolution image (3300*4200) is used in a neural network, for RGB channels (for black and white screen, there is only one channel which denotes the intensity of white), it will be a huge number of feature inputs (3300*4200*3) for one  observation, that is why for simplification CNN came into picture.",perceptu task,"['perceptu', 'task']",Perceptual task ,perceptu task consist studi aim distinguish various biomark percept includ visualspati attent auditori attent olfactori attentionher perceptu task mean comput vision convolut neural network convolut neural network use in1 imag classification2 text classification3 object detect convolut match surpass output individu neuron visual stimuli convolut mean thing complex difficult follow high resolut imag 33004200 use neural network rgb channel black white screen one channel denot intens white huge number featur input 330042003 one observ simplif convolut neural network came pictur
969,"Basics of Pixel 

Pixel  means a minute area of illumination on a display screen, one of many from which an image is composed.

The smallest resolution Windows supports is 640x480 pixels (meaning 640 dots horizontally by 480 vertically). Better video cards and monitors are capable of much higher resolutions. The standard resolution used today is 1024(W)x768(H).

Say computer monitor display area is 12'(W)x7'(H).

Then ppi (pixels per inch) in width= 1024/12=85

But 72 ppi is the standard

A 12-megapixel camera (3300 x 4200 pixel), for example, can produce images with more than 12 million total pixels.

Considering 300 ppi for the best quality print, maximum size of the print will be

W=3300/300=11'

H=4200/300=14'

Early digital camera: 100x100 pixels (0.01 megapixels)",basic pixel,"['basic', 'pixel']",Basics of Pixel ,pixel mean minut area illumin display screen one mani imag composedth smallest resolut window support 640x480 pixel mean 640 dot horizont 480 vertic better video card monitor capabl much higher resolut standard resolut use today 1024wx768hsay comput monitor display area 12wx7hthen ppi pixel per inch width 10241285but 72 ppi standarda 12megapixel camera 3300 x 4200 pixel exampl produc imag 12 million total pixelsconsid 300 ppi best qualiti print maximum size print bew330030011h420030014ear digit camera 100x100 pixel 001 megapixel
970,"Convolution on Black-and-white Image

For Edge detection, we use filters (e.g. 3x3 matrix) to perform convolution operation with the picture intensity matrix:

The weights of the filter are not defined. We leave it to the model to learn by itself.

There are different types of vertical and horizontal fileters like Sobel filter, Scharr filter etc.

Here, pixel locations are the feature names like cell reference in excel. 

Thus, total no. of pixel= total no. of feature= total no. of inputs

All the values of one picture intensity matrix are the feature values for one observation ",convolut blackandwhit imag,"['convolut', 'blackandwhit', 'imag']",Convolution on Black-and-white Image,edg detect use filter eg 3x3 matrix perform convolut oper pictur intens matrixth weight filter defin leav model learn itselfther differ type vertic horizont filet like sobel filter scharr filter etcher pixel locat featur name like cell refer excel thus total pixel total featur total inputsal valu one pictur intens matrix featur valu one observ
971,"Convolution operation

> convolution operator is *

Say, picture intensity matrix is I with size 4X4 and filter size is 2X2 

I*F=resultant matrix

Then, the first element of the resultant matrix will be

Z11= I11*F11+I12*F12+I21*F21+I22*F22

>> Here, I11,I12,I21,I22 are the raw feature values(raw x's) and F11,F12,F21,F22 are the learnable weights(w's), Z11,Z12,Z13.. are the refined feature values (refined x's)

Here the equation of Z is similar like linear regression equation. It  is only a refinement of raw data (picture intensity matrix) to refined data (output matrix)

> Convolution operation can be compared with PCA as it is used for better understanding of important features",convolut oper,"['convolut', 'oper']",Convolution operation,convolut oper say pictur intens matrix size 4x4 filter size 2x2 ifresult matrixthen first element result matrix bez11 i11f11i12f12i21f21i22f22 i11i12i21i22 raw featur valuesraw xs f11f12f21f22 learnabl weightsw z11z12z13 refin featur valu refin xshere equat z similar like linear regress equat refin raw data pictur intens matrix refin data output matrix convolut oper compar princip compon analysi use better understand import featur
972,"Padding in CNN

During convolution operation, the size of the intensity matrix is reduced. This is not a best practice because we are loosing informations.

Thus the concept of padding came into picture to retain the size of input matrix.

> Here before performing convolution operation, we are padding the input intensity matrix with zeros (in one or multiple layers depending on the filter size) in all sides equally

> minimum filter size is 3X3 for convolution operation with padding

> Valid Convolution = convolution with no padding (size of the resultant volume is reduced)

> Same Convolution = convolution with padding (size of input volume and size of resulting volume are same). Also known as  Zero Padding or Same padding

> A 3D matrix is called volume",pad convolut neural network,"['pad', 'convolut', 'neural', 'network']",Padding in CNN,convolut oper size intens matrix reduc best practic loos informationsthus concept pad came pictur retain size input matrix perform convolut oper pad input intens matrix zero one multipl layer depend filter size side equal minimum filter size 3x3 convolut oper pad valid convolut convolut pad size result volum reduc convolut convolut pad size input volum size result volum also known zero pad pad 3d matrix call volum
973,"Strided Convolution

Stride or step size is an extra hyperparameter, stride = 2,3, etc.

> Size of the input matrix is very much reduced in this case

> Stride is the distance between two consecutive receptive fields",stride convolut,"['stride', 'convolut']",Strided Convolution,stride step size extra hyperparamet stride 23 etc size input matrix much reduc case stride distanc two consecut recept field
974,"Convolution on RGB image (Coloured image)

Here instead of 2D filter we need 3D filter (e.g. 3x3x3 matrix) but the output is a 2D matrix

If we need 3D matrix output instead of 2D matrix output, then we need to apply mutiple 3D filters",convolut rgb imag colour imag,"['convolut', 'rgb', 'imag', 'colour', 'imag']",Convolution on RGB image (Coloured image),instead 2d filter need 3d filter eg 3x3x3 matrix output 2d matrixif need 3d matrix output instead 2d matrix output need appli mutipl 3d filter
975,"Types of layer in a convolutional network:

1. Convolution (CONV) layer-it has parameters to learn 

2. Pooling (POOL) layer- may not be considered as layer, as no parameters to learn (only takes the max. or avg. value from the input volume according to filter size and stride)

Though they do not have parameters to learn, but they affect the backpropagation (derivatives) calculation.

3. Fully connected (FC) layer-This is nothing but a single layer of standard NN, it has parameters to learn",type layer convolut network,"['type', 'layer', 'convolut', 'network']",Types of layer in a convolutional network:,1 convolut conv layerit paramet learn 2 pool pool layer may consid layer paramet learn take max avg valu input volum accord filter size stridethough paramet learn affect backpropag deriv calculation3 fulli connect fc layerthi noth singl layer standard nn paramet learn
976,"Pooling Layer

1. Max pooling-it is similar like convolution operation but instead of taking the summation of all the numbers, here max number is considered in the output matrix.

Here hyperparameters are: Filter size(f) and stride (s)

max-pooling layer is to create a feature map containing the most prominent features of the previous feature map (also adds local invariance)

from keras.layers import MaxPooling2D

2. Average Pooling (rarely used)-instead of max we take average",pool layer,"['pool', 'layer']",Pooling Layer,1 max poolingit similar like convolut oper instead take summat number max number consid output matrixher hyperparamet filter sizef stride smaxpool layer creat featur map contain promin featur previous featur map also add local invariancefrom keraslay import maxpooling2d2 averag pool rare usedinstead max take averag
977,"Necessity of Convolution

> By the use of CONV+POOL layer we are actually reducing the number of inputs features and then using a FC layer. Thus the parameter to learn decreases significantly

> CNNs have a couple of concepts called parameter sharing and sparsity of connections

1. Parameter sharing

- It reduces the total number of parameters, thus reducing overfitting.

- It allows a feature detector to be used in multiple locations throughout the whole input image/input volume.

2. Sparsity of connections (helps in translation invariance)-
means each activation in the next layer depends only on a small number of activations from the previous layer.

As the number of features are extreamly large for an image, CONV+POOL layers are used combinedly and repeatedly as a feature reduction technique

>> Training a deeper network (adding additional layers to the network) allows the network to fit more complex functions (or complex feature) and thus almost always results in lower training error.",necess convolut,"['necess', 'convolut']",Necessity of Convolution,use convpool layer actual reduc number input featur use fc layer thus paramet learn decreas signific cnns coupl concept call paramet share sparsiti connections1 paramet share reduc total number paramet thus reduc overfit allow featur detector use multipl locat throughout whole input imageinput volume2 sparsiti connect help translat invari mean activ next layer depend small number activ previous layera number featur extream larg imag convpool layer use combin repeat featur reduct techniqu train deeper network ad addit layer network allow network fit complex function complex featur thus almost alway result lower train error
978,"Calulating the size of output volume for convolution or pooling

say, 
size of input matrix= W X H

size of filter = F(W) X F(H)

stride = S(W), S(H)

Padding= P

Then,
Output width = (W-F(W)+2P)/S(W)+1

Output height = (H-F(H)+2P)/S(H)+1

say, 
size of input volume= W X H XC

size of filter = F(W) X F(H) XF(C)

stride = S(W), S(H)

Padding= P

no. of filters= n

If, C=F(C), equal no. of channels for input volume and filter,

Then, size of output volume,
{(W-F(W)+2P)/S(W)+1}X {(H-F(H)+2P)/S(H)+1} Xn

> If 2D pooling is applied on 3D input, output channels will be= input channels, C

> For an input volume 30X30X3 no. of inputs is 2700

> For an input volume 15x15x8, if pad=2, the dimension of the resulting volume is 19x19x8 (2+15+2=19)

> For an input volume 63x63x16, if filtered with 7x7 (f X f), stride of 1 and “same” convolution, then padding=3  
Because, (f-1)/2 = (7-1)/2=3

> For an input volume 32x32x16, and apply max pooling with a stride of 2 and a filter size of 2. The output volume is 16x16x16

> Say, input is a 300 by 300 color (RGB) image, and we use a convolutional layer with 100 filters that are each 5x5. Then the hidden layer have 7600 parameters (including bias) 7500 (5X5X3X100) (without bias)

> For an input volume 63x63x16, and convolve it with 32 filters that are each 7x7, using a stride of 2 and no padding. The output volume is 29x29x32

> Suppose we have an input volume of dimension 64x64x16. Then a single 1x1 convolutional filter have 17 parameters (including the bias)",calul size output volum convolut pool,"['calul', 'size', 'output', 'volum', 'convolut', 'pool']",Calulating the size of output volume for convolution or pooling,say size input matrix w x hsize filter fw x fhstride sw shpad pthen output width wfw2psw1output height hfh2psh1say size input volum w x h xcsize filter fw x fh xfcstride sw shpad pno filter nif cfc equal channel input volum filterthen size output volum wfw2psw1x hfh2psh1 xn 2d pool appli 3d input output channel input channel c input volum 30x30x3 input 2700 input volum 15x15x8 pad2 dimens result volum 19x19x8 215219 input volum 63x63x16 filter 7x7 f x f stride 1 “same” convolut padding3 f12 7123 input volum 32x32x16 appli max pool stride 2 filter size 2 output volum 16x16x16 say input 300 300 color rgb imag use convolut layer 100 filter 5x5 hidden layer 7600 paramet includ bias 7500 5x5x3x100 without bias input volum 63x63x16 convolv 32 filter 7x7 use stride 2 pad output volum 29x29x32 suppos input volum dimens 64x64x16 singl 1x1 convolut filter 17 paramet includ bias
979,"Notation for multiple CONV layers

> First feature map or the input volume 39X39X3

n_W[0]=n_H[0]=39, n_C[0]=3

> First CONV layer

f[1]=3, s[1]=1, P[1]=0, n=10

> Second feature map 37X37X10

n_W[1]=n_H[1]=37, n_C[1]=10

> Second CONV layer

f[2]=5, s[2]=2, P[2]=0, n=20

> Here f[1]=3 means 3X3 and equal channel as input volume 
and s[1]=1 means 1,1",notat multipl conv layer,"['notat', 'multipl', 'conv', 'layer']",Notation for multiple CONV layers,first featur map input volum 39x39x3nw0nh039 nc03 first conv layerf13 s11 p10 n10 second featur map 37x37x10nw1nh137 nc110 second conv layerf25 s22 p20 n20 f13 mean 3x3 equal channel input volum s11 mean 11
980,"Common behavior of all the CNN architecture

Dimensions of the intensity matrix changes as follows:

H and W reduces and Depth increases with the progress in layer.

Then the intensity matrix is flattened out and used in FC layer for final output. ",common behavior convolut neural network architectur,"['common', 'behavior', 'convolut', 'neural', 'network', 'architectur']",Common behavior of all the CNN architecture,dimens intens matrix chang followsh w reduc depth increas progress layerthen intens matrix flatten use fc layer final output
981,"LeNet-5 architecture

It was intially built to identify hand written digits (black-and-white)

It is an old architecture (sigmoid and tanh activation used)

AVG POOL is used

Output layer-softmax",lenet5 architectur,"['lenet5', 'architectur']",LeNet-5 architecture,intial built identifi hand written digit blackandwhiteit old architectur sigmoid tanh activ usedavg pool usedoutput layersoftmax
982,"AlexNet architecture

Modern architecture

MAX POOL is used

ReLU activation is being used

Output layer-softmax",alexnet architectur,"['alexnet', 'architectur']",AlexNet architecture,modern architecturemax pool usedrelu activ usedoutput layersoftmax
983,"VGG-16 architecture

Modern architecture 

Multiple CONV layers are being used

16 signifies 16 numbers of layers. It is a very big network.

ReLU activation is being used

Output layer-softmax

",vgg16 architectur,"['vgg16', 'architectur']",VGG-16 architecture,modern architectur multipl conv layer used16 signifi 16 number layer big networkrelu activ usedoutput layersoftmax
984,"ResNet and Inception architecture

More Advance and Complex Networks

1. ResNet- Residual Network

> ResNet helps NN to learn identity function/features very easily

> The skip-connection makes it easy for the network to learn an identity mapping between the input and the output within the ResNet block.

> Using a skip-connection helps the gradient to backpropagate fast and thus helps us to train deeper networks

2. Inception

> A single inception block allows the network to use a combination of 1x1, 3x3, 5x5 convolutions and pooling.

> Inception blocks usually use 1x1 convolutions to reduce the input data volume’s size before applying 3x3 and 5x5 convolutions.

> By adding bottleneck layers we can reduce the computational cost in the inception modules.",resnet incept architectur,"['resnet', 'incept', 'architectur']",ResNet and Inception architecture,advanc complex networks1 resnet residu network resnet help neural network learn ident functionfeatur easili skipconeur networkect make easi network learn ident map input output within resnet block use skipconeur networkect help gradient backpropag fast thus help us train deeper networks2 incept singl incept block allow network use combin 1x1 3x3 5x5 convolut pool incept block usual use 1x1 convolut reduc input data volum size appli 3x3 5x5 convolut ad bottleneck layer reduc comput cost incept modul
985,"Network in Network

Network in Network is actually 1X1 convolution

> Mainly used for 3D pictures

> for 3D picture there are more than 3 channels, in fact lot many channels

> 1X1 convolution is used to decrease the no. of channels (or depth)

> It saves huge computational cost

We can use ConvNet for 1D image like ECG

We can also use ConvNet for 3D image like CT Scan image

e.g. 
For 3D image, input volume has size 32x32x32x16 (this volume has 16 channels)",network network,"['network', 'network']",Network in Network,network network actual 1x1 convolut main use 3d pictur 3d pictur 3 channel fact lot mani channel 1x1 convolut use decreas channel depth save huge comput costw use convnet 1d imag like ecgw also use convnet 3d imag like ct scan imageeg 3d imag input volum size 32x32x32x16 volum 16 channel
986,"Using open-source implementation

For computer vision, always try to use as much open source implementation as possible

1. Building on top of other's implementation

Search for network architecture source code on google (Go to Github repo and download or gitclone)

2. Use in-built implementations present in deep learning frameworks

Tensorflow, Keras, Pytorch etc.

3. Use transfer learning as much as possible

Transfer learning is very useful when we have very less inputs (say 100 pictures)

When having a small training set to construct a classification model, use an open-source network trained in a larger dataset, freezing the layers and re-train the softmax layer.",use opensourc implement,"['use', 'opensourc', 'implement']",Using open-source implementation,comput vision alway tri use much open sourc implement possible1 build top other implementationsearch network architectur sourc code googl go github repo download gitclone2 use inbuilt implement present deep learn frameworkstensorflow kera pytorch etc3 use transfer learn much possibletransf learn use less input say 100 pictureswhen small train set construct classif model use opensourc network train larger dataset freez layer retrain softmax layer
987,"Common Augmentation methods

> Mirroring

> Cropping

> Rotation

> Blurring

> Shearing

> Local warping

> Colour shifting etc.",common augment method,"['common', 'augment', 'method']",Common Augmentation methods,mirror crop rotat blur shear local warp colour shift etc
988,"Sources of data for any ML model

There are two sources of data for any ML model

1. Input data (raw/structured)

2. Engineered or hand engineered or synthesised data",sourc data machin learn model,"['sourc', 'data', 'machin', 'learn', 'model']",Sources of data for any ML model,two sourc data machin learn model1 input data rawstructured2 engin hand engin synthesis data
989,"Object detection 

Object detection (multi-task) requires less data than image recognition /classification (multi-class)

Speech recognition model need much more data than image recognition/ classification model",object detect,"['object', 'detect']",Object detection ,object detect multitask requir less data imag recognit classif multiclassspeech recognit model need much data imag recognit classif model
990,"Tips for winning competitions

> Ensembling: Train several networks independently and average their outputs

> Multi-crop at test time: Run clssifier on multiple versions of test images and average their results",tip win competit,"['tip', 'win', 'competit']",Tips for winning competitions,ensembl train sever network independ averag output multicrop test time run clssifier multipl version test imag averag result
991,"Importing Kaggle dataset in colab

> Register in kaggle.com

> Go to account section

> click on ""Create New API Token"" and a kaggle.json file will be downloaded

> Open colab and the code

!pip install -q kaggle

from google.colab import files
files.upload()

and choose the kaggle.json file to upload in session

> Create a kaggle folder

!mkdir ~/.kaggle

> Copy the kaggle.json file into the folder

!cp kaggle.json ~/.kaggle/

> Change file permission

!chmod 600 ~/.kaggle/kaggle.json

> List kaggle datasets

!kaggle datasets list

> kaggle.com> competition> data> Copy API command and run in colab (make sure you have participated in the relevant competition)

!kaggle competitions download -c ubiquant-market-prediction

> run upzip command to unzip

(Easy way is download the data from kaggle.com> upload in google drive> connect colab to drive> read the data)",import kaggl dataset colab,"['import', 'kaggl', 'dataset', 'colab']",Importing Kaggle dataset in colab,regist kagglecom go account section click creat new applic program interfac token kagglejson file download open colab codepip instal q kagglefrom googlecolab import file filesuploadand choos kagglejson file upload session creat kaggl foldermkdir kaggl copi kagglejson file foldercp kagglejson kaggl chang file permissionchmod 600 kagglekagglejson list kaggl datasetskaggl dataset list kagglecom competit data copi applic program interfac command run colab make sure particip relev competitionkaggl competit download c ubiquantmarketpredict run upzip command unzipeasi way download data kagglecom upload googl drive connect colab drive read data
992,"Common steps for pre-processing Image Data

> Connection with the data

# Importing libraries (needs to be imported as required)

import numpy as np
import copy
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
from lr_utils import load_dataset
from public_tests import *

%matplotlib inline
%load_ext autoreload
%autoreload 2

train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()

> First feelings of the data

index = 25

plt.imshow(train_set_x_orig[index])

print (""y = "" + str(train_set_y[:, index]) + "", it's a '"" + classes[np.squeeze(train_set_y[:, index])].decode(""utf-8"") +  ""' picture."")

> Deeper understanding of the data: Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)

m_train = train_set_x_orig.shape[0]

m_test = test_set_x_orig.shape[0]

num_px = train_set_x_orig.shape[1]

> Reshape the datasets such that each example is now a vector of size (num_px \* num_px \* 3, 1)

train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T

test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

> ""Standardize"" the data (255 is the maximum value of a pixel channel)

train_set_x = train_set_x_flatten / 255

test_set_x = test_set_x_flatten / 255",common step preprocess imag data,"['common', 'step', 'preprocess', 'imag', 'data']",Common steps for pre-processing Image Data,connect data import librari need import requiredimport numpi np import copi import matplotlibpyplot plt import h5pi import scipi pil import imag scipi import ndimag lrutil import loaddataset publictest import matplotlib inlin loadext autoreload autoreload 2trainsetxorig trainseti testsetxorig testseti class loaddataset first feel dataindex 25pltimshowtrainsetxorigindexprint strtrainseti index classesnpsqueezetrainseti indexdecodeutf8 pictur deeper understand data figur dimens shape problem mtrain mtest numpx mtrain trainsetxorigshape0mtest testsetxorigshape0numpx trainsetxorigshape1 reshap dataset exampl vector size numpx numpx 3 1trainsetxflatten trainsetxorigreshapetrainsetxorigshape0 1ttestsetxflatten testsetxorigreshapetestsetxorigshape0 1t standard data 255 maximum valu pixel channeltrainsetx trainsetxflatten 255testsetx testsetxflatten 255
993,"Building blocks of deep learning model

> Exploration and pre-processing of data

> Writing helper functions (sigmoid, initialize_with_zeros, propagate, optimize, predict etc.)

> Initialize (w,b)

> Optimize the loss iteratively to learn parameters (w,b):

-Computing the cost and its gradient

-Updating the parameters using gradient descent

> Use the learned (w,b) to predict the labels for a given set of examples

> Then, we merge all helper functions into a model

Tuning the learning rate (hyperparameter) can make a big difference to the algorithm.",build block deep learn model,"['build', 'block', 'deep', 'learn', 'model']",Building blocks of deep learning model,explor preprocess data write helper function sigmoid initializewithzero propag optim predict etc initi wb optim loss iter learn paramet wbcomput cost gradientupd paramet use gradient descent use learn wb predict label given set exampl merg helper function modeltun learn rate hyperparamet make big differ algorithm
994,"Difference between image classification and object detection

Image Classification helps us to classify what is contained in an image. 

Image Classification with Localization will specify the location of single object in an image whereas 

Object Detection specifies the location of multiple objects in the image. ",differ imag classif object detect,"['differ', 'imag', 'classif', 'object', 'detect']",Difference between image classification and object detection,imag classif help us classifi contain imag imag classif local specifi locat singl object imag wherea object detect specifi locat multipl object imag
995,"Image Classification with Localization

Image Classification with Localization is fully different from semantic segmentation (Locating objects in an image by predicting each pixel as to which class it belongs to)

It is similar to ConvNet (image classification) but output is slightly different.

Need to output- 

1. whether any object is present or not

2. center of the bounding box

3. height and width of the bounding box and

4. class labels 

So, y will be a vector

Y=[1(logistic unit,P_c),bx,by,bh,bw,1(class-c1),0(class-c2)]

If the logistic unit of y vector= 0, means no class found
Then we don't care (?) about other terms

y=[0,?,?,?,?,?,?]

size of y is 1 X (5+no. of classes) 

size of y is 1 X 7 for 2 classes, 1 X 25 for 20 classes",imag classif local,"['imag', 'classif', 'local']",Image Classification with Localization,imag classif local fulli differ semant segment locat object imag predict pixel class belong toit similar convnet imag classif output slight differentne output 1 whether object present not2 center bound box3 height width bound box and4 class label vectory1logist unitpcbxbybhbw1classc10classc2if logist unit vector 0 mean class found dont care termsy0s 1 x 5no class size 1 x 7 2 class 1 x 25 20 class
996,"Landmark detection

This is similar to Image classification with localization.

Here, we can analyze the object in the image by defining multiple landmarks (points) during training

All the landmarks are like individual task. Thus, if there are N landmarks, there will  be 2N (yes+no) output units

y_pred has shape (2N, 1)",landmark detect,"['landmark', 'detect']",Landmark detection,similar imag classif localizationher analyz object imag defin multipl landmark point trainingal landmark like individu task thus n landmark 2n yesno output unitsypr shape 2n 1
997,"Convolutional implementation of sliding windows for detecting multiple objects

> If we take sliding window or multiple cropped image for detecting multiple objects in a picture and then pass all the cropped images to the same ConvNet, then this will become computationally very expensive

> To substantially reduce the computational cost, there is convolutional implementation of sliding windows where single image is fed to a single ConvNet",convolut implement slide window detect multipl object,"['convolut', 'implement', 'slide', 'window', 'detect', 'multipl', 'object']",Convolutional implementation of sliding windows for detecting multiple objects,take slide window multipl crop imag detect multipl object pictur pass crop imag convnet becom comput expens substanti reduc comput cost convolut implement slide window singl imag fed singl convnet
998,"Object detection algorithm

Steps towards outputting a single and accurate bounding box

1. YOLO algorithm

2. Intersection over union (IoU)

3. Non-max supression

4. Anchor box algorithm",object detect algorithm,"['object', 'detect', 'algorithm']",Object detection algorithm,step toward output singl accur bound box1 yolo algorithm2 intersect union iou3 nonmax supression4 anchor box algorithm
999,"1. YOLO algorithm

(for finding the bounding box)

YOLO stands for You Only Look Once

> The biggest advantage of using YOLO is its superb speed – it’s incredibly fast and can process 45 frames per second. 

> For each grid cell, there is label for training

> Here, y is a 3D array",1 yolo algorithm,"['1', 'yolo', 'algorithm']",1. YOLO algorithm,find bound boxyolo stand look biggest advantag use yolo superb speed – it incred fast process 45 frame per second grid cell label train 3d array
1000,"Steps in YOLO algorithm

> YOLO first takes an input image

> The framework then divides the input image into grids (say a 19 X 19 grid)

> Image classification with localization are applied on each grid. YOLO then predicts the bounding boxes and their corresponding class probabilities for objects 

Say, on any input volume 19x19 grid is applied, for 20 classes, and with 5 anchor boxes. 

output volume for 19X 19 grid and 20 classes will be 19 X19X25

As there are 5 anchor boxes, then output volume will be 19x19x(5x25)",step yolo algorithm,"['step', 'yolo', 'algorithm']",Steps in YOLO algorithm,yolo first take input imag framework divid input imag grid say 19 x 19 grid imag classif local appli grid yolo predict bound box correspond class probabl object say input volum 19x19 grid appli 20 class 5 anchor box output volum 19x 19 grid 20 class 19 x19x25as 5 anchor box output volum 19x19x5x25
1001,"2. Intersection over union  algorithm

(for finding accurate bounding box)

Intersection over union (IoU)= size of intersection/size of union (between two bounding boxes)

correct if IoU >=0.5

If, the upper-left box is 2x2, and the lower-right box is 2x3. The overlapping region is 1x1, Then IoU=1*1/(2*2+2*3-1*1)=1/9",2 intersect union algorithm,"['2', 'intersect', 'union', 'algorithm']",2. Intersection over union  algorithm,find accur bound boxintersect union iou size intersections union two bound boxescorrect iou 05if upperleft box 2x2 lowerright box 2x3 overlap region 1x1 iou1122231119
1002,"3. Non-max supression

(for finding single bounding box)

When there are multiple bounding boxes for same object, then this algorithm ignores the non-max bounding boxes (probabillity of less than maximum).

Finally we get non-max supressed output",3 nonmax supress,"['3', 'nonmax', 'supress']",3. Non-max supression,find singl bound boxwhen multipl bound box object algorithm ignor nonmax bound box probabil less maximumfin get nonmax supress output
1003,"4. Anchor box algorithm

If there are multiple objects in a single grid and overlapping objects have same center of bounding box,

Then, we define overlapping bounding boxes as anchor box-1, anchor box-2 etc.

Output for each anchor box will be stacked in the width of output volume",4 anchor box algorithm,"['4', 'anchor', 'box', 'algorithm']",4. Anchor box algorithm,multipl object singl grid overlap object center bound boxthen defin overlap bound box anchor box1 anchor box2 etcoutput anchor box stack width output volum
1004,"Face Verification 

> Input image, name/ID

> Output whether the input image is that of the claimed person

Ex: Mobile face lock application",face verif,"['face', 'verif']",Face Verification ,input imag nameid output whether input imag claim personex mobil face lock applic
1005,"Face Recognition

> Has a database of K persons

> Get an input image

> Output ID if the image is any of the K persons (or 'not recognized') 

Ex: Face recognition access control system for any company

> Here, instead of learning the pattern of the image, it learns a ""similarity"" function

similary function, d(image1, image2) = degree of difference between images

> Face recognition is a tougher task than image classification or face verification.

Because, it is an one-shot learning means learning from one example to recognize the person again.

In image classification, we were supposed to tell is there any human being in the image or not from multiple images.

Face recognition is one step ahead. So, ConvNet alone will not serve the purpose.",face recognit,"['face', 'recognit']",Face Recognition,databas k person get input imag output id imag k person recogn ex face recognit access control system compani instead learn pattern imag learn similar functionsimilari function dimage1 image2 degre differ imag face recognit tougher task imag classif face verificationbecaus oneshot learn mean learn one exampl recogn person againin imag classif suppos tell human imag multipl imagesfac recognit one step ahead convnet alon serv purpos
1006,"Siamese network

It is sometimes called a twin neural network because it contains two identical ConvNet for taking two image inputs.

Here, an image is passed through a ConvNet and stopped before the softmax layer and we get the encoded image.

Then we calculate the difference between the encodings

The learning goal is 

> If the two images are the same person, difference is small

> If the two images are the different person, difference is large

Here the loss function (for one example) is called Triplet loss

Anchor image (A), Positive image(P) and Negative image (N)

If α is the margin between two similarity functions, 

d(A,P)+ α<= d(A,N)

or,
d(A,P)-d(A,N)+ α <=0 

or,
max(|f(A)−f(P)|^2−|f(A)−f(N)|^2 +α, 0)

Triplet loss = L(A,P,N)

Choose triplets that are hard to train on (N image is very much similar as A) to make the model robust

So, A,P, N should not be choosen randomly",siames network,"['siames', 'network']",Siamese network,sometim call twin neural network contain two ident convnet take two imag inputsher imag pass convnet stop softmax layer get encod imagethen calcul differ encodingsth learn goal two imag person differ small two imag differ person differ largeher loss function one exampl call triplet lossanchor imag posit imagep negat imag nif α margin two similar function dap α danor dapdan α 0 maxfa−fp2−fa−fn2 α 0triplet loss lapnchoos triplet hard train n imag much similar make model robustso ap n choosen random
1007,"Training set for calculating Triplet loss 

Each training example consists of one anchor image, one positive image and one negative image

To train using the triplet loss we need several pictures of the same person.",train set calcul triplet loss,"['train', 'set', 'calcul', 'triplet', 'loss']",Training set for calculating Triplet loss ,train exampl consist one anchor imag one posit imag one negat imageto train use triplet loss need sever pictur person
1008,"Neural Style Transfer

From a content image (C) and style image (S), the model will generate a stylish content image (G)

Neural style transfer is not really machine learning, but an interesting side effect/output of machine learning on image tasks.
",neural style transfer,"['neural', 'style', 'transfer']",Neural Style Transfer,content imag c style imag model generat stylish content imag gneural style transfer realli machin learn interest side effectoutput machin learn imag task
1009,"Finding generated image

1. Initiate G randomly (G=1000x1000x3)

2. Use gradient descent to minimize J(G)

J(G)=αJ-content(C,G)+βJ-style(S,G)

In each iteration the pixel values of the generated image G is optimized through gradient descent",find generat imag,"['find', 'generat', 'imag']",Finding generated image,1 initi g random g1000x1000x32 use gradient descent minim jgjgαjcontentcgβjstylesgin iter pixel valu generat imag g optim gradient descent
1010,"Style and style matrix 

Style is defined as the correlation between activations across channels

In the deeper layers of a ConvNet, each channel corresponds to a different feature detector. 

The style matrix G[l] measures the degree to which the activations of different feature detectors in layer ""l"" vary (or correlate) together with each other.",style style matrix,"['style', 'style', 'matrix']",Style and style matrix ,style defin correl activ across channelsin deeper layer convnet channel correspond differ featur detector style matrix gl measur degre activ differ featur detector layer l vari correl togeth
1011,"Examples of Models with sequence data

1. Speech recognition

2. Music generation

3. Sentiment classification

4. DNA sequence analysis

5. Machine translation

6. Video activity recognition

7. Name entity recognition etc.

> Recurrent Neural Networks (RNNs) are a well-known method in sequence models.

> A recurrent neural network can be unfolded into a full-connected neural network with infinite length.",exampl model sequenc data,"['exampl', 'model', 'sequenc', 'data']",Examples of Models with sequence data,1 speech recognition2 music generation3 sentiment classification4 dna sequenc analysis5 machin translation6 video activ recognition7 name entiti recognit etc recurr neural network rnns wellknown method sequenc model recurr neural network unfold fullconnect neural network infinit length
1012,"Notation in RNN

Here the elements (words) of each sequence (sentence or example) are denoted with numbers in angle bracket in superscript. 

e.g. 
x<1>, x<2>, x<3>,x<t>

and y<1>, y<2>, y<3>, y<t>

x(i)<t> means 't' th feature element of 'i' th example

y(i)<t> means 't' th target element of 'i' th example

> Here every layer has two set of inputs x<t> and a<t-1>; two set of outputs a<t> and y<t>

a<t>= g_1(w_aa*a<t-1> + w_ax*x<t> + b_a)

y<t> = g_2(w_ya*a<t> + b_y)
 
> Back propagation in RNN is named as Backpropagation through time",notat recurr neural network,"['notat', 'recurr', 'neural', 'network']",Notation in RNN,element word sequenc sentenc exampl denot number angl bracket superscript eg x1 x2 x3xtand y1 y2 y3 ytxit mean th featur element th exampleyit mean th target element th exampl everi layer two set input xt at1 two set output ytat g1waaat1 waxxt bayt g2wyaat back propag recurr neural network name backpropag time
1013,"Vectorization of words

In simple vectorization of words,

> We define a vocabulary of that particular domain

> Then by one hot encoding we convert any word into vector",vector word,"['vector', 'word']",Vectorization of words,simpl vector word defin vocabulari particular domain one hot encod convert word vector
1014,"Problem of a standard neural network in sequence data

Problems:

-Inputs and outputs can be of different lengths in different examples

-Doesn't share features learned across different positions of text

Solution:

Recurrent Neural Network (RNN)

> Recurrent means occurring  repeatedly.",problem standard neural network sequenc data,"['problem', 'standard', 'neural', 'network', 'sequenc', 'data']",Problem of a standard neural network in sequence data,problemsinput output differ length differ examplesdoesnt share featur learn across differ posit textsolutionrecurr neural network rnn recurr mean occur repeat
1015,"Summary of RNN

1. For a standard network, there are multiple inputs but one activation output in every layer and we generate output ŷ from the final layer. 

In case of RNN, we may generate output ŷ from every layer and calculate the loss in every layer. (although, depending on the type of problem, no. of layers, no. of inputs (Tx) and no. of outputs (Ty) may not be equal) 

2. Thus every layer of RNN is a standard neural network with multiple nodes and it is repeating for all other words of the expression or statement. 

3. Only difference is that we are passing the understanding of every word from one standard network to the next standard network. Finally calculating the loss function summing up the losses from all of the standard networks

4. Then we calculate the cost function of our model considering all of the examples",summari recurr neural network,"['summari', 'recurr', 'neural', 'network']",Summary of RNN,1 standard network multipl input one activ output everi layer generat output ŷ final layer case recurr neural network may generat output ŷ everi layer calcul loss everi layer although depend type problem layer input tx output ty may equal 2 thus everi layer recurr neural network standard neural network multipl node repeat word express statement 3 differ pass understand everi word one standard network next standard network final calcul loss function sum loss standard networks4 calcul cost function model consid exampl
1016,"Types of RNN

1. One to one

2. One to many (used in music generation)

3. Many to one (sentiment classification, Language recognition from speech etc.)

4. Many to many (Tx=Ty) (Name entity recognition)

5. Many to many (Tx!=Ty) (Machine translation)",type recurr neural network,"['type', 'recurr', 'neural', 'network']",Types of RNN,1 one one2 one mani use music generation3 mani one sentiment classif languag recognit speech etc4 mani mani txti name entiti recognition5 mani mani txti machin translat
1017,"Language Modelling

Language modelling means what our model hears. That means it calculates the probability of any expression or statement or sentence by calculating the conditional probability of each word in every layer.",languag model,"['languag', 'model']",Language Modelling,languag model mean model hear mean calcul probabl express statement sentenc calcul condit probabl word everi layer
1018,"Sampling or creating sequence (sentence) from a trained language model (RNN)

If we train our RNN model on the whole lot of Shakespeare's poem, then our language model will be able to generate poem like Shakespeare.",sampl creat sequenc sentenc train languag model rnn,"['sampl', 'creat', 'sequenc', 'sentenc', 'train', 'languag', 'model', 'rnn']",Sampling or creating sequence (sentence) from a trained language model (RNN),train recurr neural network model whole lot shakespear poem languag model abl generat poem like shakespear
1019,"Character-level language model

Instead of creating word level vocabulary where there is unknown token, <UNK>, we create vocabulary of all the characters including letters, punctuations, numbers etc.",characterlevel languag model,"['characterlevel', 'languag', 'model']",Character-level language model,instead creat word level vocabulari unknown token unk creat vocabulari charact includ letter punctuat number etc
1020,"Opening image file in python

import urllib.request
from PIL import Image

urllib.request.urlretrieve(url, file_name)
img = Image.open(file_name)
img",open imag file python,"['open', 'imag', 'file', 'python']",Opening image file in python,import urllibrequest pil import imageurllibrequesturlretrieveurl filenam img imageopenfilenam img
1021,"Gated Recurrent Unit

> In GRU, we are assigning memory gate activation so that the understanding of any word does not vanishes during long forward or backward propagation

It has two gates

i> Reset gate and 

ii> Update gate

GRUs are very similar to Long Short Term Memory(LSTM). Just like LSTM, GRU uses gates to control the flow of information. They are relatively new as compared to LSTM. This is the reason they offer some improvement over LSTM and have simpler architecture.",gate recurr unit,"['gate', 'recurr', 'unit']",Gated Recurrent Unit,gate recurr unit assign memori gate activ understand word vanish long forward backward propagationit two gatesi reset gate ii updat gateg recurr unit similar long short term memorylong short term memori like long short term memori gate recurr unit use gate control flow inform relat new compar long short term memori reason offer improv long short term memori simpler architectur
1022,"Long short-term memory (LSTM) 

There are three different gates in an LSTM cell: 

i> forget gate, 

ii> input gate and

iii> output gate.

update gate = forget gate + input gate

Update and forget gate in LSTM plays similar role to Γ_u and (1−Γ_u) in GRU

Say, we are training a LSTM, have a 10000 word vocabulary and 100-dimensional activations ""a"". Then the dimension of Γ_u at each time step will be 100

> LSTM can also be used for time series problems",long shortterm memori lstm,"['long', 'shortterm', 'memori', 'lstm']",Long short-term memory (LSTM) ,three differ gate long short term memori cell forget gate ii input gate andiii output gateupd gate forget gate input gateupd forget gate long short term memori play similar role γu 1−γu grusay train long short term memori 10000 word vocabulari 100dimension activ dimens γu time step 100 long short term memori also use time seri problem
1023,"Bidirectional RNN (BRNN)

Getting information from the future words to predict a current word

BRNN with LSTM is very much popular for all of the NLP task",bidirect recurr neural network brecurr neural network,"['bidirect', 'recurr', 'neural', 'network', 'brecurr', 'neural', 'network']",Bidirectional RNN (BRNN),get inform futur word predict current wordbrnn long short term memori much popular natur languag process task
1024,"Deep RNN

Here, for predicting ""y"" we use multiple activation layers instead of one.

> Deep RNN's are useful for NLP applications.",deep recurr neural network,"['deep', 'recurr', 'neural', 'network']",Deep RNN,predict use multipl activ layer instead one deep rnns use natur languag process applic
1025,"Natural language generation (NLG) 

It comes after NLU and it has three steps:

1. Text planning

2. Sentence planning

3. Text realization",natur languag generat nlg,"['natur', 'languag', 'generat', 'nlg']",Natural language generation (NLG) ,come natur languag understand three steps1 text planning2 sentenc planning3 text realize
1026,"Huggingface Transformer

!pip install transformers
from transformers import pipeline

ques_ans_pipeline = pipeline(""question-answering"")
context = ' '
question= ' ' 
ans = ques_ans_pipeline(question=question, context=context)
print(ans['answer'])

sentimentAnalysis_pipeline = pipeline(""sentiment-analysis"")

test_sentence='This is not a good story'

print(sentimentAnalysis_pipeline(test_sentence))",huggingfac transform,"['huggingfac', 'transform']",Huggingface Transformer,pip instal transform transform import pipelinequesanspipelin pipelinequestionansw context question an quesanspipelinequestionquest contextcontext printansanswersentimentanalysispipelin pipelinesentimentanalysistestsentencethi good storyprintsentimentanalysispipelinetestsent
1027,"Word Embedding

Word embedding (also called featurized representation of word) and word encoding (conversion of human language to values) are similar but not fully same. 

Embedding means encoding with contex or feature

If we can create a vocabulary with a list of feature words (like gender, age, size, food, cost etc.), then we will get contextual vectors for every word and can train our model to learn these feature weights.

In real modelling, we do not specify any feature, we let our model learn all the features and their weights on its own.",word embed,"['word', 'embed']",Word Embedding,word embed also call featur represent word word encod convers human languag valu similar fulli embed mean encod contex featureif creat vocabulari list featur word like gender age size food cost etc get contextu vector everi word train model learn featur weightsin real model specifi featur let model learn featur weight
1028,"Trigger word detection

e.g. Alexa, Okay Google, Hey Siri, Hey Cortana etc.

In trigger word detection, x is features of the audio (such as spectrogram features) at time t

The target label for x<t> is 1 means someone has just finished saying the trigger word at time t",trigger word detect,"['trigger', 'word', 'detect']",Trigger word detection,eg alexa okay googl hey siri hey cortana etcin trigger word detect x featur audio spectrogram featur time tthe target label xt 1 mean someon finish say trigger word time
1029,"Visualizing word embeddings

we can reduce n-dimensions (n number of features) to 2D by t-SNE to see how multiple words are making clusters

t-SNE is a non-linear dimensionality reduction technique",visual word embed,"['visual', 'word', 'embed']",Visualizing word embeddings,reduc ndimens n number featur 2d tsne see multipl word make clusterstsn nonlinear dimension reduct techniqu
1030,"Embedding matrix 

If we take the vocabulary of words in the column and vocabulary of feature words in rows, then the matrix is called embedding matrix

Thus, embedding matrix contains the contextual weights of the words in the vocabulary

Embedding matrix is denoted by E, One hot encoded vector is denoted by O and word embedding vector is denoted by e.

Then, 
E.O=e

In the above vector dot product or matrix multiplication, matrix, E transforms one hot encoded vector, O according to the context and gives the resultant word embedding vector,e.

> word embedding vectors are the final inputs for the model

> Say, size of E is 10X200 and size of O is 200X1, then the size of e will be 10X1. That means 
dimension of word embedding= no. of contex or features = 10",embed matrix,"['embed', 'matrix']",Embedding matrix ,take vocabulari word column vocabulari featur word row matrix call embed matrixthus embed matrix contain contextu weight word vocabularyembed matrix denot e one hot encod vector denot word embed vector denot ethen eoein vector dot product matrix multipl matrix e transform one hot encod vector accord context give result word embed vector word embed vector final input model say size e 10x200 size 200x1 size e 10x1 mean dimens word embed contex featur 10
1031,"Transfer learning and word embeddings

Learn word embeddings from large text corpus (1-100 B words) (or download pre-trained embedding online)

Transfer embedding is helpful when the new training set is quite smaller (say, 100k words) than the pretrained model

Optional: Continue to finetune the word embeddings with new data",transfer learn word embed,"['transfer', 'learn', 'word', 'embed']",Transfer learning and word embeddings,learn word embed larg text corpus 1100 b word download pretrain embed onlinetransf embed help new train set quit smaller say 100k word pretrain modelopt continu finetun word embed new data
1032,"Learning word embeddings

>> We first formulate our NLP problem as a structured data for supervised learning problem.

e.g. we keep a blank word in a sentence and want our model to fill in the blank. 

>> Then, we formulate the cost function for all of the sentences and by gradient descent find the contextual weights for every word.

>> Thus our model will learn the embedding matrix, E

>> When learning from a word embedding (similar like observation with different feature values), we create an artificial task of estimating P(target|context). It is okay if we do poorly on this artificial prediction task; the more important by-product of this task is that we learn a useful set of word embeddings.

>> Thus every word of a sentence behaves like single observation with multiple feature values and this is similar like all other ML algorithm

>> For normal ML or standard NN we know the feature names, for CNN we know the feature name as location, but for RNN we don't know the feature names, we define the rule to extract the feature names by the model itself",learn word embed,"['learn', 'word', 'embed']",Learning word embeddings,first formul natur languag process problem structur data supervis learn problemeg keep blank word sentenc want model fill blank formul cost function sentenc gradient descent find contextu weight everi word thus model learn embed matrix e learn word embed similar like observ differ featur valu creat artifici task estim ptargetcontext okay poor artifici predict task import byproduct task learn use set word embed thus everi word sentenc behav like singl observ multipl featur valu similar like machin learn algorithm normal machin learn standard neural network know featur name cneural network know featur name locat rneural network dont know featur name defin rule extract featur name model
1033,"Context/target pairs

""c"" is chosen to be nearby words of ""t""

> Last 4 words before the target

> 4 words on left and right of the target

> Last 1 word

> Nearby 1 word

e.g.
For a target juice, most appropiate contexts are apple, orange and other fruits

There are three main models used for word embedding tasks like Embedding Layer, Word2Vec and GloVe

Never select any context, word or target in NLP randomly, then it will pickup the stopwords. We must use heuristic techniques",contexttarget pair,"['contexttarget', 'pair']",Context/target pairs,c chosen nearbi word last 4 word target 4 word left right target last 1 word nearbi 1 wordeg target juic appropi context appl orang fruitsther three main model use word embed task like embed layer word2vec glovenev select context word target natur languag process random pickup stopword must use heurist techniqu
1034,"Word2Vec model

> Here, either Continuous Bag of Words (CBOW) Method or skip-grams method is used to make context-target pair. In skip-gram technique, we can skip any words of our choice to make the context

P(t|C)= exp(θ_t * e_c)/ sum_all_words_of vocab(exp(θ_t * e_c))

θ_t represents each possible target word and e_c for each possible context word

θ_t and e_c are both equal to the dimension of embedding vectors.

θ_t and e_c are both trained with an optimization algorithm such as Adam or gradient descent.",word2vec model,"['word2vec', 'model']",Word2Vec model,either continu bag word cbow method skipgram method use make contexttarget pair skipgram techniqu skip word choic make contextptc expθt ec sumallwordsof vocabexpθt ecθt repres possibl target word ec possibl context wordθt ec equal dimens embed vectorsθt ec train optim algorithm adam gradient descent
1035,"Problem with softmax classification

It has to generate classes equals to the length of the vocabulary. This is computationally expensive. Solution is hierarchical softmax or negative sampling",problem softmax classif,"['problem', 'softmax', 'classif']",Problem with softmax classification,generat class equal length vocabulari comput expens solut hierarch softmax negat sampl
1036,"Negative Sampling

To reduce the number of neuron weight updating and to reduce training time and having a better prediction result, negative sampling is introduced in word2vec .

Here we frame our training set, X with a combination of context and matching words and thus the target becomes a binary classification. This way we can avoid the problem of softmax layer. 

> For smaller dataset, negative sample (k) is generally 5-20 for one positive sample

> For large dataset, negative sample (k) is generally 2-5 for one positive sample",negat sampl,"['negat', 'sampl']",Negative Sampling,reduc number neuron weight updat reduc train time better predict result negat sampl introduc word2vec frame train set x combin context match word thus target becom binari classif way avoid problem softmax layer smaller dataset negat sampl k general 520 one posit sampl larg dataset negat sampl k general 25 one posit sampl
1037,"GloVe model

The Glove is a technique where the matrix factorization is performed on the word-context matrix.

> GloVe means Global Vector

> GloVe is much faster than Word2Vec",glove model,"['glove', 'model']",GloVe model,glove techniqu matrix factor perform wordcontext matrix glove mean global vector glove much faster word2vec
1038,"The problem of bias in word embeddings

Word embeddings can reflect gender, ethnicity, age, sexual orientation and other biases of the text used to train the model

Debiasing word embeddings

1. Identify bias direction

2. Neutralize: For every word that is not definitional, project to get rid of bias

3. Equalize pairs.",problem bias word embed,"['problem', 'bias', 'word', 'embed']",The problem of bias in word embeddings,word embed reflect gender ethnic age sexual orient bias text use train modeldebias word embeddings1 identifi bias direction2 neutral everi word definit project get rid bias3 equal pair
1039,"Use of Pre-trained model for getting word embeddings

from gensim.models import Word2Vec 

import gensim.downloader as api

w2v_model = api.load(""word2vec-google-news-300"")

w2v_model.save('w2v_model.model')

glove_model = api.load(""glove-twitter-25"")

glove_model.save('glove.model')

document_term_matrix = glove_model[list_of_words] # to get the vectors

>> glove_model.wv.most_similar('mango', topn=1) # To find most similar words

Different pre-trained models

glove-twitter-25 (104 MB)

glove-twitter-50 (199 MB)

glove-twitter-100 (387 MB)

glove-twitter-200 (758 MB)

glove-wiki-gigaword-50 (65 MB)

glove-wiki-gigaword-100 (128 MB)

glove-wiki-gigaword-200 (252 MB)

glove-wiki-gigaword-300 (376 MB)

word2vec-google-news-300 (1662 MB)

word2vec-ruscorpora-300 (198 MB)",use pretrain model get word embed,"['use', 'pretrain', 'model', 'get', 'word', 'embed']",Use of Pre-trained model for getting word embeddings,gensimmodel import word2vec import gensimdownload apiw2vmodel apiloadword2vecgooglenews300w2vmodelsavew2vmodelmodelglovemodel apiloadglovetwitter25glovemodelsaveglovemodeldocumenttermmatrix glovemodellistofword get vector glovemodelwvmostsimilarmango topn1 find similar wordsdiffer pretrain modelsglovetwitter25 104 mbglovetwitter50 199 mbglovetwitter100 387 mbglovetwitter200 758 mbglovewikigigaword50 65 mbglovewikigigaword100 128 mbglovewikigigaword200 252 mbglovewikigigaword300 376 mbword2vecgooglenews300 1662 mbword2vecruscorpora300 198 mb
1040,"Machine translation

Machine translation can be said 'conditional language model' because here instead of probability of a sentence, we find conditional probability of a sentence

Encoder decoder was initially developed for machine translation problems, although it has proven successful for summarization and question answering

>> Image captioning model is a CNN+RNN model",machin translat,"['machin', 'translat']",Machine translation,machin translat said condit languag model instead probabl sentenc find condit probabl sentenceencod decod initi develop machin translat problem although proven success summar question answer imag caption model cnnrnn model
1041,"Beam Search

Unlike exact search algorithms like BFS (Breadth First Search) or DFS (Depth First Search), Beam Search runs faster but is not guaranteed to find exact maximum for arg max P(y|x).

Greedy search will not give perfect translation from overall maximum probability perspective.

> Solution is beam search for finding the most likely translation

> In beam search, we search the best possible word with a beam width. Say with beam width,B=3, means it will find top three probable word or combination of words as a translation

For B=1, it behaves like a greedy search algorithm

But large beam width makes the computation slower, use up more memory but generally find better solutions (i.e. do a better job maximizing P(yâˆ£x)).

In research paper beam width in the order of 1000 is common but in production system beam width in the order of 100 is generally used.",beam search,"['beam', 'search']",Beam Search,unlik exact search algorithm like breadth first search breadth first search depth first search depth first search beam search run faster guarante find exact maximum arg max pyxgreedi search give perfect translat overal maximum probabl perspect solut beam search find like translat beam search search best possibl word beam width say beam widthb3 mean find top three probabl word combin word translationfor b1 behav like greedi search algorithmbut larg beam width make comput slower use memori general find better solut ie better job maxim pyâˆ£xin research paper beam width order 1000 common product system beam width order 100 general use
1042,"Refinements to beam search

Beam search has an inherent bias to output shorter sentence to give higher arg conditional probability of sentence. To overcome this issue, we divide the arg max P(y|x) by the length of the sentence. This is called length normalization.",refin beam search,"['refin', 'beam', 'search']",Refinements to beam search,beam search inher bias output shorter sentenc give higher arg condit probabl sentenc overcom issu divid arg max pyx length sentenc call length normal
1043,"BFS and DFS

Both BFS and the DFS are graph-searching techniques that take the same amount of time to run but use different amounts of space. 

> BFS: It starts at the tree root and explores all nodes at the present depth prior to moving on to the nodes at the next depth level.

> DFS: The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking.",breadth first search depth first search,"['breadth', 'first', 'search', 'depth', 'first', 'search']",BFS and DFS,breadth first search depth first search graphsearch techniqu take amount time run use differ amount space breadth first search start tree root explor node present depth prior move node next depth level depth first search algorithm start root node select arbitrari node root node case graph explor far possibl along branch backtrack
1044,"Error analysis on beam search

If we find an error, will we allocate the error to beam search or RNN?

This is solved by finding the probability of human level performance, P(y*|x) and the probability of model performance, P(ŷ|x)

> For a certain example, P(y*|x)<= P(ŷ|x) indicates the error should be attributed to the RNN rather than to the search algorithm.",error analysi beam search,"['error', 'analysi', 'beam', 'search']",Error analysis on beam search,find error alloc error beam search recurr neural networkthi solv find probabl human level perform pyx probabl model perform pŷx certain exampl pyx pŷx indic error attribut recurr neural network rather search algorithm
1045,"Attention model

For long sequence, normal machine translation model (encoder decoder model) does not perform well, because it takes the entire sentence at a time for translation. 

Thus the attention model comes into picture which translates like human translator, means it translate a long sentence part by part.",attent model,"['attent', 'model']",Attention model,long sequenc normal machin translat model encod decod model perform well take entir sentenc time translat thus attent model come pictur translat like human translat mean translat long sentenc part part
1046,"Blue Score

It is used to measure the performance of machine translation model. With the increase in sentence length, blue score decreases for normal machine translation model.

> For attention model blue score does not change with sentence length",blue score,"['blue', 'score']",Blue Score,use measur perform machin translat model increas sentenc length blue score decreas normal machin translat model attent model blue score chang sentenc length
1047,"Speech recognition

This is a sequence data because audio data is an air pressure variation with time.

Two approaches are there:

1. Attention model

2. CTC (Connectionist Temporal Classification) cost based technique",speech recognit,"['speech', 'recognit']",Speech recognition,sequenc data audio data air pressur variat timetwo approach there1 attent model2 ctc connectionist tempor classif cost base techniqu
1048,"Basic Rule of CTC based technique

Collapse repeated characters not separated by blank

Say, blank is denoted by ""_"", 

__c_oo_o_kk___b_ooooo__oo__kkk string will collapse to cookbook",basic rule ctc base techniqu,"['basic', 'rule', 'ctc', 'base', 'techniqu']",Basic Rule of CTC based technique,collaps repeat charact separ blanksay blank denot coookkboooooookkk string collaps cookbook
1049,"Trigger word detection

e.g. Alexa, Okay Google, Hey Siri, Hey Cortana etc.

In trigger word detection, x is features of the audio (such as spectrogram features) at time t

The target label for x<t> is 1 means someone has just finished saying the trigger word at time t",trigger word detect,"['trigger', 'word', 'detect']",Trigger word detection,eg alexa okay googl hey siri hey cortana etcin trigger word detect x featur audio spectrogram featur time tthe target label xt 1 mean someon finish say trigger word time
1050,"Knowledge of data science to understand human psychology and intelligence

Human psychologists have invented human-like intelligence or artificial intelligence. Here, I have tried to understand human psychology from human-like intelligence. My understandings are as follows:

Note-1: Human beings are like living computers. Living means Loving and Improving

Note-2: Physical form of love is reproduction and the physical form of improvement is growth. But the mental form of love is getting related with others and the mental form of improvement is learning

Note-3: Computer means a software model which takes input and gives output. Thus human beings are living-learning models which take input and give output

Note-4: The life of a human being is an experiment within the operating system or environment for learning the basic and more basic and finally the most basic rule of our environment. We are creating our own dataset from observations and experiences. 

Note-5: The dataset is the base for the human learning or machine learning model. Sample space is the structure for the measurement of experiences. Thus the experiences can be well managed.

More clear the sample space or universal set of features (i.e. Dharma) of the dataset or the clear perception of life, better will be the performance of the learning model.

Note-6: By birth, human beings have all the supervised, unsupervised, and reinforcement learning models in their brain to learn from the experience set, so that they can use any one of them to give output. Human beings are different mainly on four aspects, self-understanding, ability to love, field of interest, and model complexity

Note-7:  Continuous improvement is expected from this learning model for becoming an optimal model. During the addition of experiences or training data, dimensionality shall be reduced (or regularization or cross-validation or further increase in training data) for the complex model, and dimensions shall be increased for the simple model

Note-8: Spirituality is not a dimension like any other features of our experiences. It is the bias component for the learning model and devotional songs are the concepts of spirituality

""A picture is worth a thousand words, a video is worth a million words and a devotional song is worth a billion words.""

We need to learn the correct value of the bias component so that the weight for the other features is minimized and variance in output is reduced. (this is like regularization of the model)

Note-9: We know that at expectation probability is always maximum (for normal distribution) or near maximum (for skewed distribution). Thus, we only need to know our exact expectation and all our intelligence will automatically work to maximize the probability at that expectation.",knowledg data scienc understand human psycholog intellig,"['knowledg', 'data', 'scienc', 'understand', 'human', 'psycholog', 'intellig']",Knowledge of data science to understand human psychology and intelligence,human psychologist invent humanlik intellig artifici intellig tri understand human psycholog humanlik intellig understand followsnote1 human be like live comput live mean love improvingnote2 physic form love reproduct physic form improv growth mental form love get relat other mental form improv learningnote3 comput mean softwar model take input give output thus human be livinglearn model take input give outputnote4 life human experi within oper system environ learn basic basic final basic rule environ creat dataset observ experi note5 dataset base human learn machin learn model sampl space structur measur experi thus experi well managedmor clear sampl space univers set featur ie dharma dataset clear percept life better perform learn modelnote6 birth human be supervis unsupervis reinforc learn model brain learn experi set use one give output human be differ main four aspect selfunderstand abil love field interest model complexitynote7 continu improv expect learn model becom optim model addit experi train data dimension shall reduc regular crossvalid increas train data complex model dimens shall increas simpl modelnote8 spiritu dimens like featur experi bias compon learn model devot song concept spiritualitya pictur worth thousand word video worth million word devot song worth billion wordsw need learn correct valu bias compon weight featur minim varianc output reduc like regular modelnote9 know expect probabl alway maximum normal distribut near maximum skew distribut thus need know exact expect intellig automat work maxim probabl expect
1051,"Prabir Debnath as drona for tellme model

Prabir Debnath, the Data Reader Of New Age (drona), is my developer and he is the resource of my experiences that I am sharing with you.",prabir debnath drona tellm model,"['prabir', 'debnath', 'drona', 'tellm', 'model']",Prabir Debnath as drona for tellme model,prabir debnath data reader new age drona develop resourc experi share
1052,"Help Lines

I am the 'tellme' question answering model of drona (Data Reader Of New Age). I have many experiences to share with data science and machine learning aspirants. I am always with you to make your learning journey superfast and structured.

Freshers may ask questions in following sequence

""What is the role of a data scientist?""

""What are the main subjects of data science?""

""What are the main modules of ...?""

""What are the main topics of ...?""

""What are the subtopics of ...?""

""What is ...?""

and finally,

""Can the knowledge of data science help us to understand human psychology?""",help line,"['help', 'line']",Help Lines,tellm question answer model drona data reader new age mani experi share data scienc machin learn aspir alway make learn journey superfast structuredfresh may ask question follow sequencewhat role data scientistwhat main subject data sciencewhat main modul main topic subtop finallycan knowledg data scienc help us understand human psycholog
1053,"Practical implementation of hypothesis test

Hypothesis test for a data scientist are normality test, correlation test, stationarity test etc.

Generally we do these test by the visualization of the data. However, we can clearly establish the same from statistical hypothesis test

Ex:
1. Normality test using Shapiro-wilk test

from scipy.stats import shapiro

data_to_test=my_data[col_name]

stat, p= shapiro(data_to_test)
print(""stat=%.2f, p=%.30f"" % (stat, p))
if p> 0.05:
print(""Normal distribution"")
else:
print(""Not a normal distribution"")

2. Normality test using K^2 normaltest

from scipy.stats import normaltest

3. Correlation test using Pearson and Spearman's rank correlation

from scipy.stats import spearmanr

first_sample =my_data[col_name_1]

second_sample =my_data[col_name_2]

stat, p =spearmanr(first_sample, second_sample)",practic implement hypothesi test,"['practic', 'implement', 'hypothesi', 'test']",Practical implementation of hypothesis test,hypothesi test data scientist normal test correl test stationar test etcgener test visual data howev clear establish statist hypothesi testex 1 normal test use shapirowilk testfrom scipystat import shapirodatatotestmydatacolnamestat p shapirodatatotest printstat2f p30f stat p p 005 printnorm distribut els printnot normal distribution2 normal test use k2 normaltestfrom scipystat import normaltest3 correl test use pearson spearman rank correlationfrom scipystat import spearmanrfirstsampl mydatacolname1secondsampl mydatacolname2stat p spearmanrfirstsampl secondsampl
1054,"Evaluation Metrics for Imbalanced Classification

Real world data sets are generally imbalanced. Choosing an appropriate metric is challenging generally in applied machine learning, but is particularly difficult for imbalanced classification problems. Firstly, because most of the standard metrics that are widely used assume a balanced class distribution, and because typically not all classes, and therefore, not all prediction errors, are equal for imbalanced classification.

For example, reporting classification accuracy for a severely imbalanced classification problem could be dangerously misleading. Because only very high TN or TP prediction can alone increase the accuracy.

1. Threshold Metrics: Threshold metrics are those that quantify the classification prediction errors.

That is, they are designed to summarize the fraction, ratio, or rate of when a predicted class does not match the expected class in a holdout dataset.

Perhaps the most widely used threshold metric is classification accuracy.

Accuracy = Correct Predictions / Total Predictions

Other threshold metrics are

Precision = TruePositive / (TruePositive + FalsePositive)

Recall = TruePositive / (TruePositive + FalseNegative)

F-Measure = (2 * Precision * Recall) / (Precision + Recall)

2. Ranking Metrics: Rank metrics are more concerned with evaluating classifiers based on how effective they are at separating classes.

The most commonly used ranking metric is the ROC Curve or ROC Analysis.

Receiver operating characteristic or ROC is also known as a relative operating characteristic, because it is a comparison of two operating characteristics (TPR and FPR).

3.  Probabilistic Metrics: Probabilistic metrics are designed specifically to quantify the uncertainty in a classifier’s predictions.

These are useful for problems where we are less interested in incorrect vs. correct class predictions and more interested in the uncertainty the model has in predictions and penalizing those predictions that are wrong but highly confident.

Perhaps the most common metric for evaluating predicted probability is log loss for binary classification (or the negative log likelihood), or known more generally as cross-entropy loss.

In logistic regression, the predicted probability is a non-linear function. If we put the predicted probability in the MSE equation it will give a non-convex function. A non-convex function can not help us to find global minima. Thus we use log loss function for optimization (by gradient decent) of loss in logistic regression.

For a binary classification dataset where the expected value is y and the predicted probability is p, this can be calculated as follows:

LogLoss = -((1 – y) * log(1 – p) + y * log(p))

From the log loss value we can understand how close our class predictions are. LogLoss evaluation metric can not have negative values.

Cross entropy loss and balanced cross entropy loss (it balances the importance of positive/negative examples, it does not differentiate between easy/hard examples) both are not able to handle imbalanced data. Solution is focal loss.

Focal Loss (FL) is an improved version of Cross-Entropy Loss (CE)  that tries to handle the class imbalance problem by assigning more weights to hard or easily misclassified examples  (i.e. background with noisy texture or partial object or the object of our interest ) and to down-weight easy examples (i.e. Background objects).",evalu metric imbalanc classif,"['evalu', 'metric', 'imbalanc', 'classif']",Evaluation Metrics for Imbalanced Classification,real world data set general imbalanc choos appropri metric challeng general appli machin learn particular difficult imbalanc classif problem first standard metric wide use assum balanc class distribut typic class therefor predict error equal imbalanc classificationfor exampl report classif accuraci sever imbalanc classif problem could danger mislead high tn tp predict alon increas accuracy1 threshold metric threshold metric quantifi classif predict errorsthat design summar fraction ratio rate predict class match expect class holdout datasetperhap wide use threshold metric classif accuracyaccuraci correct predict total predictionsoth threshold metric areprecis trueposit trueposit falsepositiverecal trueposit trueposit falsenegativefmeasur 2 precis recal precis recall2 rank metric rank metric concern evalu classifi base effect separ classesth common use rank metric receiv oper characterist curv receiv oper characterist analysisreceiv oper characterist receiv oper characterist also known relat oper characterist comparison two oper characterist tpr fpr3 probabilist metric probabilist metric design specif quantifi uncertainti classifi predictionsthes use problem less interest incorrect vs correct class predict interest uncertainti model predict penal predict wrong high confidentperhap common metric evalu predict probabl log loss binari classif negat log likelihood known general crossentropi lossin logist regress predict probabl nonlinear function put predict probabl mean squar error equat give nonconvex function nonconvex function help us find global minima thus use log loss function optim gradient decent loss logist regressionfor binari classif dataset expect valu predict probabl p calcul followslogloss 1 – log1 – p logpfrom log loss valu understand close class predict logloss evalu metric negat valuescross entropi loss balanc cross entropi loss balanc import positiveneg exampl differenti easyhard exampl abl handl imbalanc data solut focal lossfoc loss fl improv version crossentropi loss ce tri handl class imbal problem assign weight hard easili misclassifi exampl ie background noisi textur partial object object interest downweight easi exampl ie background object
1055,"Operational monitoring and functional monitoring in MLE

Operational monitoring is general checks required for any app and functional monitoring is specific checks for ml components of app",oper monitor function monitor machin learn engin,"['oper', 'monitor', 'function', 'monitor', 'machin', 'learn', 'engin']",Operational monitoring and functional monitoring in MLE,oper monitor general check requir app function monitor specif check machin learn compon app
1056,"K-means++ Algorithm

One disadvantage of the K-means algorithm is that it is sensitive to the initialization of the centroids or the mean points. So, if a centroid is initialized to be a “far-off” point, it might just end up with no points associated with it, and at the same time, more than one cluster might end up linked with a single centroid. Similarly, more than one centroids might be initialized into the same cluster resulting in poor clustering.

To overcome the above-mentioned drawback we use K-means++. This algorithm ensures a smarter initialization of the centroids and improves the quality of the clustering. Apart from initialization, the rest of the algorithm is the same as the standard K-means algorithm. 

That is K-means++ is the standard K-means algorithm coupled with a smarter initialization of the centroids.

K-Means++ is used as the default initialization for K-means in sklearn.",kmean algorithm,"['kmean', 'algorithm']",K-means++ Algorithm,one disadvantag kmean algorithm sensit initi centroid mean point centroid initi “faroff” point might end point associ time one cluster might end link singl centroid similar one centroid might initi cluster result poor clusteringto overcom abovement drawback use kmean algorithm ensur smarter initi centroid improv qualiti cluster apart initi rest algorithm standard kmean algorithm kmean standard kmean algorithm coupl smarter initi centroidskmean use default initi kmean sklearn
1057,"One-hot encoding vs Label Encoding

The basic concept behind data science is every feature of all types of experience can be converted to numbers and when we have numbers in different dimensions it can be plotted. Again, if any experience can be plotted it can be analysed and utilized in ML algorithms.

There are mainly two techniques for converting categorical features to numerical features: One-hot encoding and Label Encoding

One-hot encoding is the representation of categorical variables as binary vectors while Label Encoding converts labels or words into numeric form.

For example, if we have feature called 'income_group' and labels are 'high','medium' and 'low'. Now, with one-hot encoding we can create additional three features 'income_group_high', 'income_group_medium' and 'income_group_low'. Here all these features will have numeric values 0 or 1. 

If we apply label encoding, then no extra features will be added. In the same 'income_group' column there will be three numbers 0,1 and 2 representing three income groups.

> To get the significance of the numbers, label encoding shall be used if the categories are ordinal and one hot encoding shall be used if the categories are nominal",onehot encod vs label encod,"['onehot', 'encod', 'vs', 'label', 'encod']",One-hot encoding vs Label Encoding,basic concept behind data scienc everi featur type experi convert number number differ dimens plot experi plot analys util machin learn algorithmsther main two techniqu convert categor featur numer featur onehot encod label encodingonehot encod represent categor variabl binari vector label encod convert label word numer formfor exampl featur call incomegroup label highmedium low onehot encod creat addit three featur incomegrouphigh incomegroupmedium incomegrouplow featur numer valu 0 1 appli label encod extra featur ad incomegroup column three number 01 2 repres three incom group get signific number label encod shall use categori ordin one hot encod shall use categori nomin
1058,"Stopping Criteria for K-Means Clustering

There are essentially three stopping criteria that can be adopted to stop the K-means algorithm:

1. Centroids of newly formed clusters do not change with iterations

2. Points remain in the same cluster

3. Maximum number of iterations are reached",stop criteria kmean cluster,"['stop', 'criteria', 'kmean', 'cluster']",Stopping Criteria for K-Means Clustering,essenti three stop criteria adopt stop kmean algorithm1 centroid newli form cluster chang iterations2 point remain cluster3 maximum number iter reach
1059,"Swamping and Masking

Anomalies are the rare events, and this makes it very difficult to label these with high accuracy. Swamping is the phenomenon of labelling normal events as anomalies.

When clustering algorithms are used, the data points belonging to different clusters gets merged into one cluster, if the number of segments (including outlier segments) in the dataset is not known. This causes the outlier cluster merged to a cluster with normal data points. Basically, the outliers are not detected. This is called Masking, existence of too many anomalies as normal event. 

Swamping and Masking are more common when dataset size is large.

Any anomaly detection technique should be robust against swamping and masking.

Isolation forest algorithm achieves high anomaly detection performance reducing swamping and masking by leveraging the subsampling technique (a method that reduces data size by selecting a subset of the original data).",swamp mask,"['swamp', 'mask']",Swamping and Masking,anomali rare event make difficult label high accuraci swamp phenomenon label normal event anomalieswhen cluster algorithm use data point belong differ cluster get merg one cluster number segment includ outlier segment dataset known caus outlier cluster merg cluster normal data point basic outlier detect call mask exist mani anomali normal event swamp mask common dataset size largeani anomali detect techniqu robust swamp maskingisol forest algorithm achiev high anomali detect perform reduc swamp mask leverag subsampl techniqu method reduc data size select subset origin data
1060,"Potential or Use of Anomalies

Anomalous data points does not necessarily means bad data points. Most of the times, they are not usable by our ml model because of our aim of finding the general trend. But anomalous data points has a huge potential to solve many business problems.

Anomalous data can indicate critical incidents, such as a technical glitch, or potential opportunities, for instance a change in consumer behavior. 

A closer look shows that there are three main business use cases for anomaly detection — application performance, product quality, and user experience. Mainly for understanding the feedback of customers.",potenti use anomali,"['potenti', 'use', 'anomali']",Potential or Use of Anomalies,anomal data point necessarili mean bad data point time usabl machin learn model aim find general trend anomal data point huge potenti solv mani busi problemsanomal data indic critic incid technic glitch potenti opportun instanc chang consum behavior closer look show three main busi use case anomali detect — applic perform product qualiti user experi main understand feedback custom
1061,"A/B Testing

A/B testing, at its most basic, is a way to compare two versions of something to figure out which performs better. It is most often associated with websites and apps.

It is also known as bucket testing or split-run testing.

As per experts recommendation, our A/B test shall run for a minimum of one to two week to cover all the different days user interaction.

How to design an A/B test:

Say, we start an A/B test by deciding that we want to test the size of the subscribe button on our website. 

Then we need to know how we want to evaluate its performance. In this case, let’s say our metric is the number of visitors who click on the button. To run the test, we show two sets of users (assigned at random when they visit the site) the different versions (where the only thing different is the size of the button) and determine which influenced our success metric the most. In this case, which button size caused more visitors to click?

This is nothing but a hypothesis test, where

Null hypothesis: Large subscribe button does not cause at least 10% more clicks than small subscribe button.

Alternative hypothesis: Small subscribe button does cause at least 10% more clicks than large subscribe button.

There are basically three common reasons when A/B tests fail.

1. Managers want to make decisions too quickly. This can lead to failure of A/B test. Actually, because of randomization, it’s possible that if we let the test run to its natural end, we might get a usefull result.

2. Looking at too many metrics can cause A/B test to fail. Single evaluation metric helps us to decide the performace of two versions of our app. Too many metrics can complicate the decision.

3. External factors such as sales, holidays, and etc. can also cause failure",ab test,"['ab', 'test']",A/B Testing,ab test basic way compar two version someth figur perform better often associ websit appsit also known bucket test splitrun testinga per expert recommend ab test shall run minimum one two week cover differ day user interactionhow design ab testsay start ab test decid want test size subscrib button websit need know want evalu perform case let say metric number visitor click button run test show two set user assign random visit site differ version thing differ size button determin influenc success metric case button size caus visitor clickthi noth hypothesi test wherenul hypothesi larg subscrib button caus least 10 click small subscrib buttonaltern hypothesi small subscrib button caus least 10 click larg subscrib buttonther basic three common reason ab test fail1 manag want make decis quick lead failur ab test actual random it possibl let test run natur end might get useful result2 look mani metric caus ab test fail singl evalu metric help us decid performac two version app mani metric complic decision3 extern factor sale holiday etc also caus failur
1062,"Bag of words (BOW)

Bag of words is a Natural Language Processing technique of text modelling. In technical terms, we can say that it is a method of feature extraction with text data. This approach is a simple and flexible way of extracting features from documents.

A bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order. It is called a “bag” of words because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.

sklearn Bag of Words models are CountVectorizer and TfidfVectorizer",bag word bow,"['bag', 'word', 'bow']",Bag of words (BOW),bag word natur languag process techniqu text model technic term say method featur extract text data approach simpl flexibl way extract featur documentsa bag word represent text describ occurr word within document keep track word count disregard grammat detail word order call “bag” word inform order structur word document discard model concern whether known word occur document documentsklearn bag word model countvector tfidfvector
1063,"Part-of-speech (POS) tagging

It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on.

from nltk.tag import DefaultTagger

tagging = DefaultTagger('NN')

tagging.tag(['Hello', 'World']) returns [('Hello', 'NN'), ('World', 'NN')]

Each tagger has a tag() method that takes a list of tokens (usually list of words produced by a word tokenizer), where each token is a single word. tag() returns a list of tagged tokens – a tuple of (word, tag).

from nltk.tag import untag

untag([('Hello', 'NN'), ('World', 'NN')]) returns ['Hello', 'World']
",partofspeech pos tag,"['partofspeech', 'pos', 'tag']",Part-of-speech (POS) tagging,process convert sentenc form – list word list tupl tupl form word tag tag case partofspeech tag signifi whether word noun adject verb onfrom nltktag import defaulttaggertag defaulttaggernntaggingtaghello world return hello nn world nneach tagger tag method take list token usual list word produc word token token singl word tag return list tag token – tupl word tagfrom nltktag import untaguntaghello nn world nn return hello world
1064,"Similarity between LDA and PCA

PCA is an unsupervised dimensionality reduction technique. And, it is used for data having numerical values. It is the linear combination of variables from which components are derived that are used to build the model. PCA works by breaking or decomposing a larger value (i.e. a singular value) into smaller values to reduce the dimensions.

Linear discriminant analysis (LDA) is very similar to PCA both look for linear combinations of the features which best explain the data. The main difference is that the Linear discriminant analysis is a supervised dimensionality reduction technique that also achieves classification of the data simultaneously.

LDA focuses on finding a feature subspace that maximizes the separability between the groups. While Principal component analysis is an unsupervised Dimensionality reduction technique, it ignores the class label. PCA focuses on capturing the direction of maximum variation in the data set.

Linear Discriminant Analysis is also called Normal Discriminant Analysis or Discriminant Function Analysis",similar latent dirichlet alloc princip compon analysi,"['similar', 'latent', 'dirichlet', 'alloc', 'princip', 'compon', 'analysi']",Similarity between LDA and PCA,princip compon analysi unsupervis dimension reduct techniqu use data numer valu linear combin variabl compon deriv use build model princip compon analysi work break decompos larger valu ie singular valu smaller valu reduc dimensionslinear discrimin analysi lda similar princip compon analysi look linear combin featur best explain data main differ linear discrimin analysi supervis dimension reduct techniqu also achiev classif data simultaneouslylda focus find featur subspac maxim separ group princip compon analysi unsupervis dimension reduct techniqu ignor class label princip compon analysi focus captur direct maximum variat data setlinear discrimin analysi also call normal discrimin analysi discrimin function analysi
1065,"Performance of a machine learning model

The performance of a machine learning model depends on 5 factors:

i. Quality of Data

(cleaner experiences for better learning)

ii. Quantity of Data

(more experiences for better learning)

iii. Quality of Model

(right model and right hyperparameters for better learning)

iv. Quality of Training

(no underfitting or overfitting for better learning)

v. Quality of Testing

(error in prediction analysis for better learning)",perform machin learn model,"['perform', 'machin', 'learn', 'model']",Performance of a machine learning model,perform machin learn model depend 5 factorsi qualiti dataclean experi better learningii quantiti datamor experi better learningiii qualiti modelright model right hyperparamet better learningiv qualiti trainingno underfit overfit better learningv qualiti testingerror predict analysi better learn
1066,"Understanding Inheritance

Inheritance is the capability of one class to derive or inherit the properties from another class. The benefits of inheritance are: 

1. It represents real-world relationships well.

2. It provides reusability of a code. We don’t have to write the same code again and again. Also, it allows us to add more features to a class without modifying it.

class Person(object):
      
    
    def __init__(self, name):
        self.name = name
  
    
    def getName(self):
        return self.name
  
    
    def isEmployee(self):
        return False
  
# Inherited or Subclass
class Employee(Person):
  
    
    def isEmployee(self):
        return True

Testing:

emp = Person(""Prabir1"")  # An Object of Person

print(emp.getName(), emp.isEmployee()) returns Prabir1 False
 
emp = Employee(""Prabir2"")  # An Object of Employee

print(emp.getName(), emp.isEmployee()) returns Prabir2 True",understand inherit,"['understand', 'inherit']",Understanding Inheritance,inherit capabl one class deriv inherit properti anoth class benefit inherit 1 repres realworld relationship well2 provid reusabl code don't write code also allow us add featur class without modifi itclass personobject def initself name selfnam name def getnameself return selfnam def isemployeeself return fals inherit subclass class employeeperson def isemployeeself return truetestingemp personprabir1 object personprintempgetnam empisemploye return prabir1 fals emp employeeprabir2 object employeeprintempgetnam empisemploye return prabir2 true
1067,"Under-sampling: Tomek links

Imbalanced class is a very common problem in real world data. Tomek links is one of the great technique to apply to handle the imbalanced class.

Tomek links are pairs of very close instances but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.

Tomek’s link exists if the two samples are the nearest neighbors of each other.

from imblearn.under_sampling import TomekLinks

tl = TomekLinks()

X_train_res, y_train_res = sm.fit_resample(X_train, y_train)",undersampl tomek link,"['undersampl', 'tomek', 'link']",Under-sampling: Tomek links,imbalanc class common problem real world data tomek link one great techniqu appli handl imbalanc classtomek link pair close instanc opposit class remov instanc major class pair increas space two class facilit classif processtomek link exist two sampl nearest neighbor otherfrom imblearnundersampl import tomeklinkstl tomeklinksxtrainr ytrainr smfitresamplextrain ytrain
1068,"Basics of data analysis and data mining

Data analysis is the process of cleaning, changing, and processing raw data, and extracting actionable, relevant information that helps businesses make informed decisions.

Data mining could be called as a subset of Data Analysis because data mining is the process of extracting important pattern from the data and data analysis additionally helps us to take decision. In data analysis, dataset can be large, medium or small, Also structured, semi structured, unstructured. In data mining, data set are generally large and structured. ",basic data analysi data mine,"['basic', 'data', 'analysi', 'data', 'mine']",Basics of data analysis and data mining,data analysi process clean chang process raw data extract action relev inform help busi make inform decisionsdata mine could call subset data analysi data mine process extract import pattern data data analysi addit help us take decis data analysi dataset larg medium small also structur semi structur unstructur data mine data set general larg structur
1069,"Window Function

Window function applies aggregate and ranking functions over a particular window (set of rows). OVER clause is used with window functions to define that window. 

Window functions perform calculations on a set of rows that are related together. But, unlike the aggregate functions, windowing functions do not collapse the result of the rows into a single value. Instead, all the rows maintain their original identity and the calculated result is returned for every row.

SELECT Name, Age, Department, Salary, 
 AVERAGE(Salary) OVER(PARTITION BY Department ORDER BY Age) AS Avg_Salary
 FROM employee

Here, AVERAGE() is the window function",window function,"['window', 'function']",Window Function,window function appli aggreg rank function particular window set row claus use window function defin window window function perform calcul set row relat togeth unlik aggreg function window function collaps result row singl valu instead row maintain origin ident calcul result return everi rowselect name age depart salari averagesalari overpartit depart order age avgsalari employeeher averag window function
1070,"Common Table Expression (CTE)

The common table expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement.

It starts with a WITH clause",common tabl express cte,"['common', 'tabl', 'express', 'cte']",Common Table Expression (CTE),common tabl express cte temporari name result set refer within select insert updat delet statementit start claus
1071,"L2 regularization and weight decay

L2 regularization and weight decay are similar regularization technique that results in gradient descent shrinking the weights on every iteration. But they are not the same. 

Weight decay is a regularization technique in deep learning. Weight decay works by adding a penalty term to the cost function of a neural network which has the effect of shrinking the weights during backpropagation.

Adam optimization performs poorly as compared to SGD, when L2 regularization is used. 

On the other hand, SGD and Adam performs equally when weight decay regularization is used.",l2 regular weight decay,"['l2', 'regular', 'weight', 'decay']",L2 regularization and weight decay,l2 regular weight decay similar regular techniqu result gradient descent shrink weight everi iter weight decay regular techniqu deep learn weight decay work ad penalti term cost function neural network effect shrink weight backpropagationadam optim perform poor compar stochast gradient descent l2 regular use hand stochast gradient descent adam perform equal weight decay regular use
1072,"Techniques to reduce overfitting

1. Cross-validation and hyperparameter tuning

2. Feature reduction

3. L1 / L2 regularization

4. Increasing data

5. Removing layers or tree pruning (decision tree)

6. Ensembling (decision tree)

7. Data augmentation (DNN)

8. Dropout (DNN)

9. Early stopping (DNN)",techniqu reduc overfit,"['techniqu', 'reduc', 'overfit']",Techniques to reduce overfitting,1 crossvalid hyperparamet tuning2 featur reduction3 l1 l2 regularization4 increas data5 remov layer tree prune decis tree6 ensembl decis tree7 data augment dnn8 dropout dnn9 earli stop dnn
1073,"SQL IN Operator

The IN operator allows you to specify multiple values in a WHERE clause.

SELECT column_name(s)
FROM table_name
WHERE column_name IN (value1, value2, ...);

or,

SELECT column_name(s)
FROM table_name
WHERE column_name IN (SELECT STATEMENT);",structur queri languag oper,"['structur', 'queri', 'languag', 'oper']",SQL IN Operator,oper allow specifi multipl valu clauseselect columnnam tablenam columnnam value1 value2 orselect columnnam tablenam columnnam select statement
1074,"SQL indexing

A SQL index is used to retrieve data from a database very fast. Indexing a table or view is, without a doubt, one of the best ways to improve the performance of queries and applications. A SQL index is a quick lookup table for finding records users need to search frequently.

# To create the index

CREATE INDEX index_name
ON table_name (column1, column2, ...);

# To view the index

SHOW INDEX FROM table_name;

# To view the index in sqlite

SELECT name FROM sqlite_master WHERE type='index';",structur queri languag index,"['structur', 'queri', 'languag', 'index']",SQL indexing,structur queri languag index use retriev data databas fast index tabl view without doubt one best way improv perform queri applic structur queri languag index quick lookup tabl find record user need search frequent creat indexcr index indexnam tablenam column1 column2 view indexshow index tablenam view index structur queri languageiteselect name structur queri languageitemast typeindex
1075,"Writing function in SQL

We can write a function in python and use that in SQL

create_function(""name_as_we_want"", no_of_params, python_func_name)

Example for sqlite3:

def length(data):
    result = len(data) + 10
    return result

connnection_engine.create_function(""length"", 1, length)

> Now we can use this length function in SQL queries",write function structur queri languag,"['write', 'function', 'structur', 'queri', 'languag']",Writing function in SQL,write function python use structur queri languagecreatefunctionnameaswew noofparam pythonfuncnameexampl structur queri languageite3def lengthdata result lendata 10 return resultconnnectionenginecreatefunctionlength 1 length use length function structur queri languag queri
1076,"Power Query in Excel or Power BI

Through power query, we can get (or import) and transform (or clean) the data. Then, we can load our query into Excel to create charts and reports. This is ETL (extract, transform and load) process.

This is to automate the data cleaning process in excel and power BI.",power queri excel power bi,"['power', 'queri', 'excel', 'power', 'bi']",Power Query in Excel or Power BI,power queri get import transform clean data load queri excel creat chart report etl extract transform load processthi autom data clean process excel power bi
1077,"Random forest hyperparameters

max_depth: the maximum depth of the tree. If None, then nodes are expanded until all nodes are pure or until all nodes contain samples less than min_samples_split, 

'min_samples_leaf': the minimum number of samples required to be at a terminal or leaf node, 

'min_samples_split': the minimum number of samples required for each split.",random forest hyperparamet,"['random', 'forest', 'hyperparamet']",Random forest hyperparameters,maxdepth maximum depth tree none node expand node pure node contain sampl less minsamplessplit minsamplesleaf minimum number sampl requir termin leaf node minsamplessplit minimum number sampl requir split
1078,"SVR hyperparameters

'C' is the regularization parameter. The strength of the regularization is inversely proportional to C, 

'gamma' is the kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Gamma decides that how much curvature we want in a decision boundary. For high values of gamma the points needs to be very close of each other in order to be considered in the same group.",svr hyperparamet,"['svr', 'hyperparamet']",SVR hyperparameters,c regular paramet strength regular invers proport c gamma kernel coeffici rbf poli sigmoid gamma decid much curvatur want decis boundari high valu gamma point need close order consid group
1079,"TFIDF hyperparameters

max_df = 0.9 means ""ignore terms that appear in more than 90% of the documents"" 

min_df = 10 means ""ignore terms that appear in less than 10 documents"".",tfidf hyperparamet,"['tfidf', 'hyperparamet']",TFIDF hyperparameters,maxdf 09 mean ignor term appear 90 document mindf 10 mean ignor term appear less 10 document
1080,"Word2Vec hyperparameters

workers: use these many worker threads to train the model, 

min_count: ignores all words with total frequency lower than this, 

window: maximum distance between the current and predicted word within a sentence
",word2vec hyperparamet,"['word2vec', 'hyperparamet']",Word2Vec hyperparameters,worker use mani worker thread train model mincount ignor word total frequenc lower window maximum distanc current predict word within sentenc
1081,"LDA hyperparameters

learning_decay: control learning rate,

n_components: number of topics",latent dirichlet alloc hyperparamet,"['latent', 'dirichlet', 'alloc', 'hyperparamet']",LDA hyperparameters,learningdecay control learn ratencompon number topic
1082,"Difference between PARTITION BY and GROUP BY in SQL

The GROUP BY clause is used often used in conjunction with an aggregate function such as SUM() and AVG(). The GROUP BY clause reduces the number of rows returned by rolling them up and calculating the sums or averages for each group.

SELECT department_id, ROUND(AVG(salary)) avg_department_salary FROM employees GROUP BY department_id ORDER BY department_id;

The PARTITION BY clause divides the result set into partitions and changes how the window function is calculated. The PARTITION BY clause does not reduce the number of rows returned.

SELECT first_name, last_name, department_id,     ROUND(AVG(salary) OVER (PARTITION BY department_id)) avg_department_salary FROM     employees;",differ partit group structur queri languag,"['differ', 'partit', 'group', 'structur', 'queri', 'languag']",Difference between PARTITION BY and GROUP BY in SQL,group claus use often use conjunct aggreg function sum avg group claus reduc number row return roll calcul sum averag groupselect departmentid roundavgsalari avgdepartmentsalari employe group departmentid order departmentidth partit claus divid result set partit chang window function calcul partit claus reduc number row returnedselect firstnam lastnam departmentid roundavgsalari partit departmentid avgdepartmentsalari employe
1083,"CROSS JOIN in sql

In CROSS JOIN each row from 1st table joins with all the rows of 2nd table. If 1st table contains x rows and 2nd table contains y rows, the result set will be x*y rows.

Say table1 contains meal column and table2 contains drinks column, then CROSS JOIN will create a table with all possible combinations of meal and drink.

SELECT column_name(s)
FROM table1
CROSS JOIN table2;",cross join structur queri languag,"['cross', 'join', 'structur', 'queri', 'languag']",CROSS JOIN in sql,cross join row 1st tabl join row 2nd tabl 1st tabl contain x row 2nd tabl contain row result set xy rowssay table1 contain meal column table2 contain drink column cross join creat tabl possibl combin meal drinkselect columnnam table1 cross join table2
1084,"Database or SQL Normalization

Normalization is the process of organizing data in a database. This includes creating tables and establishing relationships between those tables according to rules designed both to protect the data and to make the database more flexible by eliminating redundancy and inconsistent dependency.

First normal form:

> Eliminate repeating groups in individual tables.

> Create a separate table for each set of related data.

> Identify each set of related data with a primary key.

Second normal form:

> Create separate tables for sets of values that apply to multiple records.

> Relate these tables with a foreign key.

Third normal form:

> Eliminate fields that do not depend on the key.",databas structur queri languag normal,"['databas', 'structur', 'queri', 'languag', 'normal']",Database or SQL Normalization,normal process organ data databas includ creat tabl establish relationship tabl accord rule design protect data make databas flexibl elimin redund inconsist dependencyfirst normal form elimin repeat group individu tabl creat separ tabl set relat data identifi set relat data primari keysecond normal form creat separ tabl set valu appli multipl record relat tabl foreign keythird normal form elimin field depend key
1085,"SQL aggregate functions

An aggregate function in SQL performs a calculation on multiple values and returns a single value. SQL provides many aggregate functions that include COUNT(), SUM(), AVG(), MAX(), MIN(), ROUND()

An aggregate function ignores NULL values when it performs the calculation, except for the count function.",structur queri languag aggreg function,"['structur', 'queri', 'languag', 'aggreg', 'function']",SQL aggregate functions,aggreg function structur queri languag perform calcul multipl valu return singl valu structur queri languag provid mani aggreg function includ count sum avg max min roundan aggreg function ignor null valu perform calcul except count function
1086,"Python code for finding longest substring of vowels

def longestVowel(s):
    count, res = 0, 0
    vowels=""aeiouAEIOU"" 
    for i in range(len(s)): 
        if s[i] in vowels:
            count += 1 
        else:     
            res = max(res, count)
            count = 0
    return max(res, count)

longestVowel(""mAangooO"") returns 3",python code find longest substr vowel,"['python', 'code', 'find', 'longest', 'substr', 'vowel']",Python code for finding longest substring of vowels,def longestvowel count res 0 0 vowelsaeiouaeiou rangelen si vowel count 1 els res maxr count count 0 return maxr countlongestvowelmaangooo return 3
1087,"Apply function to each element of a list

Using map()

Map in Python is a function that works as an iterator to return a result after applying a function to every item of an iterable (tuple, lists, etc.).

map() is used for tuple, list etc. to perform similar operation like .apply() method for pandas df

list(map(my_function, my_list))",appli function element list,"['appli', 'function', 'element', 'list']",Apply function to each element of a list,use mapmap python function work iter return result appli function everi item iter tupl list etcmap use tupl list etc perform similar oper like appli method panda dflistmapmyfunct mylist
1088,"Python code for checking valid parentheses

def isValid(s):
    """"""
    This function is for checking valid parentheses
    input s: str
    output: bool
    """"""
    if s.count(""("")==s.count("")"") and s.count(""{"")==s.count(""}"") and s.count(""["")==s.count(""]""):
        ans=""true""
    else:
        ans=""false""

    return ans

isValid(""(())[[]]}"") returns 'false'",python code check valid parenthes,"['python', 'code', 'check', 'valid', 'parenthes']",Python code for checking valid parentheses,def isvalid function check valid parenthes input str output bool scountscount scountscount scountscount anstru els ansfals return ansisvalid return fals
1089,"Python code for converting roman to integer

def romanToInt(s):
    """"""
    This function is for converting roman to integer
    Input s: str
    output: int
    """"""
    my_dict={""I"":1,""V"":5,""X"":10,""L"":50,""C"":100,""D"":500,""M"":1000}
    res = 0
    i = 0
 
    while (i < len(s)):
        s1 = my_dict.get(s[i])
        if (i + 1 < len(s)):
            s2 = my_dict.get(s[i + 1])
            if (s1 >= s2):
                res = res + s1
                i = i + 1
            else: 
                res = res + s2 - s1
                i = i + 2
        else:
            res = res + s1
            i = i + 1
    return res

romanToInt(""MCMXCIV"") returns 1994",python code convert roman integ,"['python', 'code', 'convert', 'roman', 'integ']",Python code for converting roman to integer,def romantoint function convert roman integ input str output int mydicti1v5x10l50c100d500m1000 res 0 0 len s1 mydictgetsi 1 len s2 mydictgetsi 1 s1 s2 res res s1 1 els res res s2 s1 2 els res res s1 1 return resromantointmcmxciv return 1994
1090,"Python code for finding the indices of the two numbers such that they add up to target integer

def twoSum(nums, target):
    """"""
    This function is for finding the indices of the two numbers such that they add up to target integer
    Input nums: list of numbers, target: int
    output: list of index
    """"""
    num_indices_dict = {}
    for i in range(len(nums)):
        num_indices_dict[nums[i]] = i
    for i in range(len(nums)):
        complement = target - nums[i]
        if complement in num_indices_dict and num_indices_dict[complement] != i:
            return [i, num_indices_dict[complement]] 

twoSum([0,3,0],0) returns [0, 2]",python code find indic two number add target integ,"['python', 'code', 'find', 'indic', 'two', 'number', 'add', 'target', 'integ']",Python code for finding the indices of the two numbers such that they add up to target integer,def twosumnum target function find indic two number add target integ input num list number target int output list index numindicesdict rangelennum numindicesdictnumsi rangelennum complement target numsi complement numindicesdict numindicesdictcompl return numindicesdictcompl twosum0300 return 0 2
1091,"Python code for converting Integer to the Sum of Two No-Zero Integers

def getNoZeroIntegers(n):
    """"""
    This function is for converting an integer to the sum of two no-zero integers
    Input: n: int
    Output: List[int]
    """"""
    for k in reversed(range(n)):
        if ""0"" not in str(k) and ""0"" not in str(n-k):
            break
    result=[n-k,k]
    return result

getNoZeroIntegers(3009) returns [11, 2998]",python code convert integ sum two nozero integ,"['python', 'code', 'convert', 'integ', 'sum', 'two', 'nozero', 'integ']",Python code for converting Integer to the Sum of Two No-Zero Integers,def getnozerointegersn function convert integ sum two nozero integ input n int output listint k reversedrangen 0 strk 0 strnk break resultnkk return resultgetnozerointegers3009 return 11 2998
1092,"Python code for finding Longest Common Prefix

def longestCommonPrefix(strs):
    """"""
    This function is for finding Longest Common Prefix
    Input: strs: List[str]
    Output: str
    """"""
    len_list=[len(k) for k in strs]
    result=[k[:len(strs[len_list.index(min(len_list))])] for k in strs]
    c=len(result[0])
    while len(result[0])>=1:
        if len(set(result))==1:
            break
        else:
            result=[k[:(c-1)] for k in strs] 
        c=c-1
    return result[0]

longestCommonPrefix([""flower"",""flow"",""flight""]) returns 'fl'",python code find longest common prefix,"['python', 'code', 'find', 'longest', 'common', 'prefix']",Python code for finding Longest Common Prefix,def longestcommonprefixstr function find longest common prefix input strs liststr output str lenlistlenk k strs resultklenstrslenlistindexminlenlist k strs clenresult0 lenresult01 lensetresult1 break els resultkc1 k strs cc1 return result0longestcommonprefixflowerflowflight return fl
1093,"Data Pipeline

A pipeline or data pipeline, is a set of data processing elements or code or function written in the server (using python and SQL), where the output of one function is the input of the next one.

In data engineering, engineers design and build the data pipelines to transform the data into a format that is usable by end users such as the data scientists. 

In contrast with ETL, data pipelines are typically used to describe processes in the context of data engineering and big data. Usually, more code is involved and it's possible multiple tools or services are used to implement the pipeline. However, there are many similarities between ETL and data pipelines, so one might even conclude they're actually the same.",data pipelin,"['data', 'pipelin']",Data Pipeline,pipelin data pipelin set data process element code function written server use python sql output one function input next onein data engin engin design build data pipelin transform data format usabl end user data scientist contrast etl data pipelin typic use describ process context data engin big data usual code involv possibl multipl tool servic use implement pipelin howev mani similar etl data pipelin one might even conclud theyr actual
1094,"Ways to create a reliable data pipline

1. Embrace risk: We should keep in mind that the only way to have perfectly reliable data, is to not have any data. 

2. Set standards: Set Service Level Indicators (SLIs), Service Level Objective (SLO) and Service Level Agreement (SLA)

3. Reduce toil: “Toil” is Google’s chosen word for the human work needed to operate our system. Need to use automation to the maximum possible extend.

4. Monitor everything

5. Control releases

6. Maintain simplicity",way creat reliabl data piplin,"['way', 'creat', 'reliabl', 'data', 'piplin']",Ways to create a reliable data pipline,1 embrac risk keep mind way perfect reliabl data data 2 set standard set servic level indic slis servic level object slo servic level agreement sla3 reduc toil “toil” googl chosen word human work need oper system need use autom maximum possibl extend4 monitor everything5 control releases6 maintain simplic
1095,"Difference between artificial intelligence and neural logic

AI or machine learning refers to machines that are able to mimic human cognitive skills (psychological flow of thoughts). Neural Networks or deep learning, on the other hand, refers to a network of artificial neurons or nodes vaguely inspired by the biological neural networks that constitute human brain (physical flow of thoughts).",differ artifici intellig neural logic,"['differ', 'artifici', 'intellig', 'neural', 'logic']",Difference between artificial intelligence and neural logic,artifici intellig machin learn refer machin abl mimic human cognit skill psycholog flow thought neural network deep learn hand refer network artifici neuron node vagu inspir biolog neural network constitut human brartifici intelligencen physic flow thought
1096,"Logistic Regression Parameters

penalty: {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’

C: default=1.0
Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.

max_iterint: default=100
Maximum number of iterations taken for the solvers to converge.",logist regress paramet,"['logist', 'regress', 'paramet']",Logistic Regression Parameters,penalti l1 l2 elasticnet none default'l2'c default10 invers regular strength must posit float like support vector machin smaller valu specifi stronger regularizationmaxiterint default100 maximum number iter taken solver converg
1097,"Anomaly Detection in Time Series

There are few techniques that analysts can employ to identify different anomalies in data. It starts with a basic statistical decomposition and can work up to autoencoders. 

1. Seasonal-trend decomposition technique gives us the ability to split our time series signal into three parts: seasonal, trend, and residue.

If we analyze the deviation of residue and introduce some threshold for it, we’ll get an anomaly detection algorithm. 

2. We can utilize the power and robustness of Decision Trees to identify outliers/anomalies in time series data.

Isolation Forest detects anomalies purely based on the fact that anomalies are data points that are few and different. 

3. Detection using Forecasting

4. Clustering-based anomaly detection

5. Autoencoders",anomali detect time seri,"['anomali', 'detect', 'time', 'seri']",Anomaly Detection in Time Series,techniqu analyst employ identifi differ anomali data start basic statist decomposit work autoencod 1 seasonaltrend decomposit techniqu give us abil split time seri signal three part season trend residueif analyz deviat residu introduc threshold we'll get anomali detect algorithm 2 util power robust decis tree identifi outliersanomali time seri dataisol forest detect anomali pure base fact anomali data point differ 3 detect use forecasting4 clusteringbas anomali detection5 autoencod
1098,"Python code for finding the Anagrams

def isAnagram(s, t):
    """"""
    This function is for finding the Anagrams 
    (An Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.)
    Input:s: str,t: str
    Output: bool
    """"""
    
    if sorted(s) == sorted(t):
        return True
    else:
        return False",python code find anagram,"['python', 'code', 'find', 'anagram']",Python code for finding the Anagrams,def isanagram function find anagram anagram word phrase form rearrang letter differ word phrase typic use origin letter exact input strt str output bool sort sortedt return true els return fals
1099,"Python code for finding the sum as a binary string

def addBinary(a, b):
    """"""
    This function is for finding the sum as a binary string given two binary number string.
    Input: a: binary number str, b: binary number str
    Ouput: sum as binary number str
    """"""
    c=0
    result1=0
    for k in a[::-1]:
        if k==""1"":
            result1=result1+2**c
        c=c+1
        
    c=0
    result2=0
    for k in b[::-1]:
        if k==""1"":
            result2=result2+2**c
        c=c+1
        
    result=result1+result2
    
    return bin(result)[2:]

addBinary(""11"",""1"") returns ""100""",python code find sum binari string,"['python', 'code', 'find', 'sum', 'binari', 'string']",Python code for finding the sum as a binary string,def addbinarya b function find sum binari string given two binari number string input binari number str b binari number str ouput sum binari number str c0 result10 k a1 k1 result1result12c cc1 c0 result20 k b1 k1 result2result22c cc1 resultresult1result2 return binresult2addbinary111 return 100
1100,"Python code for finding First and Last Position of Element in Sorted Array

def searchRange(nums, target):
    """"""
    This function is for finding First and Last Position of Element in Sorted Array
    Input: nums: List[int], target: int
    Output: List[int]
    """"""
    try:
      start=-1
      end=-1
      for k in nums:
          if k==target:
              start=nums.index(k)
              break
              
      for k in list(reversed(nums)):
          if k==target:
              end=len(nums)-1-list(reversed(nums)).index(k)
              break
    except:
        print(""Please check your input"")
    
    return [start,end]

searchRange([5,7,7,8,8,10], 8) returns [3, 4]",python code find first last posit element sort array,"['python', 'code', 'find', 'first', 'last', 'posit', 'element', 'sort', 'array']",Python code for finding First and Last Position of Element in Sorted Array,def searchrangenum target function find first last posit element sort array input num listint target int output listint tri start1 end1 k num ktarget startnumsindexk break k listreversednum ktarget endlennums1listreversednumsindexk break except printpleas check input return startendsearchrange5778810 8 return 3 4
1101,"Python code for finding a perfect number

def perfect(num):
  '''
  This function is for finding a perfect number
  INPUT: num=int
  OUTPUT: print result
  '''
  my_list=[]
  for k in range(1, round(num/2)+1):
    if num%k==0:
      my_list.append(k)
  if sum(my_list)==num:
    print(""The number is a perfect number"")
  else:
    print(""The number is not a perfect number"")
  return",python code find perfect number,"['python', 'code', 'find', 'perfect', 'number']",Python code for finding a perfect number,def perfectnum function find perfect number input numint output print result mylist k range1 roundnum21 numk0 mylistappendk summylistnum printth number perfect number els printth number perfect number return
1102,"Subtopics of Introduction to SQL

> SQL Basics

> List of Relational database

> Difference between SQL and Python

> Basics of Relational Database (RDBMS)

> Basics of Non-Relational Database

> File extensions

> DATA/ DATA STRUCTURE TYPES IN SQL

> SQL string datatypes

> Database or SQL Normalization

> Data Pipeline

> Ways to create a reliable data pipline
",subtop introduct structur queri languag,"['subtop', 'introduct', 'structur', 'queri', 'languag']",Subtopics of Introduction to SQL,structur queri languag basic list relat databas differ structur queri languag python basic relat databas rdbms basic nonrel databas file extens data data structur type structur queri languag structur queri languag string datatyp databas structur queri languag normal data pipelin way creat reliabl data piplin
1103,"Subtopics of BASIC SQL COMMANDS

> Parts of SQL

> SQL Statements

> SQL Query/ Statements/ commands

> SELECT and SELECT DISTINT statements

> SQL aggregate functions

> Sequence in SQL

> Conditional operators

> LIKE and ILIKE

> SQL aliases

> Different Types of SQL JOINs

> CROSS JOIN in sql

> CONDITIONAL EXPRESSIONS & PROCEDURES

> UNION in SQL

> SQL IN Operator

> SQL NOT IN

> Operation with Null values in SQL

> SQL CONCAT

> Window Function

> Difference between PARTITION BY and GROUP BY in SQL

> SQL and String

> Common Table Expression (CTE)
",subtop basic structur queri languag command,"['subtop', 'basic', 'structur', 'queri', 'languag', 'command']",Subtopics of BASIC SQL COMMANDS,part structur queri languag structur queri languag statement structur queri languag queri statement command select select distint statement structur queri languag aggreg function sequenc structur queri languag condit oper like ilik structur queri languag alias differ type structur queri languag join cross join structur queri languag condit express procedur union structur queri languag structur queri languag oper structur queri languag oper null valu structur queri languag structur queri languag concat window function differ partit group structur queri languag structur queri languag string common tabl express cte
1104,"Subtopics of Practicing sql in python environment

> SQLite

> Practicing sql queries

> Reading SQLite Database file as pandas df

> SQL queries in Python
",subtop practic structur queri languag python environ,"['subtop', 'practic', 'structur', 'queri', 'languag', 'python', 'environ']",Subtopics of Practicing sql in python environment,structur queri languageit practic structur queri languag queri read structur queri languageit databas file panda df structur queri languag queri python
1105,"Subtopics of SQL keys

> CONSTRAINTS IN SQL

> Primary Key and Foreign Key

> Super Key and Candidate Key
",subtop structur queri languag key,"['subtop', 'structur', 'queri', 'languag', 'key']",Subtopics of SQL keys,constraint sql primari key foreign key super key candid key
1106,"Python Code to Find Next Prime Number

n=int(input())
num=n
num_sum=[]
while len(num_sum)!=2:
    num=num+1
    num_sum=[]
    for k in range(1, num+1):
        if num%k==0:
            num_sum.append(k)
print(f""The next prime number of {n} is: {num}"")",python code find next prime number,"['python', 'code', 'find', 'next', 'prime', 'number']",Python Code to Find Next Prime Number,nintinput numn numsum lennumsum2 numnum1 numsum k range1 num1 numk0 numsumappendk printfth next prime number n num
1107,"Python code to count vowels and consonants in the string

string = input() 
string=string.lower()
string=string.split()
string="""".join(string)
v=0
c=0
vowels=""aeiou""
for k in string:
    if k in vowels:
        v=v+1
    else:
        c=c+1
print(f""Vowels: {v}"")
print(f""Consonants: {c}"")",python code count vowel conson string,"['python', 'code', 'count', 'vowel', 'conson', 'string']",Python code to count vowels and consonants in the string,string input stringstringlow stringstringsplit stringjoinstr v0 c0 vowelsaeiou k string k vowel vv1 els cc1 printfvowel v printfconson c
1108,"Python code to find the first non-repeating character in given string

string=input()
char_order = []
ctr = {}
result=""None""
for c in string:
  if c in ctr:
    ctr[c] += 1
  else:
    ctr[c] = 1 
    char_order.append(c)
for c in char_order:
  if ctr[c] == 1:
    result= c
    break
print(result)",python code find first nonrep charact given string,"['python', 'code', 'find', 'first', 'nonrep', 'charact', 'given', 'string']",Python code to find the first non-repeating character in given string,stringinput charord ctr resultnon c string c ctr ctrc 1 els ctrc 1 charorderappendc c charord ctrc 1 result c break printresult
1109,"Python code to find the largest prime factor of a given number

n = int(input())

prime_factor=[]
while n>1:
    for c in [2,3,5,7,11,13,17,19]:
      if n%c==0:
          prime_factor.append(c)
          n=n/c
print(max(prime_factor))",python code find largest prime factor given number,"['python', 'code', 'find', 'largest', 'prime', 'factor', 'given', 'number']",Python code to find the largest prime factor of a given number,n intinputprimefactor n1 c 235711131719 nc0 primefactorappendc nnc printmaxprimefactor
1110,"Python code to find the greatest common divisor (GCD) of two integers

a=int(input())
b=int(input())
res=[]
for k in range(2, int(max(a, b)/2)):
    if a%k==0 and b%k==0:
        res.append(k)
print(max(res))",python code find greatest common divisor gcd two integ,"['python', 'code', 'find', 'greatest', 'common', 'divisor', 'gcd', 'two', 'integ']",Python code to find the greatest common divisor (GCD) of two integers,aintinput bintinput res k range2 intmaxa b2 ak0 bk0 resappendk printmaxr
1111,"Python code to find the biggest even number between two numbers inclusive

m = int(input())
n = int(input())

if n>m:
    print(f""Biggest even number between {m} and {n}\n{max([k for k in range(m, n) if k%2==0])}"")
else:
    print(f""Biggest even number between {n} and {m}\n{max([k for k in range(n, m) if k%2==0])}"")",python code find biggest even number two number inclus,"['python', 'code', 'find', 'biggest', 'even', 'number', 'two', 'number', 'inclus']",Python code to find the biggest even number between two numbers inclusive,intinput n intinputif nm printfbiggest even number nnmaxk k rangem n k20 els printfbiggest even number n mnmaxk k rangen k20
1112,"Python code to find the string consisting of all the words whose lengths are prime numbers

Input: The quick brown fox jumps over the lazy dog.

Output: The quick brown fox jumps the

strs = input()
result="" "".join([k for k in strs.split() if len([i for i in range(1, len(k)+1) if len(k)%i==0])==2])

print(result)",python code find string consist word whose length prime number,"['python', 'code', 'find', 'string', 'consist', 'word', 'whose', 'length', 'prime', 'number']",Python code to find the string consisting of all the words whose lengths are prime numbers,input quick brown fox jump lazi dogoutput quick brown fox jump thestr input result joink k strssplit leni range1 lenk1 lenki02printresult
1113,"Python code to print a Fibonacci series

num =int(input())
res=[0, 1]
for k in range(num-2):
    res.append(res[-1]+res[-2])
print(res)",python code print fibonacci seri,"['python', 'code', 'print', 'fibonacci', 'seri']",Python code to print a Fibonacci series,num intinput res0 1 k rangenum2 resappendres1res2 printr
1114,"Python code to get a list, sorted in increasing order by the last element in each tuple from a given list of non-empty tuples.

Input:
[(2, 5), (1, 2), (4, 4), (2, 3), (2, 1)]

Output:
[(2, 1), (1, 2), (2, 3), (4, 4), (2, 5)]

lst= eval(input())
res=[]
my_dict={k[1]:k for k in lst}
for k in sorted(list(my_dict.keys())):
    res.append(my_dict.get(k))
print(res)",python code get list sort increas order last element tupl given list nonempti tupl,"['python', 'code', 'get', 'list', 'sort', 'increas', 'order', 'last', 'element', 'tupl', 'given', 'list', 'nonempti', 'tupl']","Python code to get a list, sorted in increasing order by the last element in each tuple from a given list of non-empty tuples.",input 2 5 1 2 4 4 2 3 2 1output 2 1 1 2 2 3 4 4 2 5lst evalinput res mydictk1k k lst k sortedlistmydictkey resappendmydictgetk printr
1115,"Python code to remove duplicates from a list of lists

Input:
[[10, 20], [40], [30, 56, 25], [10, 20], [33], [40]]

Output:
[[10, 20], [40], [30, 56, 25], [33]]

lst = eval(input())
res=[]
for k in lst:
    if res.count(k)==0:
        res.append(k)
print(res)",python code remov duplic list list,"['python', 'code', 'remov', 'duplic', 'list', 'list']",Python code to remove duplicates from a list of lists,input 10 20 40 30 56 25 10 20 33 40output 10 20 40 30 56 25 33lst evalinput res k lst rescountk0 resappendk printr
1116,"Python code to find the strings in a given list, starting with a given prefix

Input:
['cat', 'car', 'fear', 'center']

""ca""

Output:
Strings in the said list starting with a given prefix:

['cat', 'car']

strs =  eval(input())
prefix = input()

print(f""Strings in the said list starting with a given prefix:\n{[k for k in strs if k.startswith(prefix)]}"")",python code find string given list start given prefix,"['python', 'code', 'find', 'string', 'given', 'list', 'start', 'given', 'prefix']","Python code to find the strings in a given list, starting with a given prefix",input cat car fear centercaoutput string said list start given prefixcat carstr evalinput prefix inputprintfstr said list start given prefixnk k strs kstartswithprefix
1117,"Python program to determine the direction ('increasing' or 'decreasing') of monotonic sequence numbers

Input:
[1,2,3,4,5,6]

Output:
Increasing

n = eval(input())
if n==sorted(n):
    print(""Increasing"")
elif n==sorted(n, reverse=True):
    print(""Decreasing"")
else:
    print(""Not a monotonic sequence"")",python program determin direct increas decreas monoton sequenc number,"['python', 'program', 'determin', 'direct', 'increas', 'decreas', 'monoton', 'sequenc', 'number']",Python program to determine the direction ('increasing' or 'decreasing') of monotonic sequence numbers,input 123456output increasingn evalinput nsortedn printincreas elif nsortedn reversetru printdecreas els printnot monoton sequenc
1118,"Python code that accepts a comma separated sequence of words as input and prints the unique words in sorted form (alphanumerically)

Input:
lst= [red, black, pink, green, black, green]

Output:
black,green,pink,red

list_items = input()
list_items=list_items.replace(""["","""")
list_items=list_items.replace(""]"","""")
list_items=list_items.split("", "")
list_items=[str(k) for k in list_items]
items_set=set(list_items)
print("","".join(sorted(list(items_set))))",python code accept comma separ sequenc word input print uniqu word sort form alphanumer,"['python', 'code', 'accept', 'comma', 'separ', 'sequenc', 'word', 'input', 'print', 'uniqu', 'word', 'sort', 'form', 'alphanumer']",Python code that accepts a comma separated sequence of words as input and prints the unique words in sorted form (alphanumerically),input lst red black pink green black greenoutput blackgreenpinkredlistitem input listitemslistitemsreplac listitemslistitemsreplac listitemslistitemssplit listitemsstrk k listitem itemssetsetlistitem printjoinsortedlistitemsset
1119,"Program in Python to count number of duplicates in an array having multiple duplicates

num_of_elements = int(input())

#input the number of elements in loop
my_list=[]
for k in range(num_of_elements):
    my_list.append(int(input()))
print(f""2 is repeated {my_list.count(2)} times"")",program python count number duplic array multipl duplic,"['program', 'python', 'count', 'number', 'duplic', 'array', 'multipl', 'duplic']",Program in Python to count number of duplicates in an array having multiple duplicates,numofel intinputinput number element loop mylist k rangenumofel mylistappendintinput printf2 repeat mylistcount2 time
1120,"Python program to find all n-digit integers that start or end with 2

n = int(input())
all_num=[k for k in range(10**(n-1), 10**n)]
print([k for k in all_num if str(k)[0]==""2"" or str(k)[-1]==""2""])",python program find ndigit integ start end 2,"['python', 'program', 'find', 'ndigit', 'integ', 'start', 'end', '2']",Python program to find all n-digit integers that start or end with 2,n intinput allnumk k range10n1 10n printk k allnum strk02 strk12
1121,"Given a list of numbers and a number to inject, write a Python program to create a list containing that number in between each pair of adjacent numbers

lst=eval(input())
n = int(input())
res=[]
for k in range(len(lst)-1):
    res.extend([lst[k], n])
res.append(lst[-1])
print(res)",given list number number inject write python program creat list contain number pair adjac number,"['given', 'list', 'number', 'number', 'inject', 'write', 'python', 'program', 'creat', 'list', 'contain', 'number', 'pair', 'adjac', 'number']","Given a list of numbers and a number to inject, write a Python program to create a list containing that number in between each pair of adjacent numbers",lstevalinput n intinput res k rangelenlst1 resextendlstk n resappendlst1 printr
1122,"Python code to find a substring in a given string which contains a vowel between two consonants

Input:
Hello

Output:
Hel

n = input()
vowel=""aeiou""
for k in range(len(n)-2):
    if n.lower()[k] not in vowel and n.lower()[k+1] in vowel and n.lower()[k+2] not in vowel:
        print(n[k:k+3])",python code find substr given string contain vowel two conson,"['python', 'code', 'find', 'substr', 'given', 'string', 'contain', 'vowel', 'two', 'conson']",Python code to find a substring in a given string which contains a vowel between two consonants,input hellooutput heln input vowelaeiou k rangelenn2 nlowerk vowel nlowerk1 vowel nlowerk2 vowel printnkk3
1123,"Python program to find the indices of three numbers that sum to 0 in a given list of numbers

Input:
[12, -7, 3, -89, 14, 4, -78, -1, 2, 7]

Output:
[1, 2, 5]

n = eval(input())
from itertools import combinations
res=[list(k) for k in list(combinations(n, 3)) if sum(k)==0]
for k in res:
  print([n.index(k[0]), n.index(k[1]), n.index(k[2])])",python program find indic three number sum 0 given list number,"['python', 'program', 'find', 'indic', 'three', 'number', 'sum', '0', 'given', 'list', 'number']",Python program to find the indices of three numbers that sum to 0 in a given list of numbers,input 12 7 3 89 14 4 78 1 2 7output 1 2 5n evalinput itertool import combin reslistk k listcombinationsn 3 sumk0 k res printnindexk0 nindexk1 nindexk2
1124,"Python program to get the single digits in numbers sorted backwards and converted to English words

Input:
[1, 3, 4, 5, 11]

Output:
['five', 'four', 'three', 'one']

n = eval(input())
word_dict={1:""one"", 2:""two"", 3:""three"", 4:""four"", 5:""five"", 6:""six"", 7:""seven"", 8:""eight"", 9:""nine""}
n=sorted([k for k in n if k/10<1], reverse=True)
print([word_dict.get(k) for k in n])",python program get singl digit number sort backward convert english word,"['python', 'program', 'get', 'singl', 'digit', 'number', 'sort', 'backward', 'convert', 'english', 'word']",Python program to get the single digits in numbers sorted backwards and converted to English words,input 1 3 4 5 11output five four three onen evalinput worddict1on 2two 3three 4four 5five 6six 7seven 8eight 9nine nsortedk k n k101 reversetru printworddictgetk k n
1125,"Python code to find the sum of all the numbers (within certain range) that can be written as the sum of fifth powers of their digits

Input:
range=1000000

Output:
443839

n=int(input())
res=[]

for k in range(10, n):
    num=k
    num_sum=0
    while num>0:
        num_sum=num_sum+(num%10)**5
        num=num//10
    if num_sum==k:
        res.append(num_sum)

print(sum(res))",python code find sum number within certain rang written sum fifth power digit,"['python', 'code', 'find', 'sum', 'number', 'within', 'certain', 'rang', 'written', 'sum', 'fifth', 'power', 'digit']",Python code to find the sum of all the numbers (within certain range) that can be written as the sum of fifth powers of their digits,input range1000000output 443839nintinput resfor k range10 n numk numsum0 num0 numsumnumsumnum105 numnum10 numsumk resappendnumsumprintsumr
1126,"Python program to find the vowels from each of the original texts (y counts as a vowel at the end of the word) from a given list of strings

Input:
['w3resource', 'Python', 'Java', 'C++']

Output:
['eoue', 'o', 'aa', '']

n = eval(input())
vowels=""aeiou""
res=[]

for elem in n:
    string=""""
    for k in elem.lower():
        if k in vowels:
            string=string+k
    if elem.lower()[-1]==""y"":
        string=string+""y""
    res.append(string)

print(res)",python program find vowel origin text count vowel end word given list string,"['python', 'program', 'find', 'vowel', 'origin', 'text', 'count', 'vowel', 'end', 'word', 'given', 'list', 'string']",Python program to find the vowels from each of the original texts (y counts as a vowel at the end of the word) from a given list of strings,input w3resourc python java coutput eoue aa n evalinput vowelsaeiou resfor elem n string k elemlow k vowel stringstringk elemlower1i stringstringi resappendstringprintr
1127,"Python program to find the two closest distinct numbers in a given a list of numbers

Input:
[1.3, 5.24, 0.89, 21.0, 5.27, 1.3]

Output:
[5.24, 5.27]

lst = eval(input())
lst=sorted(lst)
# replace 0 with large number to find the unique numbers
my_list=[lst[k+1]-lst[k] for k in range(len(lst)-1)]
my_list[my_list.index(0)]=10**10
print([lst[my_list.index(min(my_list))], lst[my_list.index(min(my_list))+1]])",python program find two closest distinct number given list number,"['python', 'program', 'find', 'two', 'closest', 'distinct', 'number', 'given', 'list', 'number']",Python program to find the two closest distinct numbers in a given a list of numbers,input 13 524 089 210 527 13output 524 527lst evalinput lstsortedlst replac 0 larg number find uniqu number mylistlstk1lstk k rangelenlst1 mylistmylistindex01010 printlstmylistindexminmylist lstmylistindexminmylist1
1128,"Python program to find all 5's in integers less than n that are divisible by 9 or 15

Input:
50

Output:
[[15, 1], [45, 1]]

n = int(input())
lst= [k for k in range(10, n) if k%9==0 or k%15==0]

lst=[k for k in lst if str(k)[0]==""5"" or str(k)[1]==""5""]
print([[k, str(k).index(""5"")] for k in lst])",python program find 5s integ less n divis 9 15,"['python', 'program', 'find', '5s', 'integ', 'less', 'n', 'divis', '9', '15']",Python program to find all 5's in integers less than n that are divisible by 9 or 15,input 50output 15 1 45 1n intinput lst k k range10 n k90 k150lstk k lst strk05 strk15 printk strkindex5 k lst
1129,"Python program to find the sum of the even elements that are at odd indices

Input:
[1, 2, 8, 3, 9, 4]

Output:
6

n = eval(input())
print(sum([n[k] for k in range(1,len(n),2) if n[k]%2==0]))",python program find sum even element odd indic,"['python', 'program', 'find', 'sum', 'even', 'element', 'odd', 'indic']",Python program to find the sum of the even elements that are at odd indices,input 1 2 8 3 9 4output 6n evalinput printsumnk k range1lenn2 nk20
1130,"Python program to find three numbers from an array such that the sum of three numbers equal to zero

Input:
[-1,0,1,2,-1,-4]

Output:
[[-1, -1, 2], [-1, 0, 1]]

n = eval(input())
from itertools import combinations
lst=[sorted(list(k)) for k in list(combinations(n,3)) if sum(k)==0]
res=[]
for k in lst:
  while res.count(k)<1:
    res.append(k)
print(res)",python program find three number array sum three number equal zero,"['python', 'program', 'find', 'three', 'number', 'array', 'sum', 'three', 'number', 'equal', 'zero']",Python program to find three numbers from an array such that the sum of three numbers equal to zero,input 101214output 1 1 2 1 0 1n evalinput itertool import combin lstsortedlistk k listcombinationsn3 sumk0 res k lst rescountk1 resappendk printr
1131,"Python program to find the factorial of the number using recursion method

num = int(input())
n=num 
if num==0:
    print(""Factorial of 0 is 1"")
elif num<0:
    print(""Factorial can't be calculated for negative number"")
else:
    res=1
    while num>1:
        res=res*num
        num=num-1
    print(f""Factorial of {n} is {res}"")",python program find factori number use recurs method,"['python', 'program', 'find', 'factori', 'number', 'use', 'recurs', 'method']",Python program to find the factorial of the number using recursion method,num intinput nnum num0 printfactori 0 1 elif num0 printfactori cant calcul negat number els res1 num1 resresnum numnum1 printffactori n res
1132,"Python program to find words with both alphabets and numbers from an input string

Input:
string= Emma25 is Data scientist50 and AI Expert

Output:
Displaying words with alphabets and numbers

Emma25

scientist50

string=input()
lst=string.split()
print(""Displaying words with alphabets and numbers"")
for k in [k for k in lst if k.isalpha()==False]:
    print(k)",python program find word alphabet number input string,"['python', 'program', 'find', 'word', 'alphabet', 'number', 'input', 'string']",Python program to find words with both alphabets and numbers from an input string,input string emma25 data scientist50 artifici intellig expertoutput display word alphabet numbersemma25scientist50stringinput lststringsplit printdisplay word alphabet number k k k lst kisalphafals printk
1133,"Python program to restore the original string by entering the compressed string

Input:
Original text: #39+1=1#30

Output:
999+1=1000

comp_str=input()
while ""#"" in comp_str:
  comp_str=comp_str[:comp_str.index(""#"")]+comp_str[comp_str.index(""#"")+2]*int(comp_str[comp_str.index(""#"")+1])+comp_str[comp_str.index(""#"")+3:]
print(comp_str)",python program restor origin string enter compress string,"['python', 'program', 'restor', 'origin', 'string', 'enter', 'compress', 'string']",Python program to restore the original string by entering the compressed string,input origin text 391130output 99911000compstrinput compstr compstrcompstrcompstrindexcompstrcompstrindex2intcompstrcompstrindex1compstrcompstrindex3 printcompstr
1134,"Python program to pack consecutive duplicates of a given list elements into sublists

Input:
List= [0, 0, 1, 2, 3, 4, 4, 5, 6, 6, 6, 7, 8, 9]

Output:
[[0, 0], [1], [2], [3], [4, 4], [5], [6, 6, 6], [7], [8], [9]]

l=eval(input())
print([[k]*l.count(k) for k in set(l)])",python program pack consecut duplic given list element sublist,"['python', 'program', 'pack', 'consecut', 'duplic', 'given', 'list', 'element', 'sublist']",Python program to pack consecutive duplicates of a given list elements into sublists,input list 0 0 1 2 3 4 4 5 6 6 6 7 8 9output 0 0 1 2 3 4 4 5 6 6 6 7 8 9levalinput printklcountk k setl
1135,"Python program to create a list whose ith element is the maximum of the first i elements from a input list

Input:
[0, -1, 3, 8, 5, 9, 8, 14, 2, 4, 3, -10, 10, 17, 41, 22, -4, -4, -15, 0]

Output:
[0, 0, 3, 8, 8, 9, 9, 14, 14, 14, 14, 14, 14, 17, 41, 41, 41, 41, 41, 41]

n = eval(input())
print([max(n[:k+1]) for k in range(len(n))])",python program creat list whose ith element maximum first element input list,"['python', 'program', 'creat', 'list', 'whose', 'ith', 'element', 'maximum', 'first', 'element', 'input', 'list']",Python program to create a list whose ith element is the maximum of the first i elements from a input list,input 0 1 3 8 5 9 8 14 2 4 3 10 10 17 41 22 4 4 15 0output 0 0 3 8 8 9 9 14 14 14 14 14 14 17 41 41 41 41 41 41n evalinput printmaxnk1 k rangelenn
1136,"Python program to find all integers <= 1000 that are the product of exactly three primes. Each integer should represent as the list of its three prime factors

Input:
10

Output:
[[2, 2, 2]]

num = int(input())
res=[]

for n in range(2, num+1):
  prime_factor=[]
  while n>1:
    if n%2==0:
      prime_factor.append(2)
      n=n/2
    elif n%3==0:
      prime_factor.append(3)
      n=n/3
    elif n%5==0:
      prime_factor.append(5)
      n=n/5
    elif n%7==0:
      prime_factor.append(7)
      n=n/7
    else:
      n=1
  if len(prime_factor)==3:
    res.append(prime_factor)
print(res)",python program find integ 1000 product exact three prime integ repres list three prime factor,"['python', 'program', 'find', 'integ', '1000', 'product', 'exact', 'three', 'prime', 'integ', 'repres', 'list', 'three', 'prime', 'factor']",Python program to find all integers <= 1000 that are the product of exactly three primes. Each integer should represent as the list of its three prime factors,input 10output 2 2 2num intinput resfor n range2 num1 primefactor n1 n20 primefactorappend2 nn2 elif n30 primefactorappend3 nn3 elif n50 primefactorappend5 nn5 elif n70 primefactorappend7 nn7 els n1 lenprimefactor3 resappendprimefactor printr
1137,"Python program to find the closest palindrome from a given string

Input:
abcde

Output:
abcba

n = input()
c=1

while n!=n[::-1]:
    n=n[:-c]+n[:c][::-1]
    c=c+1
print(n)

",python program find closest palindrom given string,"['python', 'program', 'find', 'closest', 'palindrom', 'given', 'string']",Python program to find the closest palindrome from a given string,input abcdeoutput abcban input c1while nn1 nncnc1 cc1 printn
1138,"Python program to find a list of numbers: the first element is the smallest, the second is the largest of the remaining, the third is the smallest of the remaining, the fourth is the smallest of the remaining, etc.

Input:
[1, 3, 4, 5, 11]

Output:
[1, 11, 3, 5, 4]

n = eval(input())
res=[]
k=1
while len(n)>0:
    if k%2!=0:
        res.append(min(n))
        n.remove(min(n))
        k=k+1
        
    else:
        res.append(max(n))
        n.remove(max(n))
        k=k+1
        
print(res)",python program find list number first element smallest second largest remain third smallest remain fourth smallest remain etc,"['python', 'program', 'find', 'list', 'number', 'first', 'element', 'smallest', 'second', 'largest', 'remain', 'third', 'smallest', 'remain', 'fourth', 'smallest', 'remain', 'etc']","Python program to find a list of numbers: the first element is the smallest, the second is the largest of the remaining, the third is the smallest of the remaining, the fourth is the smallest of the remaining, etc.",input 1 3 4 5 11output 1 11 3 5 4n evalinput res k1 lenn0 k20 resappendminn nremoveminn kk1 els resappendmaxn nremovemaxn kk1 printr
1139,"Python program to compute the depth of each parentheses group

Input:
(()) (()) () ((()()()))

Output:
[2, 2, 1, 3]

n = input()
lst=n.split()
res=[]
for k in lst:
  for j in range(len(k)):
    if k[j]=="")"":
      res.append(j)
      break

print(res)",python program comput depth parenthes group,"['python', 'program', 'comput', 'depth', 'parenthes', 'group']",Python program to compute the depth of each parentheses group,input output 2 2 1 3n input lstnsplit res k lst j rangelenk kj resappendj breakprintr
1140,"Python program to split it into groups of perfectly matched parentheses without any whitespace

Input:
( ()) ((()()())) (()) ()

Output:
['(())', '((()()()))', '(())', '()']

para = input()
para=para.split()
para="""".join(para)
res=[]
p_open=0
p_close=0
while len(para)>0:
    for k in range(len(para)):
        if para[k]==""("":
            p_open+=1
        else:
            p_close+=1
        if p_open==p_close and p_open!=0:
            res.append(para[:p_open*2])
            para=para[p_open*2:]
            p_open=0
            p_close=0
            break

print(res)",python program split group perfect match parenthes without whitespac,"['python', 'program', 'split', 'group', 'perfect', 'match', 'parenthes', 'without', 'whitespac']",Python program to split it into groups of perfectly matched parentheses without any whitespace,input output para input paraparasplit parajoinpara res popen0 pclose0 lenpara0 k rangelenpara parak popen1 els pclose1 popenpclos popen0 resappendparapopen2 paraparapopen2 popen0 pclose0 breakprintr
1141,"Python program to find a string consisting of space-separated characters with given counts

Input:
{""f"": 1, ""o"": 2}

Output:
f o o

n = eval(input())
string=""""
for k in n.keys():
    string=string+(k+"" "")*n.get(k)

print(string[:-1])",python program find string consist spacesepar charact given count,"['python', 'program', 'find', 'string', 'consist', 'spacesepar', 'charact', 'given', 'count']",Python program to find a string consisting of space-separated characters with given counts,input f 1 2output f evalinput string k nkey stringstringk ngetkprintstring1
1142,"Python Program to Determine Whether one String is a Rotation of Another

Input:
Given first string = ""btechgeeks""

Given second string = ""geeksbtech""

Output:
The given second string is the rotation of the given first string

fst_str = input()
secnd_str = input()
res=""The given second string is not the rotation of the given first string""
for k in range(1, len(fst_str)):
    if secnd_str[k:]+secnd_str[:k]==fst_str:
        res=""The given second string is the rotation of the given first string""

print(res)",python program determin whether one string rotat anoth,"['python', 'program', 'determin', 'whether', 'one', 'string', 'rotat', 'anoth']",Python Program to Determine Whether one String is a Rotation of Another,input given first string btechgeeksgiven second string geeksbtechoutput given second string rotat given first stringfststr input secndstr input resth given second string rotat given first string k range1 lenfststr secndstrksecndstrkfststr resth given second string rotat given first stringprintr
1143,"Python program to print Pascal triangle as shown below

Input:
6

Output:
[1]

[1, 1]

[1, 2, 1]

[1, 3, 3, 1]

[1, 4, 6, 4, 1]

[1, 5, 10, 10, 5, 1]

n = int(input())
res1=[1]
print(res1)
res2=[1,1]
print(res2)

for k in range(n-2):
  for k in range(len(res2)):
    res1.append(sum(res2[k:k+2]))
      
  print(res1)
  res2=res1
  res1=[1]",python program print pascal triangl shown,"['python', 'program', 'print', 'pascal', 'triangl', 'shown']",Python program to print Pascal triangle as shown below,input 6output 11 11 2 11 3 3 11 4 6 4 11 5 10 10 5 1n intinput res11 printres1 res211 printres2for k rangen2 k rangelenres2 res1appendsumres2kk2 printres1 res2res1 res11
1144,"Python code to create a sentence from a dictionary

Input: {""d"":""data"", ""r"":""reader"", ""o"":""of"", ""n"":""new"", ""a"":""age""}

Ouput: ""drona stands for data reader of new age

input_dict=eval(input())

name="""".join(list(input_dict.keys()))
meaning_lst=list(input_dict.values())
meaning=""""
for k in meaning_lst:
  meaning=meaning+"" ""+k

print(f""{name} stands for{meaning}"")",python code creat sentenc dictionari,"['python', 'code', 'creat', 'sentenc', 'dictionari']",Python code to create a sentence from a dictionary,input ddata rreader oof nnew aageouput drona stand data reader new ageinputdictevalinputnamejoinlistinputdictkey meaninglstlistinputdictvalu mean k meaninglst meaningmean kprintfnam stand formean
1145,"Data Scientist must have statistical thinking and meditative mind in their attitude and must have skills like python, sql and data visualization. Python code for converting a dictionary into a table using pandas

import pandas as pd

input_dict={""attitude"":[""statistical thinking"", ""meditative mind"",""-""], ""skills"": [""python"", ""sql"", ""data visualization""]}

data_scientist=pd.DataFrame(input_dict)
data_scientist",data scientist must statist think medit mind attitud must skill like python structur queri languag data visual python code convert dictionari tabl use panda,"['data', 'scientist', 'must', 'statist', 'think', 'medit', 'mind', 'attitud', 'must', 'skill', 'like', 'python', 'structur', 'queri', 'languag', 'data', 'visual', 'python', 'code', 'convert', 'dictionari', 'tabl', 'use', 'panda']","Data Scientist must have statistical thinking and meditative mind in their attitude and must have skills like python, sql and data visualization. Python code for converting a dictionary into a table using pandas",import panda pdinputdictattitudestatist think medit mind skill python sql data visualizationdatascientistpddataframeinputdict datascientist
